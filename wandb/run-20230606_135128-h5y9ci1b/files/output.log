Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
                                                                                                                                            /home/qcqced/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))                                                                             | 0/111421 [00:00<?, ?it/s]
[1/5] Train & Validation
/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py:283: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(
                                                                                                                                            /home/qcqced/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in MeanBackward0. Traceback of forward call that caused the error:
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 23, in <module>                       | 1/111421 [00:01<45:26:54,  1.47s/it]
    main('googleai4code.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 19, in main
    getattr(train_loop, cfg.loop)(cfg)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/train_loop.py", line 34, in train_loop
    train_loss = train_input.train_fn(
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py", line 268, in train_fn
    loss = loss + criterion(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/model/loss.py", line 147, in forward
    return F.margin_ranking_loss(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 3334, in margin_ranking_loss
    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)
 (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0%|                                                                                                 | 1/111421 [00:02<71:05:08,  2.30s/it]
Traceback (most recent call last):
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 23, in <module>
    main('googleai4code.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 19, in main
    getattr(train_loop, cfg.loop)(cfg)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/train_loop.py", line 34, in train_loop
    train_loss = train_input.train_fn(
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py", line 277, in train_fn
    scaler.scale(loss).backward()
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.