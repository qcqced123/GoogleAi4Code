Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                                                | 0/73 [00:00<?, ?it/s]
  torch.nn.utils.clip_grad_norm(                                                                                     | 0/73 [00:00<?, ?it/s]

































  0%|                                                                                                                | 0/27 [00:00<?, ?it/s]
Traceback (most recent call last):                                                                                   | 0/27 [00:00<?, ?it/s]
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 23, in <module>
    main('dictionarywise_trainer.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 19, in main
    getattr(train_loop, cfg.loop)(cfg)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/train_loop.py", line 37, in train_loop
    valid_metric = train_input.valid_fn(
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py", line 155, in valid_fn
    val_metric = val_metric + val_metrics(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/model/metric.py", line 170, in forward
    ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/model/metric.py", line 170, in <listcomp>
    ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth
ValueError: -0.7391637563705444 is not in list
tensor([[-0.7392, -0.8608, -0.9375, -1.0253, -0.7495, -0.7678, -0.9284, -0.7979,
         -0.6553, -0.4485, -0.7518, -0.7289, -0.7165, -0.9276, -0.7292, -0.8498,
         -0.8908, -0.9026, -0.8230, -0.8801, -0.7245, -0.7039, -0.8738, -0.8849,
         -1.0135, -0.7365, -0.9410, -0.7869, -0.7661, -0.7009, -0.9838, -0.9982,
         -0.5639, -0.6093, -0.9105, -0.9832, -1.0392, -0.6219, -0.8961, -1.1208,
         -0.9068, -0.9178, -0.9438, -0.6641, -0.9000, -0.9400, -0.7245, -0.8771,
         -0.9123, -0.7962, -0.7984, -0.8457, -0.7274, -0.8773, -0.8763, -0.9634,
         -0.9278, -0.9015]])
[[-0.7391637563705444, -0.8608432412147522, -0.9374753832817078, -1.0253286361694336, -0.7494508028030396, -0.7677640318870544, -0.9283851981163025, -0.7978543043136597, -0.6552786827087402, -0.4484618306159973, -0.7517772912979126, -0.7288979887962341, -0.7165132164955139, -0.9275998473167419, -0.7291557192802429, -0.8498403429985046, -0.8907788991928101, -0.9026300311088562, -0.822984516620636, -0.8801259398460388, -0.7245379090309143, -0.7039464116096497, -0.8737764954566956, -0.8849170804023743, -1.0134750604629517, -0.7365052700042725, -0.9410086870193481, -0.7868703007698059, -0.7661295533180237, -0.7009433507919312, -0.9838077425956726, -0.9982461333274841, -0.5639150142669678, -0.6092903017997742, -0.9104638695716858, -0.9832445979118347, -1.039237380027771, -0.6219194531440735, -0.8960539698600769, -1.1208134889602661, -0.906781017780304, -0.9178304672241211, -0.9437623620033264, -0.6641226410865784, -0.9000487923622131, -0.9400038123130798, -0.7245094180107117, -0.8770883679389954, -0.9123430252075195, -0.7961513996124268, -0.798408567905426, -0.8457246422767639, -0.7273674011230469, -0.8773190379142761, -0.8762651085853577, -0.9633548855781555, -0.9277788996696472, -0.9014847874641418]]
[-0.7391637563705444, -0.8608432412147522, -0.9374753832817078, -1.0253286361694336, -0.7494508028030396, -0.7677640318870544, -0.9283851981163025, -0.7978543043136597, -0.6552786827087402, -0.4484618306159973, -0.7517772912979126, -0.7288979887962341, -0.7165132164955139, -0.9275998473167419, -0.7291557192802429, -0.8498403429985046, -0.8907788991928101, -0.9026300311088562, -0.822984516620636, -0.8801259398460388, -0.7245379090309143, -0.7039464116096497, -0.8737764954566956, -0.8849170804023743, -1.0134750604629517, -0.7365052700042725, -0.9410086870193481, -0.7868703007698059, -0.7661295533180237, -0.7009433507919312, -0.9838077425956726, -0.9982461333274841, -0.5639150142669678, -0.6092903017997742, -0.9104638695716858, -0.9832445979118347, -1.039237380027771, -0.6219194531440735, -0.8960539698600769, -1.1208134889602661, -0.906781017780304, -0.9178304672241211, -0.9437623620033264, -0.6641226410865784, -0.9000487923622131, -0.9400038123130798, -0.7245094180107117, -0.8770883679389954, -0.9123430252075195, -0.7961513996124268, -0.798408567905426, -0.8457246422767639, -0.7273674011230469, -0.8773190379142761, -0.8762651085853577, -0.9633548855781555, -0.9277788996696472, -0.9014847874641418]