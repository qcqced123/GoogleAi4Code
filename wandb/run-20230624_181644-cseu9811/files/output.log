Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|                                                                                                            | 0/111421 [00:00<?, ?it/s]
[1/5] Train & Validation
torch.Size([1, 471, 1024])
[tensor([[-0.5005]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7114]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8926]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.1250]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8452]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4075]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.2676]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1385]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7534]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7124]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2487]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3613]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0070]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0848]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3242]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2285]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6787]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3455]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0219]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6650]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3481]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1569]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3853]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4258]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8701]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2450]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5737]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0541]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5010]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4868]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6318]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6514]], device='cuda:0', dtype=torch.float16,

  0%|                                                                                                 | 1/111421 [00:02<65:16:11,  2.11s/it]
torch.Size([1, 787, 1024])
[tensor([[0.0339]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6567]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1305]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4580]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5649]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7573]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0788]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8647]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6626]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.2227]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4587]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.9624]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0074]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2380]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1346]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.7134]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.2031]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3655]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2247]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5059]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3396]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3384]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3752]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.5547]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>)]
torch.Size([1, 1942, 1024])
[tensor([[-0.4729]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.7158]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3618]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4966]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0174]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4761]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5244]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[0.0540]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.6777]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0279]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.4365]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0751]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[1.2588]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3926]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4805]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6543]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3958]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5615]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0861]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1876]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0060]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.4888]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0946]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2563]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.9517]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4106]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.1787]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.3613]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.1699]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[0.2927]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.7070]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0929]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.8418]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0039]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5601]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4980]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[2.6484]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[2.0508]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3489]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1186]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6270]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4888]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4480]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2233]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3059]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0620]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2452]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.5088]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5381]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.5591]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0543]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1011]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4670]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3936]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0155]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3645]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0515]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2240]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6387]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2734]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6777]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2361]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1708]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7876]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3621]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5483]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3706]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6606]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4592]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7661]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2151]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2197]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.9233]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7432]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4944]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2129]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4399]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6475]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7700]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2001]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3433]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2529]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1652]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7998]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0625]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.5044]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2776]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3018]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0491]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3479]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.9390]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1461]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2930]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1649]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2257]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0881]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4573]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6875]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8057]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1091]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0957]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.0068]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5664]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6543]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0645]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0889]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.9756]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.1934]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.4902]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7720]], device='cuda:0', dtype=torch.float16,

Traceback (most recent call last):                                                                    | 2/111421 [00:02<42:59:05,  1.39s/it]
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 23, in <module>
    main('pairwise_trainer.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 18, in main
    getattr(train_loop, cfg.loop)(cfg)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/train_loop.py", line 34, in train_loop
    train_loss = train_input.train_fn(
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py", line 281, in train_fn
    scaler.scale(loss).backward()
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'AddmmBackward0' returned nan values in its 2th output.