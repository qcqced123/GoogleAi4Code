Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|                                                                                                            | 0/111421 [00:00<?, ?it/s]
[1/5] Train & Validation
torch.Size([1, 471, 1024])
[tensor([[-0.3469]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4934]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6191]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.7793]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5859]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2825]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8789]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0961]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5225]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4939]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1724]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2505]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0049]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0587]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2249]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1584]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4707]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2395]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0153]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4609]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2413]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1086]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2671]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2952]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6030]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1699]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3979]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0375]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3472]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3374]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4380]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4514]], device='cuda:0', dtype=torch.float16,

  0%|                                                                                                 | 2/111421 [00:02<39:30:44,  1.28s/it]
torch.Size([1, 787, 1024])
[tensor([[0.0236]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4556]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0906]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3174]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3916]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5254]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0544]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5996]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4592]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8472]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3181]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6675]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0050]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1652]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0933]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4949]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.8345]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2534]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1556]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3506]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2355]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2347]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2603]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3845]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>)]
torch.Size([1, 1942, 1024])
[tensor([[-0.3279]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4966]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2512]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3438]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0118]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3296]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3635]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[0.0380]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.1631]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0202]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.3027]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0522]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[0.8726]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2722]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3323]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4534]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2742]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3892]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0594]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1298]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0039]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.3389]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0652]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1776]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6602]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2847]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.8169]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.9438]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.8110]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[0.2031]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.1826]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0649]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.2764]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0038]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3879]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3450]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.8359]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[1.4229]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2424]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0826]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.4351]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3386]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3093]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1545]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2119]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0436]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1699]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3530]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3738]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3882]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0375]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0696]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3235]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2729]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0105]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2515]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0383]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1547]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4424]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1890]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4695]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1642]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1184]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5454]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.2507]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3801]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2566]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4570]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3184]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5308]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1490]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1526]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6401]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5156]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3420]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1469]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3049]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4487]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5342]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1389]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2379]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1748]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1147]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5542]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0433]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.3469]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1926]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2100]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0338]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2412]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6509]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1016]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.2034]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.1149]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.1571]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0612]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3176]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4756]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5581]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0746]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.0647]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.6978]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.3904]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.4548]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0448]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.0628]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[0.6768]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.8267]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-1.0332]], device='cuda:0', dtype=torch.float16,
       grad_fn=<AddmmBackward0>), tensor([[-0.5352]], device='cuda:0', dtype=torch.float16,
Traceback (most recent call last):                                                                    | 2/111421 [00:02<39:30:44,  1.28s/it]
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 23, in <module>
    main('pairwise_trainer.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py", line 18, in main
    getattr(train_loop, cfg.loop)(cfg)
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/train_loop.py", line 34, in train_loop
    train_loss = train_input.train_fn(
  File "/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/trainer/trainer.py", line 281, in train_fn
    scaler.scale(loss).backward()
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'AddmmBackward0' returned nan values in its 2th output.