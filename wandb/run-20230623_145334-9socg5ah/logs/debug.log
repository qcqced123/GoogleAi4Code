2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Current SDK version is 0.15.2
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Configure stats pid to 59873
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Loading settings from /home/qcqced/.config/wandb/settings
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Loading settings from /home/qcqced/바탕화면/ML_Test/GoogleAi4Code/wandb/settings
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program': '/home/qcqced/바탕화면/ML_Test/GoogleAi4Code/train.py'}
2023-06-23 14:53:34,884 INFO    MainThread:59873 [wandb_init.py:_log_setup():507] Logging user logs to /home/qcqced/바탕화면/ML_Test/GoogleAi4Code/wandb/run-20230623_145334-9socg5ah/logs/debug.log
2023-06-23 14:53:34,885 INFO    MainThread:59873 [wandb_init.py:_log_setup():508] Logging internal logs to /home/qcqced/바탕화면/ML_Test/GoogleAi4Code/wandb/run-20230623_145334-9socg5ah/logs/debug-internal.log
2023-06-23 14:53:34,885 INFO    MainThread:59873 [wandb_init.py:init():547] calling init triggers
2023-06-23 14:53:34,885 INFO    MainThread:59873 [wandb_init.py:init():554] wandb.init called with sweep_config: {}
config: {'amp_scaler': True, 'anneal_epochs': 1, 'anneal_strategy': 'cos', 'awp': False, 'awp_eps': 0.01, 'awp_lr': 0.0001, 'batch_scheduler': True, 'batch_size': 1, 'betas': [0.9, 0.999], 'cfg_name': 'CFG', 'checkpoint_dir': './saved/model/', 'clipping_grad': True, 'competition': 'GoogleAi4Code', 'dataset': 'PairwiseDataset', 'device': device(type='cuda', index=0), 'epochs': 5, 'freeze': False, 'gpu_id': 0, 'gradient_checkpoint': True, 'init_weight': 'xavier_normal', 'layerwise_adam_epsilon': 1e-06, 'layerwise_lr': 0.0003, 'layerwise_lr_decay': 0.9, 'layerwise_use_bertadam': False, 'layerwise_weight_decay': 0.01, 'llrd': True, 'load_pretrained': False, 'loop': 'train_loop', 'loss_fn': 'MarginRankingLoss', 'margin': 0.5, 'max_grad_norm': 1000, 'max_len': 2048, 'metrics': 'KendallTau', 'model': 'microsoft/deberta-v3-large', 'model_arch': 'PairwiseModel', 'n_folds': 5, 'n_gpu': 1, 'n_gradient_accumulation_steps': 1, 'name': 'PairwiseTrainer', 'nth_awp_start_epoch': 2, 'num_cycles': 1, 'num_freeze': 12, 'num_reinit': 4, 'num_workers': 4, 'optimizer': 'AdamW', 'optuna': False, 'pooling': 'SubSequenceGEMPooling', 'reduction': 'mean', 'reinit': True, 'resume': False, 'scheduler': 'cosine_annealing', 'seed': 42, 'state_dict': '', 'stop_mode': 'max', 'swa': False, 'swa_lr': 5e-06, 'swa_start': 135, 'test': False, 'tokenizer': DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[CD]']}, clean_up_tokenization_spaces=True), 'train': True, 'val_batch_size': 1, 'wandb': True, 'warmup_ratio': 0.1}
2023-06-23 14:53:34,885 INFO    MainThread:59873 [wandb_init.py:init():596] starting backend
2023-06-23 14:53:34,885 INFO    MainThread:59873 [wandb_init.py:init():600] setting up manager
2023-06-23 14:53:34,890 INFO    MainThread:59873 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2023-06-23 14:53:34,893 INFO    MainThread:59873 [wandb_init.py:init():606] backend started and connected
2023-06-23 14:53:34,897 INFO    MainThread:59873 [wandb_init.py:init():700] updated telemetry
2023-06-23 14:53:34,917 INFO    MainThread:59873 [wandb_init.py:init():737] communicating run to backend with 60.0 second timeout
2023-06-23 14:53:35,389 INFO    MainThread:59873 [wandb_run.py:_on_init():2177] communicating current version
2023-06-23 14:53:35,415 INFO    MainThread:59873 [wandb_run.py:_on_init():2186] got version response upgrade_message: "wandb version 0.15.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2023-06-23 14:53:35,415 INFO    MainThread:59873 [wandb_init.py:init():787] starting run threads in backend
2023-06-23 14:53:40,302 INFO    MainThread:59873 [wandb_run.py:_console_start():2158] atexit reg
2023-06-23 14:53:40,302 INFO    MainThread:59873 [wandb_run.py:_redirect():2013] redirect: SettingsConsole.WRAP_RAW
2023-06-23 14:53:40,303 INFO    MainThread:59873 [wandb_run.py:_redirect():2078] Wrapping output streams.
2023-06-23 14:53:40,303 INFO    MainThread:59873 [wandb_run.py:_redirect():2103] Redirects installed.
2023-06-23 14:53:40,303 INFO    MainThread:59873 [wandb_init.py:init():829] run started, returning control to user process
2023-06-23 14:54:03,354 WARNING MsgRouterThr:59873 [router.py:message_loop():77] message_loop has been closed
