{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b94680d-57cb-4c51-b9e7-3c79529d3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3616d6ee-2f48-40e7-b77b-cc1a4762d28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>['1862f0a6', '2a9e43d6', '038b763d', '2eefe0ef...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[0, 2, 4, 6, 8, 10, 14, 16, 18, 20, 22, 24, 25...</td>\n",
       "      <td>['# This Python 3 environment comes with many ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>['c417225b', '51e3cd89', '2600b4eb', '75b65993...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 21, 22,...</td>\n",
       "      <td>['import numpy as np # linear algebra import p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bdd4021779</td>\n",
       "      <td>['3fdc37be', '073782ca', '8ea7263c', '80543cd8...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 8, 12]</td>\n",
       "      <td>['import pandas as pd import numpy as np impor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001daf4c2c76d</td>\n",
       "      <td>['86605076', 'df6c939f', '00f87d0a', '986fd4f1...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[2, 5, 7, 11, 13, 14, 15, 17, 18, 19, 20, 21, ...</td>\n",
       "      <td>['# This Python 3 environment comes with many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002115f48f982</td>\n",
       "      <td>['18281c6c', 'e3b6b115', '4a044c54', '365fe576...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 0]</td>\n",
       "      <td>['import numpy as np # linear algebra import p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139251</th>\n",
       "      <td>fffc30d5a0bc46</td>\n",
       "      <td>['ff1ea6a0', 'a01ce9b3', 'bf92a015', '095812e6...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 12, 13, 15, 16, 17, 18, 19...</td>\n",
       "      <td>[\"import warnings warnings filterwarnings 'ign...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139252</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>['978a5137', 'faa48f03', '28dfb12a', 'eea2e812...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14,...</td>\n",
       "      <td>['import matplotlib pyplot as plt import numpy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139253</th>\n",
       "      <td>fffc63ff750064</td>\n",
       "      <td>['5015c300', '8238198c', 'f4781d1d', 'b5532930...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>[\"import numpy as np import pandas as pd impor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139254</th>\n",
       "      <td>fffcd063cda949</td>\n",
       "      <td>['7e6266ad', 'd8281fc5', '3e0e4a47', '21387fc8...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[0, 1, 3, 4, 6, 7, 8, 10, 12, 14, 16, 17, 18, ...</td>\n",
       "      <td>[\"import os GPU_id 0 os environ 'CUDA_VISIBLE_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139255</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>['9c3b96a5', '1398a873', 'f71c538e', '8b44a5e8...</td>\n",
       "      <td>['code', 'code', 'code', 'code', 'code', 'code...</td>\n",
       "      <td>[1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, ...</td>\n",
       "      <td>[\"import os for dirname _ filenames in os walk...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139256 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                            cell_id  \\\n",
       "0       00001756c60be8  ['1862f0a6', '2a9e43d6', '038b763d', '2eefe0ef...   \n",
       "1       00015c83e2717b  ['c417225b', '51e3cd89', '2600b4eb', '75b65993...   \n",
       "2       0001bdd4021779  ['3fdc37be', '073782ca', '8ea7263c', '80543cd8...   \n",
       "3       0001daf4c2c76d  ['86605076', 'df6c939f', '00f87d0a', '986fd4f1...   \n",
       "4       0002115f48f982  ['18281c6c', 'e3b6b115', '4a044c54', '365fe576...   \n",
       "...                ...                                                ...   \n",
       "139251  fffc30d5a0bc46  ['ff1ea6a0', 'a01ce9b3', 'bf92a015', '095812e6...   \n",
       "139252  fffc3b44869198  ['978a5137', 'faa48f03', '28dfb12a', 'eea2e812...   \n",
       "139253  fffc63ff750064  ['5015c300', '8238198c', 'f4781d1d', 'b5532930...   \n",
       "139254  fffcd063cda949  ['7e6266ad', 'd8281fc5', '3e0e4a47', '21387fc8...   \n",
       "139255  fffe1d764579d5  ['9c3b96a5', '1398a873', 'f71c538e', '8b44a5e8...   \n",
       "\n",
       "                                                cell_type  \\\n",
       "0       ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "1       ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "2       ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "3       ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "4       ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "...                                                   ...   \n",
       "139251  ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "139252  ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "139253  ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "139254  ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "139255  ['code', 'code', 'code', 'code', 'code', 'code...   \n",
       "\n",
       "                                                     rank  \\\n",
       "0       [0, 2, 4, 6, 8, 10, 14, 16, 18, 20, 22, 24, 25...   \n",
       "1       [4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 21, 22,...   \n",
       "2              [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 8, 12]   \n",
       "3       [2, 5, 7, 11, 13, 14, 15, 17, 18, 19, 20, 21, ...   \n",
       "4                             [1, 2, 3, 4, 5, 6, 7, 8, 0]   \n",
       "...                                                   ...   \n",
       "139251  [1, 3, 5, 7, 9, 11, 12, 13, 15, 16, 17, 18, 19...   \n",
       "139252  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14,...   \n",
       "139253  [0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
       "139254  [0, 1, 3, 4, 6, 7, 8, 10, 12, 14, 16, 17, 18, ...   \n",
       "139255  [1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, ...   \n",
       "\n",
       "                                                   source  fold  \n",
       "0       ['# This Python 3 environment comes with many ...     0  \n",
       "1       ['import numpy as np # linear algebra import p...     0  \n",
       "2       ['import pandas as pd import numpy as np impor...     1  \n",
       "3       ['# This Python 3 environment comes with many ...     1  \n",
       "4       ['import numpy as np # linear algebra import p...     2  \n",
       "...                                                   ...   ...  \n",
       "139251  [\"import warnings warnings filterwarnings 'ign...     4  \n",
       "139252  ['import matplotlib pyplot as plt import numpy...     0  \n",
       "139253  [\"import numpy as np import pandas as pd impor...     4  \n",
       "139254  [\"import os GPU_id 0 os environ 'CUDA_VISIBLE_...     4  \n",
       "139255  [\"import os for dirname _ filenames in os walk...     4  \n",
       "\n",
       "[139256 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" load train feature dataframe \"\"\"\n",
    "\n",
    "train_df = pd.read_csv('perfect_final_train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3f5fd9-449b-4ee2-a2f6-25f6ec4f7a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>1862f0a6</td>\n",
       "      <td>code</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2a9e43d6</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np import pandas as pd import ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>import warnings warnings filterwarnings 'ignore'</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2eefe0ef</td>\n",
       "      <td>code</td>\n",
       "      <td>matplotlib rcParams update 'font.size' 14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds train_true_values train_pre...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370642</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>0d770d6b</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## REMOVING THE OUTLIERS</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370643</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>d45ddc62</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### DIMENSIONALITY CURSE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370644</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>1a63248d</td>\n",
       "      <td>markdown</td>\n",
       "      <td># BANGALORE HOUSE PRICE PREDICTION</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370645</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>a8ffc8b4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>* We have achieved 75.2% accuracy in predictin...</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370646</th>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>4e2d4c2d</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## DATA INGESTION</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6370647 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id   cell_id cell_type  \\\n",
       "0        00001756c60be8  1862f0a6      code   \n",
       "1        00001756c60be8  2a9e43d6      code   \n",
       "2        00001756c60be8  038b763d      code   \n",
       "3        00001756c60be8  2eefe0ef      code   \n",
       "4        00001756c60be8  0beab1cd      code   \n",
       "...                 ...       ...       ...   \n",
       "6370642  fffe1d764579d5  0d770d6b  markdown   \n",
       "6370643  fffe1d764579d5  d45ddc62  markdown   \n",
       "6370644  fffe1d764579d5  1a63248d  markdown   \n",
       "6370645  fffe1d764579d5  a8ffc8b4  markdown   \n",
       "6370646  fffe1d764579d5  4e2d4c2d  markdown   \n",
       "\n",
       "                                                    source  rank  fold  \n",
       "0        # This Python 3 environment comes with many he...     0     0  \n",
       "1        import numpy as np import pandas as pd import ...     2     0  \n",
       "2         import warnings warnings filterwarnings 'ignore'     4     0  \n",
       "3                matplotlib rcParams update 'font.size' 14     6     0  \n",
       "4        def evaluate_preds train_true_values train_pre...     8     0  \n",
       "...                                                    ...   ...   ...  \n",
       "6370642                           ## REMOVING THE OUTLIERS    43     4  \n",
       "6370643                          ### DIMENSIONALITY CURSE     33     4  \n",
       "6370644                # BANGALORE HOUSE PRICE PREDICTION      0     4  \n",
       "6370645  * We have achieved 75.2% accuracy in predictin...    69     4  \n",
       "6370646                                 ## DATA INGESTION      3     4  \n",
       "\n",
       "[6370647 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" load full feature dataframe \"\"\"\n",
    "\n",
    "full_df = pd.read_csv('perfect_tmp_train.csv')\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d084c631-2821-463b-8a66-92ca69bee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data Preprocessing Util Function \"\"\"\n",
    "\n",
    "def zero_filtering(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Add eps value for zero embedding, because competition metric is cosine similarity\n",
    "    Cosine Similarity will be returned NaN, when input value has zero\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    x[x == 0] = eps\n",
    "    return x\n",
    "\n",
    "def create_word_normalizer():\n",
    "    \"\"\"\n",
    "    Create a function that normalizes a word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def normalize(word):\n",
    "        w = word.lower()\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        w = ps.stem(w)\n",
    "        return w\n",
    "    return normalize\n",
    "\n",
    "def __normalize_words(titles: list) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a list of words\n",
    "    1) Remove stop words\n",
    "    2) Apply Porter Stemmer, Lemmatizer\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalizer = create_word_normalizer()\n",
    "    titles = [normalizer(t) for t in titles if t not in stop_words]\n",
    "    return titles\n",
    "\n",
    "def normalize_words(words: np.ndarray, unique=True) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a list of words\n",
    "    1) Apply __normalize_word function\n",
    "    2) Apply Regular Expression to remove special characters\n",
    "    \"\"\"\n",
    "    if type(words) is str:\n",
    "        words = [words]\n",
    "    sep_re = r'[\\s\\(\\){}\\[\\];,\\.]+'\n",
    "    num_re = r'\\d'\n",
    "    words = re.split(sep_re, ' '.join(words).lower())\n",
    "    words = [w for w in words if len(w) >= 3 and not re.match(num_re, w)]\n",
    "    if unique:\n",
    "        words = list(set(words))\n",
    "        words = set(__normalize_words(words))\n",
    "    else:\n",
    "        words = __normalize_words(words)\n",
    "    return words\n",
    "\n",
    "def links_to_word(text: str) -> str:\n",
    "    return re.sub(\"https?:\\/\\/[^\\s]+\", \" link \", text)\n",
    "\n",
    "def no_char(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def no_html_tags(text: str) -> str:\n",
    "    return re.sub(\"<.*?>\", \" \", text)\n",
    "\n",
    "def no_multi_spaces(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def underscore_to_space(text: str) -> str:\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "def preprocess_text(source: str) -> str:\n",
    "    # Remove all the special characters\n",
    "    source = re.sub(r'\\W', ' ', str(source))\n",
    "    source = re.sub(r'^b\\s+', '', source)\n",
    "    source = source.lower()\n",
    "    return source\n",
    "\n",
    "def cleaning_words(text: str) -> str:\n",
    "    tmp_text = links_to_word(text)\n",
    "    tmp_text = no_html_tags(tmp_text)\n",
    "    tmp_text = underscore_to_space(tmp_text)\n",
    "    tmp_text = no_char(tmp_text)\n",
    "    tmp_text = preprocess_text(tmp_text)\n",
    "    tmp_text = no_multi_spaces(tmp_text)\n",
    "    return tmp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6893b50-fcf2-4424-bf5a-81c100ec87ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this python 3 environment comes with many helpful analytics libraries installed it is defined by the kaggle python docker image link for example here s several helpful packages to load import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv input data files are available in the read only input directory for example running this by clicking run or pressing shift enter will list all files under the input directory import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename you can write up to 20gb to the current directory kaggle working that gets preserved as output when you create version using save run all you can also write temporary files to kaggle temp but they won t be saved outside of the current session\n",
      "import numpy as np import pandas as pd import random from sklearn model selection import train test split cross val score from sklearn preprocessing import standardscaler robustscaler from catboost import catboostregressor from sklearn ensemble import randomforestregressor from sklearn metrics import r2 score as r2 from sklearn model selection import kfold gridsearchcv from datetime import datetime import matplotlib import matplotlib pyplot as plt import seaborn as sns matplotlib inline\n",
      "import warnings warnings filterwarnings ignore \n",
      "matplotlib rcparams update font size 14\n",
      "def evaluate preds train true values train pred values test true values test pred values print train r2 t str round r2 train true values train pred values 3 print test r2 t str round r2 test true values test pred values 3 plt figure figsize 18 10 plt subplot 121 sns scatterplot train pred values train true values plt xlabel predicted values plt ylabel true values plt title train sample prediction plt subplot 122 sns scatterplot test pred values test true values plt xlabel predicted values plt ylabel true values plt title test sample prediction plt show\n",
      "train dataset path kaggle input real estate price prediction moscow train csv test dataset path kaggle input real estate price prediction moscow test csv \n",
      "train df pd read csv train dataset path train df tail\n",
      "train df dtypes\n",
      "num feat list train df select dtypes exclude object columns obj feat list train df select dtypes include object columns target price num feat\n",
      "test df pd read csv test dataset path test df tail\n",
      "print строк в трейне train df shape 0 print строк в тесте test df shape 0\n",
      "train df shape 1 1 test df shape 1\n",
      "submission df pd read csv kaggle input real estate price prediction moscow sample submission csv \n",
      "train df id train df id astype str\n",
      "train df num feat hist figsize 16 16 plt show\n",
      "train df describe \n",
      "grid sns jointplot train df rooms train df price kind reg grid fig set figwidth 8 grid fig set figheight 8\n",
      "grid sns jointplot train df kitchensquare train df price kind reg grid fig set figwidth 8 grid fig set figheight 8\n",
      "train df temp train df loc train df kitchensquare 250 grid sns jointplot train df temp kitchensquare train df temp price kind reg grid fig set figwidth 8 grid fig set figheight 8\n",
      "plt figure figsize 16 8 train df price hist bins 30 plt ylabel count plt xlabel price plt title target distribution plt show\n",
      "correlation train df corrwith train df price sort values ascending false correlation drop price inplace true plt figure figsize 16 8 plt bar correlation index correlation plt xticks rotation 90 plt xlabel features fontsize 15 plt ylabel correlation fontsize 15 plt title feature correlation fontsize 15 plt show\n",
      "class data def init self константы для обработки выбросов на основе анализа данных self square min 15 self square max 300 self lifesquare min 10 self lifesquare max 280 self rooms min 1 self rooms max 5 self housefloor min 1 self housefloor max 50 self kitchensquare min 3 self kitchensquare max 30 self current year datetime now year self medians none self districtid value counts none self squaremeterprice by districtid none self healthcare 1 by districtid none def fit self train df медианные значения self medians train df lifesquare housefloor median подсчет популярных районов self districtid value counts dict train df districtid value counts подсчет средней цены за м2 по району train df temp train df loc train df square self square min train df square self square max train df temp squaremeterprice train df temp price train df temp square self squaremeterprice by districtid train df temp groupby districtid as index false agg squaremeterprice mean rename columns squaremeterprice averagesquaremeterprice подсчет среднего значения признака healthcare 1 по району self healthcare 1 by districtid train df groupby districtid as index false agg healthcare 1 mean rename columns healthcare 1 averagehealthcare 1 del train df temp def transform self train df обработка пропусков train df lifesquare housefloor train df lifesquare housefloor fillna self medians обработка выбросов площадь train df loc train df square self square max square self square max train df loc train df square self square min square self square min жилая площадь train df loc train df lifesquare self lifesquare min lifesquare self lifesquare min train df loc train df lifesquare self lifesquare max lifesquare self lifesquare max площадь кухни train df loc train df kitchensquare self kitchensquare min kitchensquare self kitchensquare min train df loc train df kitchensquare self kitchensquare max kitchensquare self kitchensquare max год постройки дома train df loc train df houseyear self current year houseyear self current year количество комнат train df loc train df rooms self rooms max rooms self rooms max train df loc train df rooms self rooms min rooms self rooms min количество этажей train df loc train df housefloor self housefloor min housefloor self housefloor min train df loc train df housefloor self housefloor max housefloor self housefloor max если этаж больше этажности дома то присваиваем случайный этаж от self housefloor min до максимального этажа в доме floor outliers train df loc train df floor train df housefloor index train df loc floor outliers floor train df loc floor outliers housefloor apply lambda self housefloor min if self housefloor min else np random randint self housefloor min обработка категорий train df pd concat train df pd get dummies train df ecology 2 prefix ecology 2 dtype int8 axis 1 train df pd concat train df pd get dummies train df ecology 3 prefix ecology 3 dtype int8 axis 1 train df pd concat train df pd get dummies train df shops 2 prefix shops 2 dtype int8 axis 1 return train df def features self train df добавление признака популярности района train df districtid counts train df districtid map self districtid value counts train df districtid counts fillna train df districtid counts median inplace true добавление признака средней стоимости м2 по району train df train df merge self squaremeterprice by districtid on districtid how left train df averagesquaremeterprice fillna train df averagesquaremeterprice median inplace true добавление признака среднего значения healthcare 1 по району train df train df merge self healthcare 1 by districtid on districtid how left train df averagehealthcare 1 fillna train df averagehealthcare 1 median inplace true return train df\n",
      "data inst data тренировочные данные data inst fit train df train df data inst transform train df train df data inst features train df валидационные данные test df data inst transform test df test df data inst features test df\n",
      "feature names averagesquaremeterprice districtid counts rooms square lifesquare kitchensquare floor housefloor houseyear helthcare 2 ecology 1 social 1 social 2 social 3 shops 1 ecology 2 a ecology 2 b ecology 3 a ecology 3 b shops 2 a shops 2 b averagehealthcare 1 target name price \n",
      "train df train df feature names target name test df test df feature names id train df feature names train df target name\n",
      "final model catboostregressor silent true learning rate 0 1 iterations 1150 eval metric r2 depth 8 final model fit y cv score cross val score final model y scoring r2 cv kfold splits 5 shuffle true random state 42\n",
      "print f r2 round cv score mean 3 \n",
      "feature importances pd dataframe zip columns final model get feature importance columns feature name importance feature importances sort values by importance ascending false inplace true feature importances head 20\n",
      "preds final pd dataframe preds final id test df id copy test df set index id inplace true test df test df feature names\n",
      "y pred final final model predict test df submission df price pred final submission df to csv predictions csv index false encoding utf 8 sep submission df head\n",
      " деление признаков на числовые и текстовые \n",
      " сортировка признаков по важности \n",
      "выбросы наблюдаются в houseyear kitchensquare признаки с аномально высоким значением которые нужно будет ограничить housefloor lifesquare rooms square \n",
      " поиск признаков с выбросами \n",
      " выводим сколько строк в тесте и на трейне \n",
      " приведение типов \n",
      " устанавливаем значения чтобы везде был одинаковый шрифт и размер \n",
      " импортируем необходимые для работы функции и классы \n",
      "создаем список признаков используемых в модели отбор признаков\n",
      " обучение модели на catboostregressor вычисления гиперпараметров модели при помощи randomized search learning rate 0 1 iterations 1150 depth 8\n",
      "за выброс посчитаем значения менее 3 кв м и больше 30 кв м \n",
      " на обучении на один признак больше чем на тесте \n",
      " считываем обучающий набор данных \n",
      "корреляция\n",
      "оценка модели\n",
      " подключаем предупреждения \n",
      " график распределения целевой переменной цены \n",
      " загрузка данных \n",
      " создание датафрейма с предсказаниями \n",
      " указываем путь к файлам с данными \n",
      " описание датасета id идентификационный номер квартиры districtid идентификационный номер района rooms количество комнат square площадь lifesquare жилая площадь kitchensquare площадь кухни floor этаж housefloor количество этажей в доме houseyear год постройки дома ecology 1 ecology 2 ecology 3 экологические показатели местности social 1 social 2 social 3 социальные показатели местности healthcare 1 helthcare 2 показатели местности связанные с охраной здоровья shops 1 shops 2 показатели связанные с наличием магазинов торговых центров price цена квартиры\n",
      "создания класса подготовки данных\n",
      " считываем тестовый набор данных \n",
      "значения меньше 1 и больше 250 отсекаем\n",
      " задаем функцию для подсчета метрик \n",
      " тип данных обучающего сета \n",
      "инициализация класса data\n",
      "признаки rooms kitchensquare housefloor имеют в некоторых наблюдениях нулевые значения\n",
      "import numpy as np linear algebra import pandas as pd pd set option display max rows 101 import os print os listdir input import cv2 import json import matplotlib pyplot as plt matplotlib inline plt rcparams font size 15 import seaborn as sns from collections import counter import pil import seaborn as sns from collections import defaultdict from pathlib import path import cv2 import os import sys import random import pandas as pd import numpy as np import matplotlib pyplot as plt matplotlib inline from skimage transform import resize from skimage morphology import label from skimage feature import hog from skimage import exposure from keras preprocessing image import imagedatagenerator array to img img to array load img from skimage feature import canny from skimage filters import sobel from skimage morphology import watershed from scipy import ndimage as ndi import warnings warnings filterwarnings ignore from skimage segmentation import mark boundaries from scipy import signal import cv2 import glob pylab pandas as pd import pydicom numpy as np import tqdm import gc gc enable import glob from skimage transform import resize from skimage morphology import label from skimage feature import hog from skimage import exposure from keras preprocessing image import imagedatagenerator array to img img to array load img from skimage feature import canny from skimage filters import sobel from skimage morphology import watershed from scipy import ndimage as ndi import warnings warnings filterwarnings ignore from skimage segmentation import mark boundaries import sys\n",
      "input dir input \n",
      "train df pd read csv input train csv sample df pd read csv input sample submission csv \n",
      "class dict defaultdict int kind class dict defaultdict int no defects num 0 defects num 0 for col in range 0 len train df 4 img names str split 0 for in train df iloc col col 4 0 values if not img names 0 img names 1 img names 2 img names 3 raise valueerror labels train df iloc col col 4 1 if labels isna all no defects num 1 else defects num 1 kind class dict sum labels isna values false 1 for idx label in enumerate labels isna values tolist if label false class dict idx 1\n",
      "print the number of images with no defects format no defects num print the number of images with defects format defects num\n",
      "fig ax plt subplots sns barplot list class dict keys list class dict values ax ax ax set title the number of images for each class ax set xlabel class class dict\n",
      "fig ax plt subplots sns barplot list kind class dict keys list kind class dict values ax ax ax set title number of classes included in each image ax set xlabel number of classes in the image kind class dict\n",
      "train size dict defaultdict int train path path input train images for img name in train path iterdir img pil image open img name train size dict img size 1\n",
      "train size dict\n",
      "test size dict defaultdict int test path path input test images for img name in test path iterdir img pil image open img name test size dict img size 1\n",
      "test size dict\n",
      "palet 249 192 12 0 185 241 114 0 218 249 50 12\n",
      "def mask2rgba mask rgba list for idx in range 4 rgba cv2 cvtcolor mask idx cv2 color gray2rgba rgba 3 rgba 0 100 rgba 3 rgba 3 palet idx rgba list append rgba return rgba list\n",
      "labels train df iloc 5 5 4 1\n",
      "def name and mask start idx col start idx img names str split 0 for in train df iloc col col 4 0 values if not img names 0 img names 1 img names 2 img names 3 raise valueerror labels train df iloc col col 4 1 mask np zeros 256 1600 4 dtype np uint8 for idx label in enumerate labels values if label is not np nan mask label np zeros 1600 256 dtype np uint8 label label split positions map int label 0 2 length map int label 1 2 for pos le in zip positions length mask label pos pos le 1 mask idx mask label reshape 256 1600 order f return img names 0 mask\n",
      "def show mask image col name mask name and mask col img cv2 imread str train path name fig ax plt subplots figsize 15 15 fig ax plt subplots 2 1 figsize 8 8 for ch in range 4 contours cv2 findcontours mask ch cv2 retr list cv2 chain approx none for in range 0 len contours cv2 polylines img contours true palet ch 2 for ctr in enumerate contours if 0 break get bounding box y h cv2 boundingrect ctr getting roi roi img y x w print roi shape res cv2 resize roi dsize 28 28 interpolation cv2 inter cubic ax 0 set title name ax 0 imshow img ax 1 set title one roi ax 1 imshow res plt show\n",
      "fig ax plt subplots 1 4 figsize 15 5 for in range 4 ax axis off ax imshow np ones 50 50 3 dtype np uint8 palet ax set title class color format 1 fig suptitle each class colors plt show\n",
      "idx no defect idx class 1 idx class 2 idx class 3 idx class 4 idx class multi idx class triple for col in range 0 len train df 4 img names str split 0 for in train df iloc col col 4 0 values if not img names 0 img names 1 img names 2 img names 3 raise valueerror labels train df iloc col col 4 1 if labels isna all idx no defect append col elif labels isna false true true true all idx class 1 append col elif labels isna true false true true all idx class 2 append col elif labels isna true true false true all idx class 3 append col elif labels isna true true true false all idx class 4 append col elif labels isna sum 1 idx class triple append col else idx class multi append col\n",
      "for idx in idx class 1 1 show mask image idx\n",
      "idx class 2 1\n",
      "for idx in idx class 2 1 show mask image idx\n",
      "for idx in idx class 3 1 show mask image idx\n",
      "for idx in idx class 4 1 show mask image idx\n",
      "for idx in idx class multi 4 show mask image idx\n",
      "for idx in idx class triple 1 show mask image idx\n",
      " these are the functions provided by kaggle to convert mask to rle and vice versa import numpy as np def mask2rle img width height rle lastcolor 0 currentpixel 0 runstart 1 runlength 0 for in range width for in range height currentcolor img y if currentcolor lastcolor if currentcolor 127 runstart currentpixel runlength 1 else rle append str runstart rle append str runlength runstart 1 runlength 0 currentpixel 0 elif runstart 1 runlength 1 lastcolor currentcolor currentpixel 1 return join rle def rle2mask rle width height mask np zeros width height astype np uint8 array np asarray int for in rle split starts array 0 2 lengths array 1 2 current position 0 for index start in enumerate starts mask int start int start lengths index 1 current position lengths index return np flipud np rot90 mask reshape height width 1\n",
      "imageid 0002cc93b jpg 1 train df head img cv2 imread input train images imageid split 0 img masks train df loc train df imageid classid imageid encodedpixels tolist p2 p97 np percentile img 2 97 img rescale exposure rescale intensity img in range p2 p97 img ben cv2 addweighted img 4 cv2 gaussianblur img 0 0 10 4 128 take the individual ship masks and create single mask array for all ships all masks np zeros 256 1600 for mask in img masks all masks rle2mask mask 256 1600 fig axarr plt subplots 5 1 figsize 15 15 axarr 0 axis off axarr 1 axis off axarr 2 axis off axarr 3 axis off axarr 0 imshow img axarr 1 imshow all masks axarr 2 imshow img rescale axarr 2 imshow img rescale axarr 3 imshow all masks alpha 0 7 axarr 4 imshow img ben plt tight layout pad 0 4 pad 0 4 plt show\n",
      " take the images with masks train masked df train df train df encodedpixels notnull\n",
      "view count 15 i chk np random randint 0 len data size view count sample imgs ben sample imgs sample imgs label file list input train images format train masked df imageid classid values tolist split 0 for in range view count for in range view count sample img cv2 imread file list sample img cv2 cvtcolor sample img cv2 color bgr2rgb sample imgs append sample img sample imgs label append file list ben sample imgs append cv2 addweighted sample img 4 cv2 gaussianblur sample img 0 0 10 4 128\n",
      " plt imshow ben sample imgs 0 plt imshow cv2 cvtcolor ben sample imgs 8 cv2 color rgb2gray\n",
      "for in range 5 fig ax plt subplots 1 2 figsize 15 15 ax 0 imshow sample imgs ax 1 imshow ben sample imgs plt autoscale tight true axis y ax 0 set title sample imgs label y 1 ax 1 set title str sample imgs label with preprocess 1 ax 0 axis off ax 1 axis off \n",
      "ben sample imgs 0 shape\n",
      "import cv2 cv2 setnumthreads 0 cv2 ocl setuseopencl false import numpy as np import math from scipy ndimage filters import gaussian filter from functools import wraps import torch import torchvision transforms functional as def vflip img return cv2 flip img 0 def hflip img return cv2 flip img 1 def random flip img code return cv2 flip img code def transpose img return img transpose 1 0 2 if len img shape 2 else img transpose 1 0 def rot90 img factor img np rot90 img factor return np ascontiguousarray img def rotate img angle height width img shape 0 2 mat cv2 getrotationmatrix2d width 2 height 2 angle 1 0 img cv2 warpaffine img mat width height flags cv2 inter linear bordermode cv2 border reflect 101 return img def shift scale rotate img angle scale dx dy height width img shape 2 cc math cos angle 180 math pi scale ss math sin angle 180 math pi scale rotate matrix np array cc ss ss cc box0 np array 0 0 width 0 width height 0 height box1 box0 np array width 2 height 2 box1 np dot box1 rotate matrix np array width 2 dx width height 2 dy height box0 box0 astype np float32 box1 box1 astype np float32 mat cv2 getperspectivetransform box0 box1 img cv2 warpperspective img mat width height flags cv2 inter linear bordermode cv2 border reflect 101 return img def center crop img height width w img shape dy height 2 dx width 2 y1 dy y2 y1 height x1 dx x2 x1 width img img y1 y2 x1 x2 return img def clip img dtype maxval return np clip img 0 maxval astype dtype def clipped func wraps func def wrapped function img args kwargs dtype maxval img dtype np max img return clip func img args kwargs dtype maxval return wrapped function def shift hsv img hue shift sat shift val shift dtype img dtype img cv2 cvtcolor img cv2 color rgb2hsv astype np int32 s cv2 split img cv2 add hue shift np where 0 255 h np where 255 255 h astype dtype clip cv2 add sat shift dtype 255 if dtype np uint8 else 1 clip cv2 add val shift dtype 255 if dtype np uint8 else 1 img cv2 merge s astype dtype img cv2 cvtcolor img cv2 color hsv2rgb return img clipped def shift rgb img shift shift shift img 0 img 0 shift img 1 img 1 shift img 2 img 2 shift return img def clahe img cliplimit 2 0 tilegridsize 8 8 img yuv cv2 cvtcolor img cv2 color rgb2lab clahe cv2 createclahe cliplimit cliplimit tilegridsize tilegridsize img yuv 0 clahe apply img yuv 0 img output cv2 cvtcolor img yuv cv2 color lab2rgb return img output def blur img ksize return cv2 blur img ksize ksize def median blur img ksize return cv2 medianblur img ksize def motion blur img ksize kernel np zeros ksize ksize xs ys np random randint 0 kernel shape 1 np random randint 0 kernel shape 0 xe ye np random randint 0 kernel shape 1 np random randint 0 kernel shape 0 cv2 line kernel xs ys xe ye 1 thickness 1 return cv2 filter2d img 1 kernel np sum kernel def random polosa img gray cv2 cvtcolor img cv2 color rgb2gray if np mean gray 100 empty np zeros img shape 2 dtype np uint8 xs ys np random randint 0 empty shape 1 np random randint 0 empty shape 0 xe ye np random randint 0 empty shape 1 np random randint 0 empty shape 0 factor np random randint 1 10 3 cv2 line empty xs ys xe ye np max gray factor thickness np random randint 10 100 empty cv2 blur empty 5 5 empty empty gray return cv2 cvtcolor empty cv2 color gray2rgb return img def distort1 img 0 dx 0 dy 0 unconverntional augmnet 3 link link link link barrel pincushion distortion height width img shape 2 map x map cv2 initundistortrectifymap intrinsics dist coeffs none none width height cv2 cv 32fc1 link link k 0 00001 dx dx width dy dy height y np mgrid 0 width 1 0 height 1 x astype np float32 width 2 dx y astype np float32 height 2 dy theta np arctan2 x x y 0 5 d 1 d map r np cos theta width 2 dx map r np sin theta height 2 dy img cv2 remap img map map interpolation cv2 inter linear bordermode cv2 border reflect 101 return img def distort2 img num steps 10 xsteps ysteps link grid distortion height width img shape 2 step width num steps xx np zeros width np float32 prev 0 for idx in enumerate range 0 width step start end x step if end width end width cur width else cur prev step xsteps idx xx start end np linspace prev cur end start prev cur step height num steps yy np zeros height np float32 prev 0 for idx in enumerate range 0 height step start end y step if end height end height cur height else cur prev step ysteps idx yy start end np linspace prev cur end start prev cur map map np meshgrid xx yy map map astype np float32 map map astype np float32 img cv2 remap img map map interpolation cv2 inter linear bordermode cv2 border reflect 101 return img def elastic transform fast image alpha sigma alpha affine random state none elastic deformation of images as described in simard2003 with modifications simard2003 simard steinkraus and platt best practices for convolutional neural networks applied to visual document analysis in proc of the international conference on document analysis and recognition 2003 based on link if random state is none random state np random randomstate 1234 shape image shape shape size shape 2 random affine center square np float32 shape size 2 square size min shape size 3 alpha float alpha sigma float sigma alpha affine float alpha affine pts1 np float32 center square square size center square 0 square size center square 1 square size center square square size pts2 pts1 random state uniform alpha affine alpha affine size pts1 shape astype np float32 cv2 getaffinetransform pts1 pts2 image cv2 warpaffine image shape size 1 bordermode cv2 border reflect 101 dx np float32 gaussian filter random state rand shape size 2 1 sigma alpha dy np float32 gaussian filter random state rand shape size 2 1 sigma alpha y np meshgrid np arange shape 1 np arange shape 0 mapx np float32 dx mapy np float32 dy return cv2 remap image mapx mapy interpolation cv2 inter linear bordermode cv2 border reflect 101 def remap color img bg center max def get lut img bg center max ma np max img me np mean img th np mean ma me 1 5 th ma 2 gap 10 channels range2 ma int th for in range 3 channels append np linspace bg gap center gap int th astype np uint8 channels append np linspace center gap max gap range2 astype np uint8 channels append max gap 256 sum map len channels channels np hstack channels return np dstack channels img adjust gamma img 5 gray cv2 cvtcolor img cv2 color rgb2gray if np mean gray 100 return img lut get lut img bg center max res cv2 lut img lut astype np uint8 return res def invert img return 255 img def channel shuffle img ch arr 0 1 2 np random shuffle ch arr img img ch arr return img clipped def gauss noise image var row col ch image shape mean var var 30 sigma var 0 5 gauss np random normal mean sigma row col ch gauss gauss reshape row col ch gauss gauss np min gauss astype np uint8 return image astype np int32 gauss def salt pepper noise image todo vs 0 5 amount 0 004 noisy image salt mode num salt np ceil amount image size vs coords np random randint 0 1 int num salt for in image shape noisy coords 255 pepper mode num pepper np ceil amount image size 1 vs coords np random randint 0 1 int num pepper for in image shape noisy coords 0 return noisy def poisson noise image todo vals len np unique image vals 2 np ceil np log2 vals noisy np random poisson image vals float vals return noisy def speckle noise image todo row col ch image shape gauss np random randn row col ch gauss gauss reshape row col ch noisy image image gauss return noisy clipped def random brightness img alpha return alpha img clipped def random contrast img alpha gray cv2 cvtcolor img cv2 color rgb2gray gray 3 0 1 0 alpha gray size np sum gray return alpha img gray def to three channel gray img gray cv2 cvtcolor img cv2 color rgb2gray invgray 255 gray clahe cv2 createclahe cliplimit 2 tilegridsize 8 8 if np mean invgray np mean gray invgray gray gray invgray res invgray gray clahe apply invgray return cv2 merge res def to gray img gray cv2 cvtcolor img cv2 color rgb2gray if np mean gray 127 gray 255 gray return cv2 cvtcolor gray cv2 color gray2rgb def add channel img lab cv2 cvtcolor img cv2 color rgb2lab clahe cv2 createclahe cliplimit 2 0 tilegridsize 21 21 lab clahe apply lab 0 if lab mean 127 lab 255 lab return np dstack img lab def fix mask msk sigmoid false if not sigmoid msk 2 msk 2 127 msk 1 msk 1 127 msk 2 0 msk 0 msk 1 0 msk 2 0 else msk msk 127 return msk astype np uint8 255 def img to tensor im normalize none tensor torch from numpy np moveaxis im 255 if im dtype np uint8 else 1 1 0 astype np float32 if normalize is not none return normalize tensor normalize return tensor def mask to tensor mask num classes sigmoid mask fix mask mask sigmoid if num classes 1 if not sigmoid softmax long mask np zeros mask shape 2 dtype np int64 if len mask shape 3 for in range mask shape 2 long mask mask 0 else long mask mask 127 1 long mask mask 0 0 mask long mask else mask np moveaxis mask 255 if mask dtype np uint8 else 1 1 0 astype np float32 else mask np expand dims mask 255 if mask dtype np uint8 else 1 0 astype np float32 return torch from numpy mask\n",
      " used from xhulu link train df isnan pd isna train df encodedpixels train df imageid train df imageid classid apply lambda x split 0 train df head\n",
      "train nan df train df groupby by imageid axis 0 agg sum train nan df reset index inplace true train nan df rename columns isnan missingcount inplace true train nan df missingcount train nan df missingcount astype np int32 train nan df allmissing train nan df missingcount 4 astype int train nan df head\n",
      "train nan df head\n",
      "train nan df label train nan df allmissing apply lambda 1 if 0 else 0\n",
      "train nan df drop missingcount allmissing axis 1 inplace true\n",
      "train nan df label value counts\n",
      "train df train nan df copy\n",
      "test df pd dataframe\n",
      "train df head\n",
      "test df imageid np array os listdir input test images \n",
      "test df label np array 0 1801\n",
      "x train train df imageid\n",
      "y train train df label\n",
      "x test test df imageid\n",
      "y test test df label\n",
      "from fastai vision import from fastai callbacks import\n",
      "data folder path input \n",
      "def go ben img img img 255 return cv2 addweighted img 4 cv2 gaussianblur img 0 0 10 4 128 go ben tfmpixel go ben\n",
      "test img imagelist from df test df path data folder folder test images trfm get transforms do flip true flip vert true max rotate 10 0 max zoom 1 1 max lighting 0 2 max warp 0 2 affine 0 75 lighting 0 75 xtra tfms contrast scale 0 5 1 1 0 75 go ben p 0 5 \n",
      "train img medium imagelist from df train df path data folder folder train images split by rand pct 0 1 label from df add test test img transform trfm size 256 600 databunch path bs 16 device torch device cuda 0 normalize imagenet stats train img small imagelist from df train df path data folder folder train images split by rand pct 0 1 label from df add test test img transform trfm size 128 400 databunch path bs 16 device torch device cuda 0 normalize imagenet stats\n",
      "train img small show batch rows 3 figsize 12 9\n",
      "from torch import nn import torch nn functional as class focalloss nn module def init self alpha 1 gamma 2 super init self alpha alpha self gamma gamma def forward self inputs targets kwargs ce loss nn crossentropyloss reduction none inputs targets pt torch exp ce loss loss self alpha 1 pt self gamma ce loss return loss mean\n",
      " yet to be implemented learn densenet201 cnn learner train img small models densenet201 metrics accuracy model dir tmp model learn densenet201 loss fn focalloss\n",
      "learn resnet50 cnn learner train img small models resnet50 metrics accuracy model dir tmp model learn resnet50 loss fn focalloss\n",
      "learn resnet50 lr find learn resnet50 recorder plot suggestion true\n",
      "learn resnet50 fit one cycle 5 max lr slice 2e 3\n",
      "learn resnet50 data train img medium learn resnet50 loss fn focalloss learn resnet50 unfreeze learn resnet50 lr find learn resnet50 recorder plot suggestion true\n",
      "learn resnet50 fit one cycle 20 max lr slice 3e 4 callbacks savemodelcallback learn resnet50 every improvement monitor accuracy name best mixup\n",
      "interp classificationinterpretation from learner learn resnet50 interp plot confusion matrix interp plot top losses 9 figsize 15 15 heatmap true\n",
      "preds learn resnet50 tta ds type datasettype test\n",
      "test df prediction preds numpy 0\n",
      "test df head 20\n",
      "os listdir input test images \n",
      "img cv2 imread input test images fc3c8279e jpg 1 img cv2 cvtcolor img cv2 color bgr2rgb plt imshow img\n",
      "test final test df loc test df prediction 0 50\n",
      "test final describe\n",
      "test df\n",
      "test final to csv filtered test no defect csv \n",
      "test final describe\n",
      " how many classes do each image have \n",
      " images with defect contain 3 type label \n",
      " there are similar numbers of images with and without defects class is imbalanced\n",
      " let s visualization masks \n",
      " images with defect label 4 \n",
      " start classification process\n",
      " check image data image size\n",
      " images with defect label 1 \n",
      " images with defect label 2 \n",
      " augmentation functions in one place will use later not now \n",
      " images with normal mask and ben s processing \n",
      " images with defect contain multi label \n",
      " multiple images with ben s preprocessing \n",
      " reference link in the northeastern university neu surface defect database six kinds of typical surface defects of the hot rolled steel strip are collected i e rolled in scale rs patches pa crazing cr pitted surface ps inclusion in and scratches sc at first look it seems for our images 1 class 1 inclusion 2 class 2 pitted 3 class 3 scratches 4 class 4 patches however might be wrong image png attachment image png \n",
      " note this kernel is fork from the amazing kernel below so please upvote the original kernel have started adding few information and preprocessing into this on my own link \n",
      " images with defect label 3 \n",
      " all image have same shape 1600 256 \n",
      " start with binary classification here \n",
      " about the competition detecting steel defect steel is one of the most important building materials of modern times steel buildings are resistant to natural and man made wear which has made the material ubiquitous around the world to help make production of steel more efficient this competition will help identify defects severstal is leading the charge in efficient steel mining and production they believe the future of metallurgy requires development across the economic ecological and social aspects of the industry and they take corporate responsibility seriously the company recently created the country s largest industrial data lake with petabytes of data that were previously discarded severstal is now looking to machine learning to improve automation increase efficiency and maintain high quality in their production the production process of flat sheet steel is especially delicate from heating and rolling to drying and cutting several machines touch flat steel by the time it s ready to ship today severstal uses images from high frequency cameras to power defect detection algorithm in this competition you ll help engineers improve the algorithm by localizing and classifying surface defects on steel sheet if successful you ll help keep manufacturing standards for steel high and enable severstal to continue their innovation leading to stronger more efficient world all around us \n",
      " import modules and define models\n",
      " almost image have no defect or one kind of defect\n",
      "import pandas as pd import numpy as np import matplotlib pyplot as plt import seaborn as sns import warnings as ws ws filterwarnings ignore \n",
      "df pd read csv kaggle input us police shootings shootings csv \n",
      "df head\n",
      "df isna sum\n",
      "sns set sns countplot df gender plt title the gender of killed person plt show\n",
      "avg age of person df age mean print avg age of person\n",
      " status of arming temp df armed value counts sort values ascending false reset index\n",
      "sns barplot index armed data temp temp armed 50 plt show\n",
      "plt figure figsize 20 20 sns barplot index state data df state value counts reset index plt title state wise count for the killing criminals plt show\n",
      "sns countplot df signs of mental illness plt title status of mental illness plt show\n",
      "sns barplot arms category index data df arms category value counts reset index plt show\n",
      "most of them were armed by the gun \n",
      " upvote it if you find it useful \n",
      " this python 3 environment comes with many helpful analytics libraries installed it is defined by the kaggle python docker image link for example here s several helpful packages to load import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv input data files are available in the read only input directory for example running this by clicking run or pressing shift enter will list all files under the input directory import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename you can write up to 5gb to the current directory kaggle working that gets preserved as output when you create version using save run all you can also write temporary files to kaggle temp but they won t be saved outside of the current session\n",
      "import matplotlib as mlp import matplotlib pyplot as plt import numpy as np plot simple sin cos function plt style use classic np linspace 1 10 200 plt plot np sin plt plot np cos plt show plt show starts an event loop looks for all currently active figure objects and opens one or more interactive windows that display your figure or figures \n",
      "matplotlib\n",
      "x np linspace 0 10 100 fig plt figure plt plot np sin plt plot np cos fig savefig my figure png \n",
      "fig canvas get supported filetypes\n",
      " one more way to draw graph plt figure create the first of two panels and set current axis plt subplot 2 1 1 rows columns panel number plt plot np sin create the second panel and set current axis plt subplot 2 1 2 plt plot np cos \n",
      " simple line plots perhaps the simplest of all plots is the visualization of single function f x here we will take first look at creating simple plot of this type as with all the following sections we ll start by setting up the notebook for plotting and importing the func tions we will use matplotlib inline import matplotlib pyplot as plt plt style use seaborn whitegrid import numpy as np for all matplotlib plots we start by creating figure and an axes in their simplest form figure and axes can be created as follows fig plt figure ax plt axis once we have created an axes we can use the ax plot function to plot some data let s start with simple sinusoid np linspace 1 10 1000 np linspace 0 10 2000 plt plot np sin if we want to create single figure with multiple lines we can simply call the plot function multiple times plt plot np sin plt plot np cos plt plot x np tan x \n",
      " adjusting the plot line colors and styles the first adjustment you might wish to make to plot is to control the line colors and styles the plt plot function takes additional arguments that can be used to spec ify these to adjust the color you can use the color keyword which accepts string argument representing virtually any imaginable color the color can be specified in variety of ways plt plot np sin 0 color blue specify color by name plt plot np sin 1 color g short color code rgbcmyk plt plot np sin 2 color 0 75 grayscale between 0 and 1 plt plot np sin 3 color ffdd44 hex code rrggbb from 00 to ff plt plot np sin 4 color 1 0 0 2 0 3 rgb tuple values 0 and 1 plt plot np sin 5 color chartreuse all html color names supported if no color is specified matplotlib will automatically cycle through set of default colors for multiple lines similarly you can adjust the line style using the linestyle keyword plt plot x 0 linestyle solid plt plot x 1 linestyle dashed plt plot x 2 linestyle dashdot plt plot x 3 linestyle dotted for short you can use the following codes plt plot x 4 linestyle solid plt plot x 5 linestyle dashed plt plot x 6 linestyle dashdot plt plot x 7 linestyle dotted\n",
      " if you would like to be extremely terse these linestyle and color codes can be com bined into single nonkeyword argument to the plt plot function plt plot x 0 g solid green x 1 is drawing line here plt plot x 1 c dashed cyan plt plot x 2 k dashdot black plt plot x 3 r dotted red these single character color codes reflect the standard abbreviations in the rgb red green blue and cmyk cyan magenta yellow black color systems com monly used for digital color graphics \n",
      " adjusting the plot axes limits matplotlib does decent job of choosing default axes limits for your plot but some times it s nice to have finer control the most basic way to adjust axis limits is to use the plt xlim and plt ylim methods plt plot np sin plt xlim 0 11 plt ylim 0 1 5\n",
      " if for some reason you d like either axis to be displayed in reverse you can simply reverse the order of the arguments plt plot np sin plt xlim 10 0 plt ylim 1 2 1 2\n",
      " useful related method is plt axis note here the potential confusion between axes with an e and axis with an i the plt axis method allows you to set the and limits with single call by passing list that specifies xmin xmax ymin ymax plt plot np sin plt axis 1 11 0 6\n",
      " the plt axis method goes even beyond this allowing you to do things like auto matically tighten the bounds around the current plot plt plot np sin plt axis tight \n",
      " it allows even higher level specifications such as ensuring an equal aspect ratio so that on your screen one unit in is equal to one unit in plt plot np sin plt axis equal \n",
      " labeling plots we ll briefly look at the labeling of plots titles axis labels and simple legends titles and axis labels are the simplest such labels there are methods that can be used to quickly set them plt plot np sin plt title a sign curve plt xlabel x value plt ylabel sinx value \n",
      " when multiple lines are being shown within single axes it can be useful to create plot legend that labels each line type again matplotlib has built in way of quickly creating such legend it is done via the you guessed it plt legend method plt plot np sin g label sin x plt plot np cos r label cos x plt axis equal plt legend this method is responsible for displaying legend as you can see the plt legend function keeps track of the line style and color and matches these with the correct label more information on specifying and formatting plot legends can be found in the plt legend docstring \n",
      " in the object oriented interface to plotting rather than calling these functions indi vidually it is often more convenient to use the ax set method to set all these prop erties at once ax plt axes ax plot np sin ax set xlim 0 10 ylim 2 2 xlabel x ylabel sin x title a sign curve \n",
      " simple scatter plots another commonly used plot type is the simple scatter plot close cousin of the line plot instead of points being joined by line segments here the points are represented individually with dot circle or other shape matplotlib inline import matplotlib pyplot as plt plt style use seaborn whitegrid import numpy as np np linspace 0 10 30 np sin plt plot y o color black the third argument in the function call is character that represents the type of sym bol used for the plotting just as you can specify options such as and to con trol the line style the marker style has its own set of short string codes \n",
      "rng np random randomstate 0 for marker in o x v s d plt plot rng rand 5 rng rand 5 marker label marker 0 format marker plt legend numpoints 1 plt xlim 0 1 8\n",
      " for even more possibilities these character codes can be used together with line and color codes to plot points along with line connecting them plt plot y ok line circle marker o black k \n",
      " additional keyword arguments to plt plot specify wide range of properties of the lines and markers plt plot y p color gray markersize 15 linewidth 4 markerfacecolor white markeredgecolor gray markeredgewidth 2 plt ylim 1 2 1 2 plt xlim 0 3\n",
      " scatter plots with plt scatter plt scatter y marker o \n",
      " let s show this by creating random scatter plot with points of many colors and sizes rng np random randomstate 0 rng randn 100 rng randn 100 colors rng rand 100 sizes 1000 rng rand 100 plt scatter y colors sizes alpha 0 3 cmap viridis plt colorbar show color scale notice that the color argument is automatically mapped to color scale shown here by the colorbar command and the size argument is given in pixels in this way the color and size of points can be used to convey information in the visualization in order to illustrate multidimensional data \n",
      " we might use the iris data from scikit learn where each sample is one of three types of flowers that has had the size of its petals and sepals carefully measured from sklearn datasets import load iris iris load iris features iris data plt scatter features 0 features 1 alpha 0 2 100 features 3 iris target cmap viridis plt xlabel iris feature names 0 plt ylabel iris feature names 1\n",
      " basic errorbars matplotlib inline import matplotlib pyplot as plt plt style use seaborn whitegrid import numpy as np np linspace 0 10 50 dy 0 9 np sin dy np random randn 50 plt errorbar y yerr dy fmt k here the fmt is format code controlling the appearance of lines and points and has the same syntax as the shorthand used in plt plot\n",
      " in addition to these basic options the errorbar function has many options to finetune the outputs using these additional options you can easily customize the aesthet ics of your errorbar plot plt errorbar y yerr dy fmt o color black ecolor lightgray elinewidth 3 capsize 0\n",
      "matplotlib inline import matplotlib pyplot as plt plt style use seaborn whitegrid import numpy as np visualizing three dimensional function we ll start by demonstrating contour plot using function x y def x return np sin 10 np cos 10 x np cos contour plot can be created with the plt contour function it takes three arguments grid of values grid of values and grid of values the and values represent positions on the plot and the values will be represented by the contour levels np linspace 0 5 60 np linspace 0 5 50 most straightforward way to prepare such data is to use the np meshgrid function which builds two dimensional grids from one dimensional arrays y np meshgrid y f y now let s look at this with standard line only contour plot plt contour y color black notice that by default when single color is used negative values are represented by dashed lines and positive values by solid lines \n",
      "plt contour y 20 cmap rdgy we chose the rdgy short for red gray colormap\n",
      " our plot is looking nicer but the spaces between the lines may be bit distracting we can change this by switching to filled contour plot using the plt contourf function notice the at the end which uses largely the same syntax as plt contour plt contourf y 20 cmap rdgy plt colorbar the colorbar makes it clear that the black regions are peaks while the red regions are valleys \n",
      " better way to handle this is to use the plt imshow function which inter prets two dimensional grid of data as an image plt imshow extent 0 5 0 5 origin lower cmap rdgy plt colorbar plt axis aspect image \n",
      " finally it can sometimes be useful to combine contour plots and image plots for example to create the effect shown in figure 4 34 we ll use partially transparent background image with transparency set via the alpha parameter and over plot contours with labels on the contours themselves using the plt clabel function contours plt contour y 3 color black plt clabel contours inline true fontsize 8 plt imshow extent 0 5 0 5 origin lower cmap rdgy alpha 0 5 plt colorbar the combination of these three functions plt contour plt contourf and plt imshow gives nearly limitless possibilities for displaying this sort of threedimensional data within two dimensional plot \n",
      " simple histogram can be great first step in understanding dataset earlier we saw preview of matplotlib s histogram function matplotlib inline import matplotlib pyplot as plt import numpy as np plt style use seaborn white data np random randn 1000 plt hist data\n",
      " the hist function has many options to tune both the calculation and the display here s an example of more customized histogram plt hist data bins 30 alpha 1 histtype stepfilled color red edgecolor none the plt hist docstring has more information on other customization options avail able find this combination of histtype stepfilled along with some transpar ency alpha to be very useful when comparing histograms of several distributions\n",
      "x1 np random normal 0 0 8 1000 x2 np random normal 1 2 1000 x3 np random normal 3 4 1000 test dict histtype stepfilled alpha 0 3 bins 40 plt hist x1 test plt hist x2 test plt hist x3 test\n",
      " if you would like to simply compute the histogram that is count the number of points in given bin and not display it the np histogram function is available counts bin edges np histogram data bins 5 print counts\n",
      " two dimensional histograms and binnings just as we create histograms in one dimension by dividing the number line into bins we can also create histograms in two dimensions by dividing points among twodimensional bins we ll start by defining some data an and array drawn from multivariate gaussian distribution mean 0 0 cov 1 1 1 2 y np random multivariate normal mean cov 10000 plt hist2d two dimensional histogram one straightforward way to plot two dimensional histogram is to use matplotlib s plt hist2d function plt hist2d y bins 30 cmap blues cb plt colorbar cb set label counts in bin \n",
      "counts xedges yedges np histogram2d y bins 30 for the generalization of this histogram binning in dimensions higher than two see the np histogramdd function\n",
      " plt hexbin hexagonal binnings the two dimensional histogram creates tessellation of squares across the axes another natural shape for such tessellation is the regular hexagon for this purpose matplotlib provides the plt hexbin routine which represents two dimensional dataset binned within grid of hexagons plt hexbin y gridsize 30 cmap blues cb plt colorbar label count in bin plt hexbin has number of interesting options including the ability to specify weights for each point and to change the output in each bin to any numpy aggregate\n",
      " kernel density estimation another common method of evaluating densities in multiple dimensions is kernel density estimation kde one extremely quick and simple kde implementation exists in the scipy stats package here is quick example of using the kde on this data from scipy stats import gaussian kde fit an array of size ndim nsamples data np vstack y kde gaussian kde data evaluate on regular grid xgrid np linspace 3 5 3 5 40 ygrid np linspace 6 6 40 xgrid ygrid np meshgrid xgrid ygrid kde evaluate np vstack xgrid ravel ygrid ravel plot the result as an image plt imshow reshape xgrid shape origin lower aspect auto extent 3 5 3 5 6 6 cmap blues cb plt colorbar cb set label density \n",
      " plot legends give meaning to visualization assigning labels to the various plot ele ments we previously saw how to create simple legend here we ll take look at cus tomizing the placement and aesthetics of the legend in matplotlib import matplotlib pyplot as plt plt style use classic matplotlib inline import numpy as np np linspace 0 10 1000 fig ax plt subplots ax plot np sin b label sine ax plot np cos r label cosine ax axis equal leg ax legend\n",
      " but there are many ways we might want to customize such legend for example we can specify the location and turn off the frame ax legend loc upper left frameon false fig\n",
      " we can use the ncol command to specify the number of columns in the legend ax legend frameon false loc lower center ncol 2 fig\n",
      " we can use rounded box fancybox or add shadow change the transparency alpha value of the frame or change the padding around the text ax legend fancybox true framealpha 1 shadow true borderpad 1 fig\n",
      " choosing elements for the legend the plt plot command is able to create multiple lines at once and returns list of created line instances passing any of these to plt legend will tell it which to identify along with the labels we d like to specify np sin np newaxis np pi np arange 0 2 0 5 lines plt plot y lines is list of plt line2d instances plt legend lines 2 first second \n",
      " generally find in practice that it is clearer to use the first method applying labels to the plot elements you d like to show on the legend plt plot y 0 label first plt plot y 1 label second plt plot y 2 plt legend framealpha 1 frameon true\n",
      " multiple legends creating new legend artist from scratch and then using the lower level ax add artist method to manually add the second artist to the plot fig ax plt subplots lines styles np linspace 0 10 1000 for in range 4 lines ax plot np sin i np pi 2 styles color black ax axis equal specify the lines and labels of the first legend ax legend lines 2 line a line b loc upper right frameon false create the second legend and add the artist manually from matplotlib legend import legend leg legend ax lines 2 line c line d loc lower right frameon false ax add artist leg\n",
      " customizing colorbars import matplotlib pyplot as plt plt style use classic matplotlib inline import numpy as np as we have seen several times throughout this section the simplest colorbar can be created with the plt colorbar function np linspace 0 10 1000 np sin np cos np newaxis plt imshow plt colorbar\n",
      " we can specify the colormap using the cmap argument to the plotting function that is creating the visualization plt imshow cmap gray \n",
      "from matplotlib colors import linearsegmentedcolormap def grayscale cmap cmap return grayscale version of the given colormap cmap plt cm get cmap cmap colors cmap np arange cmap convert rgba to perceived grayscale luminance cf link rgb weight 0 299 0 587 0 114 luminance np sqrt np dot colors 3 2 rgb weight colors 3 luminance np newaxis return linearsegmentedcolormap from list cmap name gray colors cmap \n",
      "def view colormap cmap plot colormap with its grayscale equivalent cmap plt cm get cmap cmap colors cmap np arange cmap cmap grayscale cmap cmap grayscale cmap np arange cmap fig ax plt subplots 2 figsize 6 2 subplot kw dict xticks yticks ax 0 imshow colors extent 0 10 0 1 ax 1 imshow grayscale extent 0 10 0 1 view colormap jet \n",
      " color limits and extensions matplotlib allows for large range of colorbar customization the colorbar itself is simply an instance of plt axes so all of the axes and tick formatting tricks we ve learned are applicable the colorbar has some interesting flexibility for example we can narrow the color limits and indicate the out of bounds values with triangular arrow at the top and bottom by setting the extend property this might come in handy\n",
      " make noise in 1 of the image pixels speckles np random random shape 0 01 speckles np random normal 0 3 np count nonzero speckles plt figure figsize 10 3 5 plt subplot 1 2 1 plt imshow cmap rdbu plt colorbar plt subplot 1 2 2 plt imshow cmap rdbu plt colorbar extend both plt clim 1 1\n",
      " discrete colorbars colormaps are by default continuous but sometimes you d like to represent discrete values the easiest way to do this is to use the plt cm get cmap function and pass the name of suitable colormap along with the number of desired bins plt imshow cmap plt cm get cmap blues 6 plt colorbar plt clim 1 1\n",
      " example handwritten digits load images of the digits 0 through 5 and visualize several of them from sklearn datasets import load digits digits load digits class 6 fig ax plt subplots 8 8 figsize 6 6 for axi in enumerate ax flat axi imshow digits images cmap binary axi set xticks yticks\n",
      " project the digits into 2 dimensions using isomap from sklearn manifold import isomap iso isomap components 2 projection iso fit transform digits data plot the results plt scatter projection 0 projection 1 lw 0 1 digits target cmap plt cm get cmap cubehelix 6 plt colorbar ticks range 6 label digit value plt clim 0 5 5 5\n",
      "matplotlib inline import matplotlib pyplot as plt plt style use seaborn white import numpy as np plt axes subplots by hand ax1 plt axes standard axes ax2 plt axes 0 65 0 65 0 2 0 2\n",
      " the equivalent of this command within the object oriented interface is fig add axes let s use this to create two vertically stacked axes fig plt figure ax1 fig add axes 0 1 0 5 0 8 0 4 xticklabels ylim 1 2 1 2 ax2 fig add axes 0 1 0 1 0 8 0 4 ylim 1 2 1 2 np linspace 0 10 ax1 plot np sin ax2 plot np cos \n",
      " plt subplot simple grids of subplots for in range 1 7 plt subplot 2 3 plt text 0 5 0 5 str 2 3 fontsize 18 ha center \n",
      " the command plt subplots adjust can be used to adjust the spacing between these plots fig plt figure fig subplots adjust hspace 0 4 wspace 0 4 for in range 1 7 ax fig add subplot 2 3 ax text 0 5 0 5 str 2 3 fontsize 18 ha center \n",
      " plt subplots the whole grid in one go here we ll create 2 3 grid of subplots where all axes in the same row share their axis scale and all axes in the same column share their axis scale fig ax plt subplots 2 3 sharex col sharey row \n",
      " axes are in two dimensional array indexed by row col for in range 2 for in range 3 ax j text 0 5 0 5 str j fontsize 18 ha center fig in comparison to plt subplot plt subplots is more consistent with python s conventional 0 based indexing\n",
      " plt gridspec more complicated arrangements grid plt gridspec 2 3 wspace 0 4 hspace 0 4 from this we can specify subplot locations and extents using the familiar python slic ing syntax plt subplot grid 0 0 plt subplot grid 0 1 plt subplot grid 1 2 plt subplot grid 1 2\n",
      " create some normally distributed data create some normally distributed data mean 0 0 cov 1 1 1 2 y np random multivariate normal mean cov 3000 set up the axes with gridspec fig plt figure figsize 6 6 grid plt gridspec 4 4 hspace 0 2 wspace 0 2 main ax fig add subplot grid 1 1 hist fig add subplot grid 1 0 xticklabels sharey main ax hist fig add subplot grid 1 1 yticklabels sharex main ax scatter points on the main axes main ax plot y ok markersize 3 alpha 0 2 histogram on the attached axes hist hist 40 histtype stepfilled orientation vertical color gray hist invert yaxis hist hist 40 histtype stepfilled orientation horizontal color gray hist invert xaxis\n",
      " text and annotation matplotlib inline import matplotlib pyplot as plt import matplotlib as mpl plt style use seaborn whitegrid import numpy as np import pandas as pd\n",
      "fig ax plt subplots facecolor lightgray ax axis 0 10 0 10 transform ax transdata is the default but we ll specify it anyway ax text 1 5 data 1 5 transform ax transdata ax text 0 5 0 1 axes 0 5 0 1 transform ax transaxes ax text 0 2 0 2 figure 0 2 0 2 transform fig transfigure ax set ylim 6 6 ax set xlim 0 2 fig\n",
      " arrows and annotation using the plt annotate function this function creates some text and an arrow and the arrows can be very flexibly specified matplotlib inline import matplotlib pyplot as plt fig ax plt subplots np linspace 0 20 1000 ax plot np cos ax axis equal ax annotate local maximum xy 6 28 1 xytext 10 4 arrowprops dict facecolor red shrink 5 05 ax annotate local minimum xy 5 np pi 1 xytext 2 6 arrowprops dict arrowstyle connectionstyle angle3 anglea 0 angleb 90 \n",
      " the arrow style is controlled through the arrowprops dictionary which has numerous options available matplotlib inline import matplotlib pyplot as plt fig ax plt subplots np linspace 0 20 2000 ax plot np cos ax axis equal \n",
      " customizing ticks major and minor ticks matplotlib inline import matplotlib pyplot as plt plt style use seaborn whitegrid import numpy as np ax plt axes xscale log yscale log print ax xaxis get major locator print ax xaxis get minor locator\n",
      "print ax xaxis get major formatter print ax xaxis get minor formatter\n",
      " hiding ticks or labels the most common tick label formatting operation is the act of hiding ticks or labels we can do this using plt nulllocator and plt nullformatter ax plt axes ax plot np random rand 50 ax yaxis set major locator plt nulllocator ax xaxis set major formatter plt nullformatter\n",
      "fig ax plt subplots 5 5 figsize 5 5 fig subplots adjust hspace 0 wspace 0 get some face data from scikit learn from sklearn datasets import fetch olivetti faces faces fetch olivetti faces images for in range 5 for in range 5 ax j xaxis set major locator plt nulllocator ax j yaxis set major locator plt nulllocator ax j imshow faces 10 j cmap bone \n",
      " reducing or increasing the number of ticks fig ax plt subplots 4 4 sharex true sharey true\n",
      " plt maxnlocator which allows us to specify the maximum number of ticks that will be displayed for every axis set the and major locator for axi in ax flat axi xaxis set major locator plt maxnlocator 3 axi yaxis set major locator plt maxnlocator 3 fig\n",
      " fancy tick formats plot sine and cosine curve fig ax plt subplots np linspace 0 3 np pi 1000 ax plot np sin lw 3 label sine ax plot np cos lw 3 label cosine set up grid legend and limits ax grid true ax legend frameon false ax axis equal ax set xlim 0 3 np pi ax xaxis set major locator plt multiplelocator np pi 2 ax xaxis set minor locator plt multiplelocator np pi 4 fig\n",
      "def format func value tick number find number of multiples of pi 2 we ll instead use plt funcformatter which accepts user defined function giving fine grained control over the tick outputs int np round 2 value np pi if 0 return 0 elif 1 return r pi 2 elif 2 return r pi elif 2 0 return r 0 pi 2 format else return r 0 pi format 2 ax xaxis set major formatter plt funcformatter format func fig\n",
      " plot customization by hand import matplotlib pyplot as plt plt style use classic import numpy as np matplotlib inline np random randn 1000 plt hist we can adjust this by hand to make it much more visually pleasing plot draw solid white grid lines plt grid color w linestyle solid hide axis spines for spine in ax spines values spine set visible false\n",
      " changing the defaults rcparams we ll start by saving copy of the current rcparams dictionary so we can easily reset these changes in the current session ipython default plt rcparams copy now we can use the plt rc function to change some of these settings from matplotlib import cycler colors cycler color ee6666 3388bb 9988dd eecc55 88bb44 ffbbbb plt rc axes facecolor e6e6e6 edgecolor none axisbelow true grid true prop cycle colors plt rc grid color w linestyle solid plt rc xtick direction out color gray plt rc ytick direction out color gray plt rc patch edgecolor e6e6e6 plt rc lines linewidth 2 plt hist \n",
      "for in range 4 plt plot np random rand 10\n",
      " stylesheets the available styles are listed in plt style available plt style available 5 the basic way to switch to stylesheet is to call\n",
      " let s create function that will make two basic types of plot def hist and lines np random seed 0 fig ax plt subplots 1 2 figsize 11 4 ax 0 hist np random randn 1000 for in range 3 ax 1 plot np random rand 10 ax 1 legend a b c loc lower left \n",
      " default style reset rcparams hist and lines\n",
      " fivethirtyeight style with plt style context fivethirtyeight hist and lines\n",
      " ggplot with plt style context ggplot hist and lines\n",
      " bayesian methods for hackers style with plt style context bmh hist and lines\n",
      " dark background with plt style context dark background hist and lines\n",
      " grayscale with plt style context grayscale hist and lines\n",
      " seaborn style import seaborn hist and lines\n",
      " we enable three dimensional plots by importing the mplot3d toolkit from mpl toolkits import mplot3d once this submodule is imported we can create three dimensional axes by passing the keyword projection 3d to any of the normal axes creation routines matplotlib inline import numpy as np import matplotlib pyplot as plt fig plt figure ax plt axes projection 3d \n",
      " three dimensional points and lines we can create these using the ax plot3d and ax scatter3d functions ax plt axes projection 3d data for three dimensional line zline np linspace 0 15 1000 yline np cos zline xline np sin zline ax plot3d xline yline zline red data for three dimensional scattered points zdata 15 np random random 100 xdata np sin zdata 0 1 np random randn 100 ydata np cos zdata 0 1 np random randn 100 ax scatter3d xdata ydata zdata zdata cmap greens \n",
      " three dimensional contour plots def x return np sin np sqrt 2 2 np linspace 6 6 30 np linspace 6 6 30 y np meshgrid y f y fig plt figure ax plt axes projection 3d ax contour3d y 50 cmap binary ax set xlabel x ax set ylabel y ax set zlabel z sometimes the default viewing angle is not optimal in which case we can use the view init method to set the elevation and azimuthal angles ax view init 60 35 fig\n",
      " wireframes and surface plots fig plt figure ax plt axes projection 3d ax plot wireframe y color black ax set title wireframe \n",
      " surface plot is like wireframe plot but each face of the wireframe is filled poly gon adding colormap to the filled polygons can aid perception of the topology of the surface being visualized ax plt axes projection 3d ax plot surface y rstride 1 cstride 1 cmap viridis edgecolor none ax set title surface \n",
      "r np linspace 0 6 20 theta np linspace 0 9 np pi 0 8 np pi 40 theta np meshgrid theta r np sin theta r np cos theta f y ax plt axes projection 3d ax plot surface y rstride 1 cstride 1 cmap viridis edgecolor none \n",
      " surface triangulations theta 2 np pi np random random 1000 6 np random random 1000 np ravel np sin theta np ravel np cos theta f y we could create scatter plot of the points to get an idea of the surface we re sampling from ax plt axes projection 3d ax scatter y c cmap viridis linewidth 0 5\n",
      " the function that will help us in this case is ax plot trisurf which creates surface by first finding set of triangles formed between adjacent points ax plt axes projection 3d ax plot trisurf y cmap viridis edgecolor none \n",
      " example visualizing möbius strip theta np linspace 0 2 np pi 30 np linspace 0 25 0 25 8 theta np meshgrid theta phi 0 5 theta radius in y plane 1 np cos phi np ravel np cos theta np ravel np sin theta np ravel np sin phi\n",
      " triangulate in the underlying parameterization from matplotlib tri import triangulation tri triangulation np ravel np ravel theta ax plt axes projection 3d ax plot trisurf y triangles tri triangles cmap viridis linewidths 0 2 ax set xlim 1 1 ax set ylim 1 1 ax set zlim 1 1\n",
      " seaborn versus matplotlib import matplotlib pyplot as plt plt style use classic matplotlib inline import numpy as np import pandas as pd now we create some random walk data create some data rng np random randomstate 0 np linspace 0 10 500 np cumsum rng randn 500 6 0 plot the data with matplotlib defaults plt plot y plt legend abcdef ncol 2 loc upper left \n",
      "import seaborn as sns sns set now let s rerun the same two lines as before plt plot y plt legend abcdef ncol 2 loc upper left \n",
      " histograms kde and densities often in statistical data visualization all you want is to plot histograms and joint dis tributions of variables data np random multivariate normal 0 0 5 2 2 2 size 2000 data pd dataframe data columns x y for col in xy plt hist data col alpha 0 5\n",
      " rather than histogram we can get smooth estimate of the distribution using kernel density estimation which seaborn does with sns kdeplot for col in xy sns kdeplot data col shade true\n",
      " histograms and kde can be combined using distplot sns distplot data x sns distplot data y \n",
      " if we pass the full two dimensional dataset to kdeplot we will get two dimensional visualization of the data sns kdeplot data\n",
      " we can see the joint distribution and the marginal distributions together using sns jointplot for this plot we ll set the style to white background with sns axes style white sns jointplot x y data kind kde \n",
      " there are other parameters that can be passed to jointplot for example we can use hexagonally based histogram instead with sns axes style white sns jointplot x y data kind hex \n",
      " pair plots iris sns load dataset iris iris head\n",
      " visualizing the multidimensional relationships among the samples is as easy as call ing sns pairplot sns pairplot iris hue species size 2 5\n",
      " faceted histograms tips sns load dataset tips tips head\n",
      "tips tip pct 100 tips tip tips total bill grid sns facetgrid tips row sex col time margin titles true grid map plt hist tip pct bins np linspace 0 40 15\n",
      " factor plots with sns axes style style ticks sns factorplot day total bill sex data tips kind box set axis labels day total bill \n",
      " joint distributions similar to the pair plot we saw earlier we can use sns jointplot to show the jointdistribution between different datasets along with the associated marginal distribu tions with sns axes style white sns jointplot total bill tip data tips kind hex \n",
      " the joint plot can even do some automatic kernel density estimation and regression sns jointplot total bill tip data tips kind reg \n",
      " bar plots time series can be plotted with sns factorplot planets sns load dataset planets planets head\n",
      "with sns axes style white sns factorplot year data planets aspect 2 kind count color steelblue set xticklabels step 5\n",
      "with sns axes style white sns factorplot year data planets aspect 4 0 kind count hue method order range 2001 2015 set ylabels number of planets discovered \n",
      " import plotly import plotly as py from plotly offline import init notebook mode iplot plot init notebook mode connected true import plotly graph objs as go word cloud library from wordcloud import wordcloud\n",
      " pip install plotly 3 10 0\n",
      " read data from input files for seaborn plots import numpy as np import csv as csv import pandas as pd median house hold in come pd read csv kaggle input fatalpoliceshootingsintheus medianhouseholdincome2015 csv encoding windows 1252 percentage people below poverty level pd read csv kaggle input fatalpoliceshootingsintheus percentagepeoplebelowpovertylevel csv encoding windows 1252 percent over 25 completed highschool pd read csv kaggle input fatalpoliceshootingsintheus percentover25completedhighschool csv encoding windows 1252 share race city pd read csv kaggle input fatalpoliceshootingsintheus shareracebycity csv shareracebycity csv encoding windows 1252 kill pd read csv kaggle input fatalpoliceshootingsintheus policekillingsus csv encoding windows 1252 \n",
      "median house hold in come head 20\n",
      "percentage people below poverty level head 20\n",
      "percent over 25 completed highschool head 20\n",
      "share race city head 20\n",
      "kill head 20\n",
      "percentage people below poverty level geographic area unique\n",
      " poverty rate of each state percentage people below poverty level replace 0 0 inplace true percentage people below poverty level poverty rate percentage people below poverty level poverty rate astype float area list list percentage people below poverty level geographic area unique area poverty ratio for in area list percentage people below poverty level percentage people below poverty level geographic area area poverty rate sum poverty rate len area poverty ratio append area poverty rate data pd dataframe area list area list area poverty ratio area poverty ratio new index data area poverty ratio sort values ascending false index values sorted data data reindex new index visualization plt figure figsize 15 10 sns barplot sorted data area list sorted data area poverty ratio plt xticks rotation 45 plt xlabel states plt ylabel poverty rate plt title poverty rate given states \n",
      "kill head\n",
      "kill name value counts\n",
      "percent over 25 completed highschool info\n",
      " high school graduation rate of the population that is older than 25 in states percent over 25 completed highschool percent completed hs replace 0 0 inplace true percent over 25 completed highschool percent completed hs percent over 25 completed highschool percent completed hs astype float area list list percent over 25 completed highschool geographic area unique area highschool for in area list percent over 25 completed highschool percent over 25 completed highschool geographic area area highschool rate sum percent completed hs len area highschool append area highschool rate sorting data pd dataframe area list area list area highschool ratio area highschool new index data area highschool ratio sort values ascending true index values sorted data2 data reindex new index visualization plt figure figsize 15 10 sns barplot sorted data2 area list sorted data2 area highschool ratio plt xticks rotation 90 plt xlabel states plt ylabel high school graduate rate plt title percentage of given state s population above 25 that has graduated high school \n",
      "percentage people below poverty level head\n",
      "percentage people below poverty level info\n",
      "percentage people below poverty level geographic area unique\n",
      "share race city head\n",
      " percentage of state s population according to races that are black white native american asian and hispanic share race city replace 0 0 inplace true share race city replace x 0 0 inplace true share race city loc share white share black share native american share asian share hispanic share race city loc share white share black share native american share asian share hispanic astype float area list list share race city geographic area unique share white share black share native american share asian share hispanic for in area list share race city share race city geographic area share white append sum share white len share black append sum share black len share native american append sum share native american len share asian append sum share asian len share hispanic append sum share hispanic len visualization ax plt subplots figsize 9 15 sns barplot share white area list color green alpha 0 5 label white sns barplot share black area list color blue alpha 0 7 label african american sns barplot share native american area list color cyan alpha 0 6 label native american sns barplot share asian area list color yellow alpha 0 6 label asian sns barplot share hispanic area list color red alpha 0 6 label hispanic ax legend loc lower right frameon true ax set xlabel percentage of races ylabel states title percentage of state s population according to races \n",
      " high school graduation rate vs poverty rate of each state sorted data area poverty ratio sorted data area poverty ratio max sorted data area poverty ratio sorted data2 area highschool ratio sorted data2 area highschool ratio max sorted data2 area highschool ratio data pd concat sorted data sorted data2 area highschool ratio axis 1 data sort values area poverty ratio inplace true visualize ax1 plt subplots figsize 20 10 sns pointplot area list area poverty ratio data data color lime alpha 0 8 sns pointplot area list area highschool ratio data data color red alpha 0 8 plt text 40 0 6 high school graduate ratio color red fontsize 17 style italic plt text 40 0 55 poverty ratio color lime fontsize 18 style italic plt xlabel states fontsize 15 color blue plt ylabel values fontsize 15 color blue plt title high school graduate vs poverty rate fontsize 20 color blue plt grid\n",
      "data head\n",
      " visualization of high school graduation rate vs poverty rate of each state with different style of seaborn code pearsonr if it is 1 there is positive correlation and if it is 1 there is negative correlation if it is zero there is no correlation between variables show the joint distribution using kernel density estimation sns jointplot data area poverty ratio data area highschool ratio plt savefig graph png plt show\n",
      "data head\n",
      " you can change parameters of joint plot kind scatter reg resid kde hex different usage of parameters but same plot with previous one sns jointplot area poverty ratio area highschool ratio data data size 5 ratio 3 color r \n",
      "kill race head\n",
      "kill race value counts\n",
      " race rates according in kill data kill race dropna inplace true labels kill race value counts index colors grey blue red yellow green brown explode 0 0 0 0 0 0 sizes kill race value counts values visual plt figure figsize 7 7 plt pie sizes explode explode labels labels colors colors autopct 1 1f plt title killed people according to races color blue fontsize 15\n",
      "data head\n",
      " visualization of high school graduation rate vs poverty rate of each state with different style of seaborn code lmplot show the results of linear regression within each dataset sns lmplot area poverty ratio area highschool ratio data data plt show\n",
      "data head\n",
      " visualization of high school graduation rate vs poverty rate of each state with different style of seaborn code cubehelix plot sns kdeplot data area poverty ratio data area highschool ratio shade true cut 3 plt show\n",
      "data head\n",
      " show each distribution with both violins and points use cubehelix to get custom sequential palette pal sns cubehelix palette 2 rot 6 dark 4 sns violinplot data data palette pal inner points plt show\n",
      "data corr\n",
      " correlation map visualization of high school graduation rate vs poverty rate of each state with different style of seaborn code ax plt subplots figsize 5 10 sns heatmap data corr annot true linewidths 0 8 linecolor blue fmt 1f ax ax plt show\n",
      "kill head\n",
      "kill manner of death unique\n",
      "sns boxplot gender age hue manner of death data kill palette prgn plt show\n",
      "sns swarmplot gender age hue manner of death data kill plt show\n",
      "data head\n",
      "sns pairplot data plt show\n",
      "kill gender value counts\n",
      "kill head\n",
      " kill properties manner of death sns countplot kill gender\n",
      " kill weapon armed kill armed value counts print armed plt figure figsize 10 7 sns barplot armed 7 index armed 7 values plt ylabel number of weapon plt xlabel weapon types plt title kill weapon color blue fontsize 15\n",
      " race of killed people kill race value counts sns countplot data kill race plt title race of killed people color blue fontsize 15\n",
      " most dangerous cities city kill city value counts plt figure figsize 12 8 sns barplot city 12 index city 12 values plt xticks rotation 60 plt title most dangerous cities color red fontsize 20\n",
      " most dangerous states state kill state value counts plt figure figsize 10 7 sns barplot state 20 index state 20 values plt title most dangerous state color blue fontsize 15\n",
      " plotly import plotly plotly as py from plotly offline import init notebook mode iplot plot import plotly as py init notebook mode connected true import plotly graph objs as go word cloud library from wordcloud import wordcloud\n",
      " read data from input files for plotly plots import numpy as np import csv as csv import pandas as pd educational attainment supplementary data pd read csv kaggle input worlduniversityrankings data educational attainment supplementary data csv cwurdata pd read csv kaggle input worlduniversityrankings data cwurdata csv education expenditure supplementary data pd read csv kaggle input worlduniversityrankings data education expenditure supplementary data csv school and country table pd read csv kaggle input worlduniversityrankings data school and country table csv shanghaidata pd read csv kaggle input worlduniversityrankings data shanghaidata csv timesdata pd read csv kaggle input worlduniversityrankings data timesdata csv \n",
      "timesdata head 20\n",
      "timesdata info\n",
      " prepare data frame df timesdata iloc 100 import graph objects as go import plotly graph objs as go creating trace1 trace1 go scatter df world rank df citations mode lines name citations marker dict color rgba 16 112 2 0 8 text df university name creating trace2 trace2 go scatter df world rank df teaching mode lines markers name teaching marker dict color rgba 80 26 80 0 8 text df university name data trace1 trace2 layout dict title citation and teaching vs world rank of top 100 universities xaxis dict title world rank ticklen 5 zeroline false fig dict data data layout layout iplot fig\n",
      " prepare data frames df2014 timesdata timesdata year 2014 iloc 100 df2015 timesdata timesdata year 2015 iloc 100 df2016 timesdata timesdata year 2016 iloc 100 import graph objects as go import plotly graph objs as go creating trace1 trace1 go scatter df2014 world rank df2014 citations mode markers name 2014 marker dict color rgba 255 128 255 0 8 text df2014 university name creating trace2 trace2 go scatter df2015 world rank df2015 citations mode markers name 2015 marker dict color rgba 255 128 2 0 8 text df2015 university name creating trace3 trace3 go scatter df2016 world rank df2016 citations mode markers name 2016 marker dict color rgba 0 255 200 0 8 text df2016 university name data trace1 trace2 trace3 layout dict title citation vs world rank of top 100 universities with 2014 2015 and 2016 years xaxis dict title world rank ticklen 5 zeroline false yaxis dict title citation ticklen 5 zeroline false fig dict data data layout layout iplot fig\n",
      " prepare data frames df2014 timesdata timesdata year 2014 iloc 3 df2014\n",
      " prepare data frames df2014 timesdata timesdata year 2014 iloc 3 import graph objects as go import plotly graph objs as go create trace1 trace1 go bar df2014 university name df2014 citations name citations marker dict color rgba 255 174 255 0 5 line dict color rgb 0 0 0 width 1 5 text df2014 country create trace2 trace2 go bar df2014 university name df2014 teaching name teaching marker dict color rgba 255 255 128 0 5 line dict color rgb 0 0 0 width 1 5 text df2014 country data trace1 trace2 layout go layout barmode group fig go figure data data layout layout iplot fig\n",
      " data preparation df2016 timesdata timesdata year 2016 iloc 7 pie1 df2016 num students pie1 list float each replace for each in df2016 num students str 2 4 str 2 4 float 2 4 2 4 labels df2016 university name figure fig data values pie1 list labels labels domain x 0 5 name number of students rates hoverinfo label percent name hole 3 type pie layout title universities number of students rates annotations font size 20 showarrow false text number of students x 0 20 y 1 iplot fig\n",
      "df2016 info\n",
      " data preparation df2016 timesdata timesdata year 2016 iloc 20 num students size float each replace for each in df2016 num students international color float each for each in df2016 international data y df2016 teaching x df2016 world rank mode markers marker color international color size num students size showscale true text df2016 university name iplot data\n",
      " prepare data x2011 timesdata student staff ratio timesdata year 2011 x2012 timesdata student staff ratio timesdata year 2012 trace1 go histogram x2011 opacity 0 75 name 2011 marker dict color rgba 171 50 96 0 6 trace2 go histogram x2012 opacity 0 75 name 2012 marker dict color rgba 12 50 196 0 6 data trace1 trace2 layout go layout barmode overlay title students staff ratio in 2011 and 2012 xaxis dict title students staff ratio yaxis dict title count fig go figure data data layout layout iplot fig\n",
      " data prepararion x2011 timesdata country timesdata year 2011 plt subplots figsize 8 8 wordcloud wordcloud background color white width 512 height 384 generate join x2011 plt imshow wordcloud plt axis off plt savefig graph png plt show\n",
      " data preparation x2015 timesdata timesdata year 2015 trace0 go box x2015 total score name total score of universities in 2015 marker dict color rgb 12 12 140 trace1 go box x2015 research name research of universities in 2015 marker dict color rgb 12 128 128 data trace0 trace1 iplot data\n",
      " import figure factory import plotly figure factory as ff prepare data dataframe timesdata timesdata year 2015 data2015 dataframe loc research international total score data2015 index np arange 1 len data2015 1 scatter matrix fig ff create scatterplotmatrix data2015 diag box index index colormap portland colormap type cat height 750 width 750 iplot fig\n",
      " table of contents section 1 importing matplotlib classic graph section one section 2 loading from script section two section 3 adjusting the plot line colors and styles section three section 4 simple scatter plots section four section 5 visualizing errors density and contour plots section five section 6 histograms binnings and density section six section 7 customizing plot legends section seven section 8 multiple subplots section eight section 9 multiple plots section nine section 10 text annotation text position arrow position section ten section 11 customizing matplotlib configurations and stylesheets section eleven section 12 three dimensional plotting in matplotlib section twelve section 13 visualization with seaborn section thirteen section 14 visualization with plotly section fourteen section 15 read data from input files for seaborn plots section fifteen section 16 bar plot using seaborn section sixteen section 17 point plot using seaborn section seventeen section 18 joint plot using seaborn section eighteen section 19 pie plot using seaborn section ninteen section 20 lm plot using seaborn section twenty section 21 kde plot using seaborn section twentyone section 22 violin plot using seaborn section twentytwo section 23 heatmap section twentythree section 24 box plot section twentyfour section 25 swarm plot section twentyfive section 26 pair plot section twentysix section 27 count plot section twentyseven section 28 read data from input files for plotly plots section twentyeight section 29 line charts plotly plots section twentynine section 30 scatter charts plotly plots section thirty section 31 bar charts plotly plots section thirtyone section 32 pie charts plotly plots section thirtytwo section 33 bubble charts plotly plots section thirtythree section 34 histogram plotly plots section thirtyfour section 35 word cloud plotly plots section thirtyfive section 36 box plots plotly plots section thirtysix section 37 scatter matrix plotly plots section thirtyseven \n",
      "now let s take look at how it works with seaborn as we will see seaborn has many of its own high level plotting routines but it can also overwrite matplotlib s default parameters and in turn get even simple matplotlib scripts to produce vastly superior output we can set the style by calling seaborn s set method by convention sea born is imported as sns\n",
      " section 16 bar plot using seaborn\n",
      " section 26 pair plot\n",
      " section 24 box plot\n",
      "sometimes it is useful to display three dimensional data in two dimensions using contours or color coded regions there are three matplotlib functions that can be helpful for this task plt contour for contour plots plt contourf for filled contour plots and plt imshow for showing images \n",
      " section 15 read data from input files for seaborn plots\n",
      " section 13 visualization with seaborn\n",
      " section 8 multiple subplots \n",
      "the most basic method of creating an axes is to use the plt axes function as we ve seen previously by default this creates standard axes object that fills the entire fig ure plt axes also takes an optional argument that is list of four numbers in the figure coordinate system these numbers represent bottom left width height in the figure coordinate system which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure for example we might create an inset axes at the top right corner of another axes by setting the and position to 0 65 that is starting at 65 of the width and 65 of the height of the figure and the and extents to 0 2 that is the size of the axes is 20 of the width and 20 of the height of the figure \n",
      " section 19 pie plot using seaborn\n",
      " section 3 adjusting the plot line colors and styles \n",
      " section 31 bar charts plotly plots\n",
      " section 32 pie charts plotly plots\n",
      " section 20 lm plot using seaborn\n",
      " section 23 heatmap\n",
      " section 9 multiple plots\n",
      " section 34 histogram plotly plots\n",
      " section 37 scatter matrix plotly plots\n",
      " section 21 kde plot using seaborn\n",
      " section 7 customizing plot legends\n",
      "there are few potential gotchas with imshow however plt imshow doesn t accept an and grid so you must manually specify the extent xmin xmax ymin ymax of the image on the plot plt imshow by default follows the standard image array definition where the origin is in the upper left not in the lower left as in most contour plots this must be changed when showing gridded data plt imshow will automatically adjust the axis aspect ratio to match the input data you can change this by setting for example plt axis aspect image to make and units match \n",
      " section 35 word cloud plotly plots\n",
      " section 4 simple scatter plots\n",
      " section 27 count plot\n",
      " section 6 histograms binnings and density\n",
      " section 22 violin plot using seaborn\n",
      "plotting from an ipython shell it can be very convenient to use matplotlib interactively within an ipython shell ipython is built to work well with matplotlib if you specify matplotlib mode to enable this mode you can use the matplotlib magic command after start ing ipython \n",
      " section 25 swarm plot\n",
      " section 1 importing matplotlib classic graph\n",
      " section 17 point plot using seaborn\n",
      " comprehensive guide to matplotlib seaborn plotly matplotlib is multiplatform data visualization library built on numpy arrays and designed to work with the broader scipy stack one of matplotlib s most important features is its ability to play well with many operating systems and graphics backends matplotlib supports dozens of backends and output types which means you can count on it to work regardless of which operating system you are using or which output format you wish \n",
      "saving figures to file one nice feature of matplotlib is the ability to save figures in wide variety of for mats you can save figure using the savefig command for example to save the previous figure as png file you can run this \n",
      " section 14 visualization with plotly\n",
      " section 28 read data from input files for plotly plots\n",
      "just as with plt hist plt hist2d has number of extra options to fine tune the plot and the binning which are nicely outlined in the function docstring further just as plt hist has counterpart in np histogram plt hist2d has counterpart in np histogram2d which can be used as follows \n",
      " section 12 three dimensional plotting in matplotlib\n",
      " section 5 visualizing errors density and contour plots\n",
      "the file format is inferred from the extension of the given filename depending on what backends you have installed many different file formats are available you can find the list of supported file types for your system by using the following method of the figure canvas object \n",
      " section 30 scatter charts plotly plots\n",
      " section 10 text annotation text position arrow position\n",
      " section 36 box plots plotly plots\n",
      " section 2 loading from script\n",
      " image png attachment image png \n",
      " section 11 customizing matplotlib configurations and stylesheets\n",
      "plotting from an ipython notebook the ipython notebook is browser based interactive data analysis tool that can com bine narrative code graphics html elements and much more into single exe cutable document plotting interactively within an ipython notebook can be done with the matplotlib command and works in similar way to the ipython shell in the ipython notebook you also have the option of embedding graphics directly in the notebook with two possible options matplotlib notebook will lead to interactive plots embedded within the notebook matplotlib inline will lead to static images of your plot embedded in the notebook for this book we will generally opt for matplotlib inline matplotlib inline after you run this command it needs to be done only once per kernel session any cell within the notebook that creates plot will embed png image of the resulting graphic \n",
      " section 29 line charts plotly plots\n",
      "plotly library plotly s python graphing library makes interactive publication quality graphs online examples of how to make line plots scatter plots area charts bar charts error bars box plots histograms heatmaps subplots multiple axes polar charts and bubble charts \n",
      " section 18 joint plot using seaborn\n",
      "importing matplotlib just as we use the np shorthand for numpy and the pd shorthand for pandas we will use some standard shorthands for matplotlib imports setting styles we will use the plt style directive to choose appropriate aesthetic styles for our fig ures here we will set the classic style which ensures that the plots we create use the classic matplotlib style \n",
      " section 33 bubble charts plotly plots\n",
      "import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv import matplotlib pyplot as plt import seaborn as sns matplotlib inline import os print os listdir input \n",
      "df pd read csv input metadata train csv df info\n",
      "df head\n",
      " let s check if targets are consistent within the same measurement id targets df groupby id measurement target id measurement agg mean targets head\n",
      "sns countplot target data targets it should be only 1 and 0 but we have cases where target is not consitent \n",
      "mislabeled targets loc targets target 1 targets target 0 3 id measurement print str mislabeled shape 0 measurments most likely mislabeled \n",
      " qc it all\n",
      "df loc df id measurement isin mislabeled\n",
      "hi there is it ok that the same measurement have different target labels between signals according to data description it should be the same or not really there are 38 cases of measurements with not consistent labels between signals quick and dirty code to show the problem below \n",
      " basic libraries import numpy as np import pandas as pd import seaborn as sb import matplotlib pyplot as plt we only need pyplot sb set set the default seaborn style for graphics\n",
      " import test and train file everything pd read json input whats cooking train json test pd read json input whats cooking test json \n",
      " making dict of ingredients with their total used times in the data set ingredientdata pd dataframe everything ingredients ingredientdict for in range 0 39774 for ingredient in ingredientdata iloc 0 if ingredient not in ingredientdict keys ingredientdict ingredient 1 else ingredientdict ingredient 1\n",
      " sorting the dictionary according to its value sorted dict sorted keys sorted ingredientdict key ingredientdict get reverse true for in sorted keys sorted dict ingredientdict \n",
      " removing the common ingredients useless for key in sorted dict keys useless append key if len useless 12 break for in useless del sorted dict \n",
      " getting the top 100 ingredients top100 for key in sorted dict keys top100 append key if len top100 100 break\n",
      "top100\n",
      " extracted into csv creating new dataset for machine learning mldict for top ingredient in top100 mldict top ingredient for in range 0 39774 if top ingredient in everything iloc i ingredients mldict top ingredient append 1 else mldict top ingredient append 0 \n",
      " cuisine id for in range 0 39774 cuisine append everything iloc i cuisine id append everything iloc i id mldict id id mldict cuisine cuisine\n",
      " mldf pd dataframe mldict mldf cuisine mldf cuisine astype category mldf top100 mldf top100 astype category \n",
      " extract into csv file machine learning csv mldf to csv r c users limka onedrive documents ntu y1s2 cz1015 mini project machine learning csv csv index none header true don t forget to add csv at the end of the path\n",
      "mldf pd read csv input cooking kaggle cooking machine learning csv csv mldf\n",
      "from sklearn tree import decisiontreeclassifier from sklearn model selection import train test split from sklearn metrics import confusion matrix from sklearn tree import export graphviz import graphviz recall the legendary total dataset train pd dataframe mldf cuisine response train pd dataframe mldf top100 predictor decision tree using train data dectree decisiontreeclassifier max depth 2 create the decision tree object dectree fit train train train the decision tree model predict response corresponding to predictors train pred dectree predict train check the goodness of fit on train data print goodness of fit of model ttrain dataset print classification accuracy t dectree score train train print plot the decision tree treedot export graphviz dectree the model feature names train columns the features out file none output file filled true node colors rounded true make pretty special characters true postscript graphviz source treedot\n",
      " decision tree using train data dectree decisiontreeclassifier max depth 100 create the decision tree object dectree fit train train train the decision tree model predict response corresponding to predictors train pred dectree predict train check the goodness of fit on train data print goodness of fit of model ttrain dataset print classification accuracy t dectree score train train print\n",
      "cuisine pred mldf mldf id isin 11462 40989 27976 22213 6487 25557 27976 1299 cuisine pred\n",
      " extract predictors for prediction pred pd dataframe cuisine pred top100 predict response corresponding to predictors pred dectree predict pred summarize the actuals and predictions pred pd dataframe pred columns predtype index cuisine pred index predicteddf pd concat cuisine pred id cuisine pred axis 1 predicting the cusine frome some of the recipe predicteddf\n",
      "from sklearn ensemble import randomforestclassifier random forest using train data clf randomforestclassifier jobs 2 estimators 100 random state 0 create the object clf fit train train train the model train pred clf predict train print goodness of fit of model ttrain dataset print classification accuracy t clf score train train print\n",
      " extract predictors for prediction pred pd dataframe cuisine pred top100 predict response corresponding to predictors pred clf predict pred summarize the actuals and predictions pred pd dataframe pred columns predtype index cuisine pred index predicteddf pd concat cuisine pred id cuisine pred axis 1 predicting the cusine frome some of the recipe predicteddf\n",
      " creating test dataset for machine learning testdict for top ingredient in top100 testdict top ingredient for in range 0 9944 if top ingredient in test iloc ingredients testdict top ingredient append 1 else testdict top ingredient append 0\n",
      "id for in range 0 9944 id append test iloc id testdict id id \n",
      "testdf pd dataframe testdict testdf top100 testdf top100 astype category \n",
      " classification tree predicting the cusine frome some of the recipe cuisine pred testdf testdf id isin 36914 2280 14729 4594 2237 45631 45523 4977 7124 pred pd dataframe cuisine pred top100 predict response corresponding to predictors pred dectree predict pred summarize the actuals and predictions pred pd dataframe pred columns predtype index cuisine pred index predicteddf pd concat cuisine pred id pred axis 1 predicteddf\n",
      " random forest predicting the cusine frome some of the recipe cuisine pred testdf testdf id isin 36914 2280 14729 4594 2237 45631 45523 4977 7124 pred pd dataframe cuisine pred top100 predict response corresponding to predictors pred clf predict pred summarize the actuals and predictions pred pd dataframe pred columns predtype index cuisine pred index predicteddf pd concat cuisine pred id pred axis 1 predicteddf\n",
      " prediction from classification tree\n",
      " prediction from random forest\n",
      " import basic libraries\n",
      " creating list consisting of the top 100 ingredients and using them to predict the cuisine later on \n",
      " creating dataframe with top 100 ingredients as columns with cuisine and id\n",
      " using for test set\n",
      " multiclass classification tree\n",
      " increasing max depth and predicting train set\n",
      " random forest and predicting train set\n",
      "since the code takes very long to run with 39 000 of rows i ve extracted it into file for first time users you have to uncomment it and run the code\n",
      " read test and train file\n",
      " this python 3 environment comes with many helpful analytics libraries installed it is defined by the kaggle python docker image link for example here s several helpful packages to load import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv input data files are available in the read only input directory for example running this by clicking run or pressing shift enter will list all files under the input directory import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename you can write up to 20gb to the current directory kaggle working that gets preserved as output when you create version using save run all you can also write temporary files to kaggle temp but they won t be saved outside of the current session\n",
      "train data pd read csv kaggle input titanic train csv test data pd read csv kaggle input titanic test csv \n",
      "import plotly express as px\n",
      "train data head 20\n",
      "train data isnull sum checking out which column has most no of nan values\n",
      "px bar data frame train data sex survived color sex facet row spacing 0 title relation between gender and survival rate \n",
      "total passengers train data sex count count males 0 count females 0 for j in zip train data sex train data survived if male and 1 count males 1 elif female and 1 count females 1 print male survival rate print float count males total passengers 100 print female survival rate print float count females total passengers 100 \n",
      "from sklearn ensemble import randomforestclassifier train data survived features pclass sex sibsp parch train pd get dummies train data features test pd get dummies test data features model randomforestclassifier estimators 100 max depth 5 random state 1 model fit train predictions model predict test output pd dataframe passengerid test data passengerid survived predictions output to csv my model csv index false print model saved \n",
      " survival rate for male passenger is 12 235 survival rate for female passenger is 26 150 \n",
      " who has more luck in here from the above data we can find out that females had more survival rate on titanic let s do more diggin in there \n",
      "from the above inference cabin needs to be either dropped or needs to be filled with appropriate values\n",
      " eda is all about asking the right questions what are some questions that come to your mind when you re checking out this data relation between survival vs age survival vs gender survival vs class ticket fare etc our aim is to find out what data points are influencing the survival rate was surviving in titanic pure luck \n",
      " checking out the titanic dataset \n",
      " machine learning model\n",
      " basic libraries import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv import os plot related libraries import matplotlib pyplot as plt import seaborn as sns linear regression model from sklearn linear model import linearregression ridgecv from sklearn preprocessing import labelencoder onehotencoder from sklearn compose import transformedtargetregressor from sklearn utils import shuffle\n",
      "train file kaggle input e commerce participants data train csv test file kaggle input e commerce participants data test csv using pandas read csv method to import data train ecomm df pd read csv train file header 0 test ecomm df pd read csv test file header 0\n",
      "train ecomm df info print 30 train ecomm df head\n",
      "test ecomm df info print 30 test ecomm df head\n",
      "train ecomm df describe \n",
      "train ecomm df columns\n",
      "sns set style whitegrid palette muted fig ax plt subplots 1 2 figsize 12 6 sns distplot train ecomm df selling price kde true ax ax 0 sns scatterplot item rating selling price data train ecomm df marker o color r ax ax 1 plt tight layout plt show\n",
      " transform the target variable target np log1p train ecomm df selling price \n",
      "fig axes plt subplots 1 2 figsize 10 5 sns distplot train ecomm df selling price kde true ax axes 0 sns distplot target kde true ax axes 1 axes 0 set title skewed values axes 1 set title normalized values plt show\n",
      " merge train and test data tempset pd concat train ecomm df test ecomm df keys 0 1 impute the unknown values with mode tempset subcategory 1 tempset subcategory 2 replace unknown np nan bfill ffill tempset subcategory 2 tempset subcategory 2 replace unknown np nan bfill ffill tempset subcategory 1 tempset subcategory 1 fillna tempset subcategory 1 mode 0 tempset subcategory 2 tempset subcategory 2 fillna tempset subcategory 2 mode 0\n",
      "tempset drop date product axis 1 inplace true\n",
      " getting the categorical columns cat data tempset select dtypes include object one hot encoding encode pd get dummies tempset columns cat data columns getting back the tran and test data train enc test encode xs 0 encode xs 1\n",
      " prepare and for fitting the model x train selling price values x train drop selling price axis 1 values test enc test drop selling price axis 1\n",
      "ridge cv ridgecv normalize true cv 10 gcv mode svd scoring neg mean squared error initializing linear regression algorithm with ridge regularizer k fold with 10 folds ridge reg transformedtargetregressor regressor ridge cv func np log1p inverse func np expm1\n",
      "ridge reg fit y predict the test data predictions ridge reg predict test\n",
      "final df pd dataframe selling price predictions final df selling price final df apply lambda round 2 final df pd concat test ecomm df final df selling price axis 1\n",
      "final df head 20\n",
      " learnings the data is mix of categorical ordinal numeric and date values the y target attribute selling price has got skewed data when we visualize its distribution we need to apply the transformation method to make it normal here np log1p method is used click to know more about the method link it is always good to start with linear model rather than ensembles or neural network the indention was to get exposure to real time data not the leaderboard pun indented first tried with linearregressor model with ridgecv during the iteration applied the data with quantiletransformer link of 300 estimators but the result was not converging towards 0 5 hence switched to log transformer link \n",
      " define and \n",
      " ridge cv implementation\n",
      " test set \n",
      " exploratory data analysis\n",
      " prepare data for model building the dataset contains date and few categorical columns we need to encode the categorical columns to number before building model \n",
      " ecommerce price prediction problem statement commerce platforms have been in existence for more than 2 decades now the popularity and its preference as common choice for buying and selling essential products have grown rapidly and exponentially over the past few years commerce has impacted the lifestyle of common people to huge extent many such platforms are competing over each other for dominance by providing consumer goods at competitive price in this hackathon we challenge data science enthusiasts to predict the price of commodities on an commerce platform given are 7 distinguishing factors that can influence the price of product on an commerce platform your objective as data scientist is to build machine learning model that can accurately predict the price of product based on the given factors input data the unzipped folder will have the following files train csv 2452 observations test csv 1051 observations target variable selling price \n",
      "let us check the info of the given dataset training set \n",
      " building linear regression model using transformedtargetregressor model this model allows us to use cross validation and regularizer functions such as ridge and lasso\n",
      " conclusion the final submission score is as follows best public score final score 0 67659 0 65363 these scores stood 38th position the challenge was quite tough solely because of the data although the feature scaling and engineering parts were not done extensively here the linear regressor with ridgecv seemed to have done pretty good job \n",
      " load the dataset the ecommerce price prediction problem has set of data in train and test file as comma separated file \n",
      " import libraries\n",
      " check for null data the training set seems to have no null data \n",
      "import pandas as pd import numpy as np importing libraries for text preprocessing from nltk corpus import stopwords from nltk stem import snowballstemmer from nltk tokenize import word tokenize from keras preprocessing text import tokenizer from keras preprocessing sequence import pad sequences keras packages for network training and evaluation from keras import models layers callbacks from keras metrics import precision recall auc from keras import backend as from tensorflow keras import optimizers and sklearn s train test split to define train test and validation sets from sklearn model selection import train test split pd options mode chained assignment none\n",
      " taking look at our data data pd read csv input are two sentences of the same topic train csv drop columns id len data len data data\n",
      " the dataset is balanced fortunately data same source value counts\n",
      " let s clean our data from stopwords and truncate the word endings to make the different forms of one word represent this word for the network stop words stopwords words english sbs snowballstemmer language english for col in sent1 sent2 print f column col clear texts for in range len data if 20000 0 print f processing sentence i len data cell text data iloc col clear text tokenizing stopwords filtering and stemming cell tokens word tokenize cell text for token in cell tokens tok low token lower if tok low not in stop words tok stem sbs stem tok low clear text tok stem clear texts append clear text data col clear texts\n",
      " cleaned texts data\n",
      " dividing our dataset into train test and validation sets in ratio of 0 9 0 05 0 05 keeping the target variable distribution in each set by stratify parameter train test train test train test split data drop columns same source data same source random state 42 train size 0 9 stratify data same source val test val test train test split test test random state 42 train size 0 5 stratify test\n",
      " let s find the sentence length params to have starter point for picking the embedding lengths max sent len 0 avg sent len 0 for col in sent1 sent2 for in range len train sent len len train col iloc split avg sent len sent len len train if sent len max sent len max sent len sent len print f max number of words in train sentence is max sent len f average number is round avg sent len 2 \n",
      " tokenization is the operation of transforming texts into numerical vectors we can modify num words parameter to choose how many most frequent words in dataset should be represented in vectors vocab sizes train inputs val inputs test inputs sent vector len int avg sent len int max sent len 3 we fit one tokenizer for all sentences as we don t want text in pairs to have different embeddings for same words tokenizer tokenizer num words 20000 tokenizer fit on texts train sent1 values train sent2 values vocab size len tokenizer word index 1 for col in sent1 sent2 train sequences tokenizer texts to sequences train col val sequences tokenizer texts to sequences val col test sequences tokenizer texts to sequences test col padding allows us to have equal length in all texts short text vectors will be extended with zeros long ones will be truncated the maxlen parameter also can be modified train inputs append pad sequences train sequences padding post maxlen sent vector len val inputs append pad sequences val sequences padding post maxlen sent vector len test inputs append pad sequences test sequences padding post maxlen sent vector len\n",
      " loading glove embeddings tutorial link embeddings index dict open input glove global vectors for word representation glove 6b 100d txt r for line in values line split word values 0 coefs np asarray values 1 dtype float32 embeddings index word coefs close print loaded s word vectors len embeddings index\n",
      "embedding matrix np zeros vocab size 100 for word in tokenizer word index items embedding vector embeddings index get word if embedding vector is not none embedding matrix embedding vector\n",
      " checking the dimensionality of input data print train inputs 0 shape print train inputs 1 shape\n",
      " keras doesn t have built in f1 score but it s useful binary classification metric we ll define it as it was proposed in link def f1 true pred def recall true pred recall metric only computes batch wise average of recall computes the recall metric for multi label classification of how many relevant items are selected true positives sum round clip true pred 0 1 possible positives sum round clip true 0 1 recall true positives possible positives epsilon return recall def precision true pred precision metric only computes batch wise average of precision computes the precision metric for multi label classification of how many selected items are relevant true positives sum round clip true pred 0 1 predicted positives sum round clip pred 0 1 precision true positives predicted positives epsilon return precision precision precision true pred recall recall true pred return 2 precision recall precision recall epsilon\n",
      " dataframe to show the final metrics of different models res df pd dataframe columns model name test precision test recall test f1 test auc \n",
      " we ll create 4 different nets let s define the fit evaluate method to avoid writing this piece of code for 4 times def compile fit evaluate model model compile optimizer adam loss binary crossentropy metrics precision recall f1 auc print model summary model fit train inputs 0 train inputs 1 train epochs 100 validation data val inputs 0 val inputs 1 val callbacks callbacks earlystopping monitor val f1 verbose 1 patience 2 restore best weights true mode max return model evaluate test inputs 0 test inputs 1 test\n",
      " simple mlp with two inputs and shared dense part input1 layers input shape sent vector len input2 layers input shape sent vector len merged layers concatenate axis 1 input1 input2 emb layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true merged flat1 layers flatten emb dense1 layers dense 128 activation relu flat1 dropout1 layers dropout 0 5 dense1 dense2 layers dense 128 activation relu dropout1 dropout2 layers dropout 0 5 dense2 dense3 layers dense 128 activation relu dropout2 output layers dense 1 activation sigmoid dense3 model mlp models model inputs input1 input2 outputs output test metrics compile fit evaluate model mlp res df res df append model name basic mlp test precision round test metrics 1 4 test recall round test metrics 2 4 test f1 round test metrics 3 4 test auc round test metrics 4 4 ignore index true\n",
      " mlp with two inputs and separate dense parts for each sentence input1 layers input shape sent vector len emb1 layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true input1 flat1 layers flatten emb1 dense11 layers dense 128 activation relu flat1 dropout11 layers dropout 0 5 dense11 dense12 layers dense 128 activation relu dropout11 dropout12 layers dropout 0 5 dense12 dense13 layers dense 128 activation relu dropout12 input2 layers input shape sent vector len emb2 layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true input2 flat2 layers flatten emb2 dense21 layers dense 128 activation relu flat2 dropout21 layers dropout 0 5 dense21 dense22 layers dense 128 activation relu dropout21 dropout22 layers dropout 0 5 dense22 dense23 layers dense 128 activation relu dropout22 merged layers concatenate axis 1 dense13 dense23 dense out layers dense 128 activation relu merged output layers dense 1 activation sigmoid dense out model mlp 2head models model inputs input1 input2 outputs output test metrics compile fit evaluate model mlp 2head res df res df append model name two head mlp test precision round test metrics 1 4 test recall round test metrics 2 4 test f1 round test metrics 3 4 test auc round test metrics 4 4 ignore index true\n",
      " simple cnn with two inputs and shared cnn part input1 layers input shape sent vector len input2 layers input shape sent vector len merged layers concatenate axis 1 input1 input2 emb layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true merged conv layers conv1d 256 5 activation relu emb pool layers maxpooling1d pool size 2 conv drop layers dropout 0 5 pool flat layers flatten drop dense layers dense 128 activation relu flat output layers dense 1 activation sigmoid dense model conv models model inputs input1 input2 outputs output test metrics compile fit evaluate model conv res df res df append model name basic cnn test precision round test metrics 1 4 test recall round test metrics 2 4 test f1 round test metrics 3 4 test auc round test metrics 4 4 ignore index true\n",
      " cnn with two inputs and separate cnn parts for each sentence input1 layers input shape sent vector len emb1 layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true input1 conv1 layers conv1d 256 5 activation relu emb1 pool1 layers maxpooling1d pool size 2 conv1 drop1 layers dropout 0 5 pool1 flat1 layers flatten drop1 dense1 layers dense 128 activation relu flat1 input2 layers input shape sent vector len emb2 layers embedding input dim vocab size output dim 100 input length sent vector len weights embedding matrix trainable true input2 pool2 layers maxpooling1d pool size 2 emb2 drop2 layers dropout 0 5 pool2 flat2 layers flatten drop2 dense2 layers dense 128 activation relu flat2 merged layers concatenate axis 1 dense1 dense2 dense out layers dense 128 activation relu merged output layers dense 1 activation sigmoid dense out model conv 2head models model inputs input1 input2 outputs output test metrics compile fit evaluate model conv 2head res df res df append model name two head cnn test precision round test metrics 1 4 test recall round test metrics 2 4 test f1 round test metrics 3 4 test auc round test metrics 4 4 ignore index true\n",
      " printing the metrics res df sort values by test f1 ascending false\n",
      "the cnns outperform the mlps they seem to find more complex relations between the sentences also updating the pre trained embeddings rather than fitting the new ones increased the auc from 0 77 to 0 82 at the moment i m trying to crack the problems of the recurrent nets hopefully they ll appear here later thanks for your attention i m open for discussion if you want to leave any comments \n",
      " greetings in this notebook we ll try to detect that sentences belong to same text with keras perceptron and 1d convolutional net with two inputs the embeddings will be initialized using glove \n",
      "import pandas as pd import numpy as np\n",
      " function to reduce the df size def reduce mem usage df verbose true numerics int16 int32 int64 float16 float32 float64 start mem df memory usage sum 1024 2 for col in df columns col type df col dtypes if col type in numerics min df col min max df col max if str col type 3 int if min np iinfo np int8 min and max np iinfo np int8 max df col df col astype np int8 elif min np iinfo np int16 min and max np iinfo np int16 max df col df col astype np int16 elif min np iinfo np int32 min and max np iinfo np int32 max df col df col astype np int32 elif min np iinfo np int64 min and max np iinfo np int64 max df col df col astype np int64 else if min np finfo np float16 min and max np finfo np float16 max df col df col astype np float16 elif min np finfo np float32 min and max np finfo np float32 max df col df col astype np float32 else df col df col astype np float64 end mem df memory usage sum 1024 2 if verbose print mem usage decreased to 5 2f mb 1f reduction format end mem 100 start mem end mem start mem return df\n",
      "train pd read csv kaggle input bdg2 class competition train csv test pd read csv kaggle input bdg2 class competition test csv wtrain pd read csv kaggle input bdg2 class competition weather train csv wtest pd read csv kaggle input bdg2 class competition weather test csv metadata pd read csv kaggle input bdg2 class competition metadata csv \n",
      "train reduce mem usage train test reduce mem usage test wtrain reduce mem usage wtrain wtest reduce mem usage wtest metadata reduce mem usage metadata\n",
      "metadata info\n",
      "train info\n",
      "wtrain info\n",
      "metadata isna sum 100 len metadata\n",
      " select columns with more than 50 missing values missing metadata isna sum 100 len metadata to drop missing missing 50 index drop metadata drop to drop axis 1 inplace true\n",
      "metadata head\n",
      "from sklearn preprocessing import ordinalencoder\n",
      " select object columns metadata object cols metadata select dtypes include object columns metadata object cols\n",
      " this returns an array encoder bdg id ordinalencoder encoded building id encoder bdg id fit transform metadata building id encoded building id train encoder bdg id transform train building id encoded building id test encoder bdg id transform train building id encoder site id ordinalencoder encoded site id encoder site id fit transform metadata site id encoded site id train encoder site id transform wtrain site id encoded site id test encoder site id transform wtest site id encoder puse ordinalencoder encoded puse encoder puse fit transform metadata primaryspaceusage encoder spuse ordinalencoder encoded spuse encoder spuse fit transform metadata sub primaryspaceusage encoder timezone ordinalencoder encoded timezone encoder timezone fit transform metadata timezone encoder meter ordinalencoder encoded meter train encoder meter fit transform train meter encoded meter test encoder meter transform test meter we can convert it to dataframe encoded building id pd dataframe encoded building id encoded site id pd dataframe encoded site id encoded puse pd dataframe encoded puse encoded spuse pd dataframe encoded spuse encoded timezone pd dataframe encoded timezone encoded meter train pd dataframe encoded meter train encoded meter test pd dataframe encoded meter test encoded building id train pd dataframe encoded building id train encoded building id test pd dataframe encoded building id test encoded site id train pd dataframe encoded site id train encoded site id test pd dataframe encoded site id test and replace it in your data metadata building id encoded building id metadata site id encoded site id metadata primaryspaceusage encoded puse metadata sub primaryspaceusage encoded spuse metadata timezone encoded timezone train meter encoded meter train train building id encoded building id train test meter encoded meter test test building id encoded building id test wtrain site id encoded site id train wtest site id encoded site id test\n",
      "metadata head\n",
      "train head\n",
      "wtrain head\n",
      "train1 pd merge train metadata how left on building id test1 pd merge test metadata how left on building id \n",
      "train1 pd merge train1 wtrain how left on timestamp site id test1 pd merge test1 wtest how left on timestamp site id \n",
      "train1 head\n",
      "test1 head\n",
      "del metadata del wtrain wtest del test train\n",
      "from sklearn metrics import mean squared error from lightgbm import lgbmregressor\n",
      "x train train1 drop meter reading axis 1 set index timestamp train train1 meter reading\n",
      "x test test1 drop row id timestamp axis 1\n",
      " model lgbmr lgbmregressor estimators 50 random state 55 train lgbmr fit train train\n",
      "prediction lgbmr predict test\n",
      "sub pd dataframe row id test1 row id meter reading prediction\n",
      "sub to csv submission csv index false\n",
      "always remember to delete your now useless data will free up some memory \n",
      "and it s all encoded we only encoded these columns to get rid of the string data type but if you want to train model that actually predicts you should use one hot encoding \n",
      "first split in x the training data and y the target data also remember to set the timestamp column as index this data type cannot be handle by the model \n",
      " load data\n",
      "we have to encode the above columns notice that the column building id is also present in energy data and the column site id is in weather data we also have to encode the column meter from energy data sets \n",
      "and for the test data set we have an extra column row id that is needed for the submission remember to remove for the prediction you will add it again after that \n",
      " sample submission this notebooks is an example on how to make submission to the competition steps to be performed are simple preprocessing just to make the data work with the model simple lightgbm model with the default parameters predict test data with that model submission first import the basic libraries to load the data other libraries needed will be imported ahead \n",
      " merge now we have to merge all the data to train our model \n",
      "and if you want use the function defined before to reduce memory usage \n",
      "once you save version of your notebook you will see data tab in your notebook page from there you can submit your prediction with the button submit sample sub png link \n",
      " missing values we are going to drop the columns from metadata that have over 50 of missing values remember this is just for sample submission you can input those missings to train your model \n",
      " model finally we can train model \n",
      "and to create the submission you will need dataframe with two columns row id and meter reading the prediction \n",
      "the following function helps reduce the memory usage it was taken from this amazing notebook link you don t really need to use it but when you start playing around with feature engineering you will notice that for each column you add the data set gets heavier and heavier you always have to keep some memory free to train your model which is high memory consuming process so keep this function handy in case you need it looks difficult but it just convert the current data types to one lighter \n",
      "now we can predict \n",
      "and let s train the model this is super basic model parameters tunning is and important part of machine learning and we are skipping that step here \n",
      " encoding to be used in machine learning model the object columns i e strings have to be encoded as numbers we are going to use ordinalencoder from scikitlearn \n",
      " this python 3 environment comes with many helpful analytics libraries installed it is defined by the kaggle python docker image link for example here s several helpful packages to load import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv import matplotlib pyplot as plt import seaborn as sns other imports import sys input data files are available in the read only input directory for example running this by clicking run or pressing shift enter will list all files under the input directory import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename you can write up to 20gb to the current directory kaggle working that gets preserved as output when you create version using save run all you can also write temporary files to kaggle temp but they won t be saved outside of the current session\n",
      "pip install scikit allel\n",
      "import allel allel version \n",
      " callset allel read vcf input end als end als genomics data answerals subset annovar hg38 anno and geno no intergenic vcf \n",
      " sorted callset keys \n",
      " callset samples \n",
      "import numpy as np np version \n",
      "pip install zarr\n",
      "import zarr zarr version \n",
      "pip install numcodecs\n",
      "import numcodecs numcodecs version \n",
      "vcf path input end als end als genomics data answerals subset annovar hg38 anno and geno no intergenic vcf ls lh vcf path\n",
      "zarr path input end als end als genomics data answerals subset annovar hg38 anno and geno no intergenic vcf \n",
      " the original snippet used data from chromosome 22 end als didn t mention any chromossome in the file trying to make it work removed the group and the fields unfortunately it didn t work allel vcf to zarr vcf path zarr path group 22 fields log sys stdout overwrite true \n",
      "allel vcf to zarr vcf path zarr path group fields log sys stdout overwrite true\n",
      "callset zarr open group zarr path mode r callset tree expand true\n",
      "gt zarr callset 22 calldata gt gt zarr info\n",
      "pos allel sortedindex callset 22 variants pos pos\n",
      "loc region pos locate range 20000000 20100000 loc region\n",
      "gt region allel genotypearray gt zarr loc region gt region\n",
      "multi allelic callset 22 variants multi allelic multi allelic\n",
      "gt allel genotypearray gt zarr gt\n",
      "gt variant selection gt compress loc variant selection axis 0 gt variant selection\n",
      "gt dask allel genotypedaskarray gt zarr gt dask\n",
      "gt variant selection gt dask compress loc variant selection axis 0 compute gt variant selection\n",
      " that snippet above took so long more than 15 minutes to read vcf and is consuming my ram then commented it \n",
      " extract data and convert to zarr format use the vcf to zarr function from scikit allel that conversion will make life easier downstream \n",
      " i hope to learn little bit how to work with vcf files scikit allel and bioinformatics \n",
      " code by alistair miles link \n",
      " the samples array contains sample identifiers extracted from the header line in the vcf file \n",
      " loading data for gene\n",
      " the callset object returned by read vcf is python dictionary dict it contains several numpy arrays each of which can be accessed via key here are the available keys \n",
      " after fspathexistnotdir path exists but is not directory r everything went wrong maybe anyone can fix it \n",
      " use dask if data is larger and or your computer doesn t have much ram\n",
      " link \n",
      " extract genotypes for the selection\n",
      " extract data from vcf\n",
      " open the zarr data \n",
      " filtering variants\n",
      " extract genotype data for these variants\n",
      " variant call format vcf \n",
      " apply the selection using almost the same syntax except that when working via dask we need to call the compute method to get the final result \n",
      " start with the scikit allel function read vcf \n",
      " this python 3 environment comes with many helpful analytics libraries installed it is defined by the kaggle python docker image link for example here s several helpful packages to load import numpy as np linear algebra import pandas as pd data processing csv file i o e g pd read csv input data files are available in the read only input directory for example running this by clicking run or pressing shift enter will list all files under the input directory import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename you can write up to 20gb to the current directory kaggle working that gets preserved as output when you create version using save run all you can also write temporary files to kaggle temp but they won t be saved outside of the current session\n",
      "test esrb df pd read csv kaggle input video games rating by esrb test esrb csv \n",
      "vidgme esrb df pd read csv kaggle input video games rating by esrb video games esrb rating csv other cols title console esrb rating def clean df df filter out games with no descriptors df df df no descriptors 0 df df drop no descriptors axis 1 return df vidgme esrb df clean df vidgme esrb df\n",
      "esrb percentages df vidgme esrb df drop other cols axis 1 sum len vidgme esrb df esrb percentages df\n",
      "vidgme esrb df pivot table index esrb rating console \n",
      "pd set option display max columns none evryone esrb df vidgme esrb df groupby esrb rating get group e evryone esrb df evryone esrb df use of drugs and alcohol 1\n",
      " style background gradient cmap viridis vidgme esrb df drop other cols axis 1 corr style background gradient cmap coolwarm \n",
      "def get and df df drop other cols esrb rating axis 1 df esrb rating return y y get and vidgme esrb df\n",
      "from sklearn svm import svc svc svc svc svc fit y svc score \n",
      "x test test get and clean df test esrb df svc score test test\n",
      " support vector classifier scored 0 8442 on test set\n",
      " interesting tidbit shovel knight is the only game with drugs and alcohol with rating of everyone in this dataset link link in the official rating it says use of alcohol only bit strange \n",
      "import pandas as pd import spacy import networkx as nx really useful network analysis library import matplotlib pyplot as plt from networkx algorithms import community not used yet import datetime access to time for timing individual notebook cells import os\n",
      "nlp spacy load en core web lg more detailed model with higher dimension word vectors 13s to load normally nlp spacy load en core web md smaller model e g for testing\n",
      "plt rcparams figure figsize 10 10 makes the output plots large enough to be useful\n",
      "rowlimit 500 this limits the tweets to manageable number data pd read csv input extractedtweets csv nrows rowlimit data shape\n",
      "data head 6\n",
      "tokens lemma pos parsed doc col to parse tweet for doc in nlp pipe data col to parse astype unicode values batch size 50 threads 3 if doc is parsed parsed doc append doc tokens append text for in doc lemma append lemma for in doc pos append pos for in doc else we want to make sure that the lists of parsed results have the same number of entries of the original dataframe so add some blanks in case the parse fails tokens append none lemma append none pos append none data parsed doc parsed doc data comment tokens tokens data comment lemma lemma data pos pos pos\n",
      "data head 8\n",
      "data tweet 0\n",
      "data tweet 1\n",
      "data tweet 10\n",
      "stop words spacy lang en stop words stop words print number of stopwords d len stop words print list stop words\n",
      "print data parsed doc 0 similarity data parsed doc 1 print data parsed doc 0 similarity data parsed doc 10 print data parsed doc 1 similarity data parsed doc 10\n",
      "data party unique\n",
      "world data data world data data data party democrat or use either of these if you want to see tweets from only one party world data data data party republican \n",
      " takes 1s for 500 nodes but of course this won t scale linearly raw nx graph undirected 0 for in world data parsed doc sure it s inefficient but it will do for in world data parsed doc if j if not raw has edge i sim similarity raw add edge j weight sim n 1 print raw number of nodes nodes and raw number of edges edges created \n",
      "edges to kill min wt 0 94 this is our cutoff value for minimum edge weight for nbrs in raw adj items print nprocessing origin node n for nbr eattr in nbrs items remove edges below certain weight data eattr weight if data min wt print 3f data print d d 3f n nbr data print nnode n n nneighbour nbr edges to kill append nbr print n len edges to kill 2 edges to kill of raw number of edges before de duplicating \n",
      "for v in edges to kill if raw has edge v catches e g those edges where we ve removed them using reverse v u raw remove edge \n",
      "strong raw print strong number of edges\n",
      "nx draw strong node size 20 edge color gray \n",
      "strong remove nodes from list nx isolates strong \n",
      "from math import sqrt count strong number of nodes equilibrium 10 sqrt count default for this is 1 sqrt n but this will blow out the layout for better visibility pos nx fruchterman reingold layout strong k equilibrium iterations 300 nx draw strong pos pos node size 10 edge color gray \n",
      "plt rcparams figure figsize 16 9 better aspect ratio for labelled nodes nx draw strong pos font size 3 node size 50 edge color gray with labels false for in pos raise positions of the labels relative to the nodes pos 1 0 03 nx draw networkx labels strong pos font size 8 font color k plt show\n",
      "n b this next step can take while e g 14 mins for the full set but only 5s for 500 rows based on link \n",
      " basic checks of the parsed data\n",
      "we could reduce increase the signal noise ratio in these texts by removing some of the more common words or stopwords by removing these from the tweets we would prevent them from influencing the analysis of whether two tweets are similar i m not addressing this is the notebook yet but will come back to it later for now let s just look at what words are included in spacy s stopword list \n",
      "i hope this notebook was useful next i d like to apply some keyword extraction to the tweets to make this visualisation more useful there ll be some topic identification using gensim s implementation of lda some more intelligent parameterisation of variables such as allowing the minimum similarity cut off to account for network size i d like to apply smarter similarity cut off such as vladimir batagelj s vertex islands link technique and should really apply tf idf if only just to see how it compares to other keyword extraction techniques \n",
      "we can also tweak the layout algorithm by for example changing the ideal distance at which the repulsive and attractive forces are in equilibrium there s good description of these forces here link this value interacts with the number of iterations in surprising ways \n",
      " load representative set of tweets demonstrate some basic spacy features test its similarity metrics build graph data structure for storing n 1 2 similarity results visualise the clusters of most similar items in the data plan the next steps\n",
      "we should now have clean graph of only hi similarity edges \n",
      " data\n",
      " removing stopwords\n",
      " analysing text similarity using spacy networkx this notebook demonstrates one way of using spacy to conduct rapid thematic analysis of small corpus of comments and introduces some unusual network visualisations topics include spacy link an open source nlp library word vectors and networkx an open source network graph analysis and visualisation library the notebook is partly reminder for myself on just how well these techniques work but hope that others find it useful i ll continue to update it with more techniques over the coming weeks if you have any suggestions feel free to make them in the comments fork the notebook etc i m keen to exchange tips and tricks \n",
      " next steps\n",
      "networkx has several useful layouts implemented but you can t beat good spring embedding layour a kind of force directed graph link in graph terminology what we see is single large component link at the centre with several pendants link visible at the edges several smaller components and peripheral cloud of isolates link force directed graphs are very intuitive satisfying and efficient way to lay out network diagrams essentially every node exerts repulsive force on every other node simultaneously every connected pair of nodes attract each other the layout algorithm iterates finding layout that balances these forces \n",
      "this next step load the spacy language model it generally takes about 13s to load this large model \n",
      "if you ve limited the rows imported then you may only have democrat tweets which occur first in the list \n",
      " testing spacy s similarity function\n",
      "of course we can specify the layout we want to use change colours sizes etc the following cell adds the text of the tweets which can make the layout hard to read \n",
      "visualising the whole graph but only those links of weights above certain cutoff allows us to get feel for good cutoff level to use when visualising the structure having filtered out these lower weighted links we can clean up the graph by removing the isolates this will enable the layout engine to show us more of the structure of the components \n",
      " plan\n",
      " using spacy to parse the tweets \n",
      " visualising the selected edges\n",
      "import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns from sklearn naive bayes import gaussiannb from sklearn tree import decisiontreeclassifier from sklearn ensemble import randomforestclassifier from sklearn svm import svc from sklearn neighbors import kneighborsclassifier import warnings warnings filterwarnings ignore from sklearn import metrics from sklearn metrics import confusion matrix from sklearn metrics import f1 score from sklearn metrics import classification report from sklearn model selection import train test split from sklearn ensemble import randomforestclassifier from sklearn ensemble import votingclassifier from sklearn ensemble import baggingclassifier from sklearn metrics import roc auc score import sklearn metrics as metrics from xgboost import xgbclassifier from sklearn ensemble import randomforestclassifier from sklearn linear model import logisticregression\n",
      " pip install xgboost\n",
      "old data pd read csv input covid19 mexico patient health dataset patient csv old data death date old data death date replace 9999 99 99 1 old data loc old data death date 1 death date 0\n",
      "old data nunique\n",
      "old data info\n",
      "old data death date old data death date astype object astype int\n",
      "old data info\n",
      " old data\n",
      "old data corr abs\n",
      "one old data old data death date 0 two old data old data death date 1 datam pd concat one two ignore index true\n",
      "datam corr abs\n",
      "x datam head 3000 datam tail 3000 data pd concat y ignore index true\n",
      "sns heatmap data corr abs annot false linewidths 5 cbar kws shrink 5\n",
      " sns heatmap data corr abs annot true train data drop death date axis 1 train data death date train pd dataframe train train\n",
      "x train test train test train test split train train test size 0 2 random state 1 print eğitim veri adet train shape 0 print eğitim label adet train shape 0 print test veri adet test shape 0 print test label adet test shape 0\n",
      "y train pd dataframe train train\n",
      "y train train astype float test test astype float \n",
      " navie bayes nb gaussiannb nb fit train train score 1 nb score test test decision tree decision tree decisiontreeclassifier decision tree fit train train score 2 decision tree score test test random forest random forest randomforestclassifier random forest fit train train score 3 random forest score test test svm sup vec svc random state 3 sup vec fit train train score 4 sup vec score test test knn knn kneighborsclassifier neighbors 3 knn fit train train score 5 knn score test test score print navie bayes acc score score 1 print decision tree acc score score 2 print random forest acc score score 3 print svm acc score score 4 print knn acc score score 5\n",
      "y pred random forest predict test pred 1 nb predict test pred 2 decision tree predict test pred 3 sup vec predict test pred 4 knn predict test\n",
      "print confusion matrix test pred sns heatmap confusion matrix test pred annot true lw 2 cbar false plt ylabel true values plt xlabel preict value plt title confusion matrix \n",
      "print classification report test pred\n",
      "print f1 score f1 score test pred\n",
      "classifier xgbclassifier classifier fit train train predic classifier predict test cm confusion matrix predic test print cm\n",
      "sns heatmap confusion matrix predic test annot true lw 2 cbar false plt ylabel true values plt xlabel preict value plt title confusion matrix \n",
      "probs random forest predict proba test preds probs 1 fpr tpr threshold metrics roc curve test pred roc auc metrics auc fpr tpr probs 1 nb predict proba test preds 1 probs 1 1 fpr 1 tpr 1 threshold 1 metrics roc curve test pred 1 roc auc 1 metrics auc fpr 1 tpr 1 probs 2 decision tree predict proba test preds 2 probs 2 1 fpr 2 tpr 2 threshold 2 metrics roc curve test pred 2 roc auc 2 metrics auc fpr 2 tpr 2 probs 3 sup vec predict proba x test preds 3 probs 3 1 fpr 3 tpr 3 threshold 3 metrics roc curve y test pred 3 roc auc 3 metrics auc fpr 3 tpr 3 probs 4 knn predict proba test preds 4 probs 4 1 fpr 4 tpr 4 threshold 4 metrics roc curve test pred 4 roc auc 4 metrics auc fpr 4 tpr 4 probs 5 classifier predict proba test preds 5 probs 5 1 fpr 5 tpr 5 threshold 5 metrics roc curve test predic roc auc 5 metrics auc fpr 5 tpr 5\n",
      "score 6 classifier score test test print score 6\n",
      "plt title receiver operating charac plt plot fpr tpr b label rf 0 2f roc auc plt plot fpr 5 tpr 5 black label xgboost 93 roc auc plt plot fpr 2 tpr 2 y label dt 90 roc auc plt plot fpr 4 tpr 4 r label knn 87 roc auc plt plot fpr 1 tpr 1 g label nb 80 roc auc plt legend loc lower right plt plot 0 1 0 1 r plt xlim 0 1 plt ylim 0 1 plt ylabel true pozitive rate plt xlabel false positive rate plt show\n",
      "f data data drop diabetes copd asthma immunosuppression hypertension other diseases cardiovascular obesity chronic kidney failure smoker axis 1\n",
      "f data corr abs\n",
      "sns heatmap data corr abs annot false linewidths 5 cbar kws shrink 5\n",
      "xf train data drop death date axis 1 yf train data death date yf train pd dataframe yf train yf train\n",
      "xf train xf test yf train yf test train test split xf train yf train test size 0 1 random state 1 print eğitim veri adet xf train shape 0 print eğitim label adet yf train shape 0 print test veri adet xf test shape 0 print test label adet yf test shape 0\n",
      "print eğitim veri adet xf train shape 0 print eğitim label adet yf train shape 0 print test veri adet xf test shape 0 print test label adet yf test shape 0 train train astype float test test astype float navie bayes nb fit xf train yf train score 10 nb score xf test yf test decision tree decision tree fit xf train yf train score 11 decision tree score xf test yf test random forest random forest fit xf train yf train score 12 random forest score xf test yf test svm sup vec fit xf train yf train score 13 sup vec score xf test yf test knn knn fit xf train yf train score 14 knn score xf test yf test print navie bayes acc score score 10 print decision tree acc score score 11 print random forest acc score score 12 print svm acc score score 13 print knn acc score score 14 yf pred11 random forest predict xf test yf pred12 nb predict xf test yf pred13 decision tree predict xf test yf pred14 sup vec predict xf test yf pred15 knn predict xf test\n",
      "print confusion matrix yf test yf pred11 sns heatmap confusion matrix yf pred11 yf test annot true lw 2 cbar false plt ylabel true values plt xlabel preict value plt title confusion matrix \n",
      "print classification report yf test yf pred11\n",
      "classifier fit xf train yf train predic11 classifier predict xf test cm1 confusion matrix predic11 yf test print cm1\n",
      "y predic11 shape\n",
      "print f1 score f1 score yf test predic11\n",
      "classifier fit xf train yf train predic11 classifier predict xf test\n",
      "score 777 classifier score xf test yf test print score 777\n",
      "probs22 random forest predict proba xf test preds22 probs22 1 fpr22 tpr22 threshold metrics roc curve yf test yf pred11 roc auc22 metrics auc fpr22 tpr22 probs23 nb predict proba xf test preds23 probs23 1 fpr23 tpr23 threshold23 metrics roc curve yf test yf pred12 roc auc23 metrics auc fpr23 tpr23 probs24 decision tree predict proba xf test preds24 probs24 1 fpr24 tpr24 threshold24 metrics roc curve yf test yf pred13 roc auc24 metrics auc fpr24 tpr24 probs 3 sup vec predict proba x test preds 3 probs 3 1 fpr 3 tpr 3 threshold 3 metrics roc curve y test pred 3 roc auc 3 metrics auc fpr 3 tpr 3 probs25 knn predict proba xf test preds25 probs25 1 fpr25 tpr25 threshold25 metrics roc curve yf test yf pred15 roc auc25 metrics auc fpr25 tpr25 probs 99 classifier predict proba xf test preds 99 probs 99 1 fpr 99 tpr 99 threshold 99 metrics roc curve yf test predic11 roc auc 99 metrics auc fpr 99 tpr 99\n",
      "plt title receiver operating charac plt plot fpr 99 tpr 99 black label xgboost 93 roc auc plt plot fpr22 tpr22 b label rf 92 roc auc plt plot fpr24 tpr24 y label dt 90 roc auc plt plot fpr25 tpr25 r label knn 89 roc auc plt plot fpr23 tpr23 g label nb 82 roc auc plt legend loc lower right plt plot 0 1 0 1 r plt xlim 0 1 plt ylim 0 1 plt ylabel true pozitive rate plt xlabel false positive rate plt show\n",
      "volting votingclassifier estimators rf random forest gbboost classifier dt decision tree voting hard fit xf train yf train\n",
      "score7842 volting score xf test yf test\n",
      "score7842\n",
      "volting2 votingclassifier estimators rf random forest gbboost classifier dt decision tree voting soft fit xf train yf train\n",
      "score78 volting2 score xf test yf test\n",
      "score78\n",
      "volting12 votingclassifier estimators rf random forest gbboost classifier dt decision tree voting hard fit train train\n",
      "score123123 volting12 score test test\n",
      "score123123\n",
      "bag baggingclassifier decision tree estimators 100 max samples 0 8 jobs 1 random state 1 fit xf train yf train\n",
      "bag score xf test yf test\n",
      "randomforestobject randomforestclassifier estimators 10 randomforestobject fit xf train yf train\n",
      "df test sonuc randomforestobject score xf test yf test print random forest doğruluk test seti round df test sonuc 2\n",
      "yf pred11\n",
      "extra data pd dataframe yf pred11 extra data1 pd dataframe yf pred12 extra data2 pd dataframe yf pred13 extra data3 pd dataframe yf pred14 extra data4 pd dataframe yf pred15 extra data5 pd dataframe predic11\n",
      "df new extra data rename columns 0 deat date df new1 extra data rename columns 0 deat date df new2 extra data rename columns 0 deat date df new3 extra data rename columns 0 deat date df new4 extra data rename columns 0 deat date df new5 extra data rename columns 0 deat date \n",
      "df new df new astype int df new1 df new1 astype int df new2 df new2 astype int df new3 df new3 astype int df new4 df new4 astype int df new5 df new5 astype int \n",
      "data sets pd concat df new df new1 df new2 df new3 df new4 df new5 ignore index true axis 1\n",
      "data sets\n",
      "yf test shape\n",
      "yf test pd concat yf test ignore index true\n",
      "x new train data sets head 520 new train yf test head 520 new test data sets tail 80 new test yf test tail 80\n",
      "classifier fit new train new train\n",
      "y pred new classifier predict new test\n",
      "score new pred classifier score new test new test\n",
      "score new pred\n",
      "y pred new\n",
      "y pred new df pd dataframe pred new\n",
      "fff yf test tail 80\n",
      "y pred new pd dataframe pred new\n",
      "fff\n",
      "fff pd concat fff ignore index true\n",
      "fff fff astype int pred new pred new astype int \n",
      "final pd concat pred new fff ignore index true axis 1\n",
      "fff\n",
      "final shape\n",
      "final\n",
      " roc auc\n",
      " machine learning algorithms\n",
      " ensemble\n",
      " recall precision and 1 score \n",
      " confusion matrix\n",
      " xgboost\n",
      " corr\n",
      " new ensemble\n",
      " recall tp tp fn precision tp tp fp measure 2 precision recall precision recall \n",
      "import pandas as pd import numpy as np from datetime import datetime configure matplotlib plotting import seaborn as sns import matplotlib pyplot as plt from pandas plotting import register matplotlib converters register matplotlib converters plt style use fivethirtyeight from pylab import rcparams rcparams figure figsize 11 9 changes default matplotlib plots to this size\n",
      " load my eda helper function created to do some high level analysis class eda df pd dataframe def init self df creates eda object for the dataframe note for time series data have the index be the timestamp prior to creating this object param df dataframe self df df def missing values self checks missing values return dataframe missing self df self df isna any axis 1 print missing values data return missing def duplicate values self duplicates self df self df duplicated subset none keep first true print duplicate values data return duplicates def duplicate indices self check whether the indices have any duplicates return dataframe duplicate indices self df self df index duplicated true print duplicate indices return duplicate indices def summary self return summary describe of dataframe return dataframe df self df reset index reset to include the index summary df describe include all transpose print summary metrics return summary def pandas profiling self import pandas profiling self df profile report style full width true def histogram kde self return seaborn plot sns pairplot self df diag kde sns distplot kde true fit st norm or st lognorm def outliers self col checks outliers anything outside of 5 to 95 quartile range param col str name of col to be tested return dataframe outliers self df self df col between self df col quantile 05 self df col quantile 95 print outliers return outliers def missing timeseries points self freq d checks whether there s any missing data points in continuous time series data param freq optional default d str frequency compliant with pandas formatting return dataframe first create date range date range pd date range start data index min end data index max freq freq now compare against dataset missing timeseries self df index self df index isin date range print missing timeseries data return missing timeseries def corr heatmap df fig ax plt subplots figsize 10 6 corr self df corr hm sns heatmap round corr 2 annot true cmap coolwarm fmt 2f linewidths 05 fig subplots adjust top 0 93 title fig suptitle wine attributes correlation heatmap fontsize 14 plt show def plot time series seasonal decomp self type add plots seasonal decomposition of timeseries data return matplotlib plot from statsmodels tsa seasonal import seasonal decompose decomposition seasonal decompose self df model multiplicative fig decomposition plot plt show def time series adf self returns augmented dickey fuller test from statsmodels tsa stattools import adfuller as adf series data kwh adf takes series not df result adf series print adf statistic f4 2 result 0 print p value f4 2 result 1\n",
      " load the data data pd read csv input hourly energy consumption pjme hourly csv data set index datetime inplace true data index pd to datetime data index data copy data copy deep true make deep copy including copy of the data and the indices data head 10\n",
      "eda helper eda data eda helper summary\n",
      "eda helper missing values\n",
      "eda helper duplicate values\n",
      "eda helper duplicate indices\n",
      "def preprocess data df we have duplicates so we need to de duplicate it grabbing first value that pops up for time series df sort index df df groupby df index first set freq to df df asfreq h return df\n",
      "data preprocess data data data index\n",
      " let s visualise the data data plot plt show\n",
      " given there s no missing data we can resample the data to daily level daily data data resample rule d sum set frequency explicitly to daily data daily data asfreq d daily data head 10\n",
      " we can confirm it is at the right frequency daily data index\n",
      "daily data plot plt show\n",
      "daily data daily data drop daily data index min daily data index max\n",
      "from statsmodels tsa seasonal import seasonal decompose decomposition seasonal decompose daily data model additive fig decomposition plot plt show\n",
      "from statsmodels tsa seasonal import seasonal decompose weekly data data resample rule w sum decomposition seasonal decompose weekly data model additive aggregate to weekly level fig decomposition plot plt show\n",
      "daily data index day name\n",
      " create new dataset for heatmap heatmap data daily data copy first we need to add weekdays as column heatmap data weekday name daily data index day name next we add the year as column and group the data up to annual day of week level heatmap data year heatmap data index year heatmap data heatmap data groupby year weekday name sum reset index heatmap data heatmap data reset index we drop off 2018 because it s not full year heatmap data heatmap data heatmap data year 2018 pivot it to uniform data format for heatmaps heatmap data heatmap data pivot index year columns weekday name values pjme mw reorder columns heatmap data heatmap data monday tuesday wednesday thursday friday saturday sunday heatmap data head 100\n",
      " visualise electricity load via heatmap sns heatmap heatmap data linewidths 5 cmap ylorrd cbar true cbar kws format 1 0f mwh set title heatmap by day of week \n",
      " create new dataset for heatmap heatmap data data copy first we need to add weekdays as column heatmap data hour data index hour next we add the year as column and group the data up to annual day of week level heatmap data year heatmap data index year heatmap data heatmap data groupby year hour sum reset index heatmap data heatmap data reset index we drop off 2018 because it s not full year heatmap data heatmap data heatmap data year 2018 pivot it to uniform data format for heatmaps heatmap data heatmap data pivot index year columns hour values pjme mw heatmap data head 100\n",
      " visualise electricity load via heatmap sns heatmap heatmap data linewidths 5 cmap ylorrd cbar true cbar kws format 1 0f mwh set title heatmap by hour of day \n",
      " create new dataset for heatmap heatmap data daily data copy first we need to add weekdays as column heatmap data month daily data index month name next we add the year as column and group the data up to annual day of week level heatmap data year heatmap data index year heatmap data heatmap data groupby year month sum reset index heatmap data heatmap data reset index we drop off 2018 because it s not full year heatmap data heatmap data heatmap data year 2018 pivot it to uniform data format for heatmaps heatmap data heatmap data pivot index year columns month values pjme mw reorder columns heatmap data heatmap data january february march april may june july august september october november december heatmap data head 10\n",
      " visualise electricity load via heatmap sns heatmap heatmap data linewidths 5 cmap ylorrd cbar true cbar kws format 1 0f mwh set title heatmap by day of week \n",
      " first let s load the data weather data 2017 pd read csv input ncei climate data washington dc temperature washington dc weather avg temp 2016 17 csv weather data 2018 pd read csv input ncei climate data washington dc temperature washington dc weather avg temp 2018 csv weather data weather data 2017 append weather data 2018 weather data head 10\n",
      "eda helper eda weather data eda helper summary\n",
      " clean data and aggregate weather data weather data date tavg weather data weather data weather data tavg isna weather data groupby date mean weather data weather data set index date weather data index pd to datetime weather data index sort to make sure plotting works weather data weather data sort values by date ascending true\n",
      "plt plot weather data label average temp c color lightseagreen plot labels legends etc plt title washington d c average temperature plt legend loc best plt xlabel timestamp plt ylabel degrees c plt legend loc best plt show\n",
      "fig ax plt subplots 2 1 figsize 20 20 plot 1 ax 0 plot weather data 2016 01 01 2018 08 02 label average temp c color lightseagreen ax 0 set title washington d c average temperature ax 0 set ylabel degrees c ax 0 legend loc best plot 2 ax 1 plot daily data 2016 01 01 2018 08 02 label pjme mw daily load mw color royalblue ax 1 set title pjm east region electricity load ax 1 set xlabel timestamp ax 1 set ylabel load mw ax 1 legend loc best add vertical lines to emphasis point import datetime as dt ax 0 axvline dt datetime 2016 8 15 color red linestyle ax 1 axvline dt datetime 2016 8 15 color red linestyle ax 0 axvline dt datetime 2016 12 15 color red linestyle ax 1 axvline dt datetime 2016 12 15 color red linestyle ax 0 axvline dt datetime 2017 7 15 color red linestyle ax 1 axvline dt datetime 2017 7 15 color red linestyle ax 0 axvline dt datetime 2018 1 1 color red linestyle ax 1 axvline dt datetime 2018 1 1 color red linestyle plt show\n",
      "correlation daily data 2016 01 01 2018 08 02 pjme mw corr weather data 2016 01 01 2018 08 02 tavg method pearson print the correlation between the pjm east region electricity load and washington d c average temperature is format correlation 100\n",
      "from statsmodels tsa stattools import adfuller as adf series daily data pjme mw adf takes series not df result adf series print adf statistic result 0 print p value 20f format result 1\n",
      "from statsmodels stats diagnostic import het breuschpagan as bp import statsmodels api as sm from statsmodels formula api import ols bp data daily data copy bp data time period range 1 len bp data 1 convert time series points into consecutive ints formula pjme mw time period ie pjme mw depends on time period ols auto adds intercept next we apply ordinary linear square baseline regression model as baseline test model ols formula bp data fit result bp model resid model model exog print adf statistic result 0 print p value 20f format result 1\n",
      "from statsmodels tsa stattools import acf from statsmodels graphics tsaplots import plot acf from statsmodels graphics tsaplots import plot pacf fig ax plt subplots 2 1 plot the acf function plot acf daily data pjme mw lags 720 alpha 1 suppresses ci plt show\n",
      "plot pacf daily data pjme mw lags 720 alpha 1 suppress ci plt show\n",
      " first we split it up between train and test we will aim for 12 month forecast horizon ie predict the last 12 months in the dataset cutoff 2017 08 03 daily data sort index train daily data cutoff test daily data cutoff\n",
      "baseline prediction train 2016 08 03 2017 08 02 baseline prediction index pd date range start 2017 08 03 end 2018 08 02 freq d baseline prediction tail\n",
      " create function that extracts dow and month def engineer date attributes df pre supposes input df has datetime as index param df dataframe return dataframe df dow df index dayofweek df month df index month df woy df index weekofyear df df reset index df df set index month dow woy return df calculate average of current by dow and month then get the shape of the curve by day of week that is represent for every dow month yow what the of the dow month is current df train 2014 08 03 2017 08 02 current df engineer date attributes current df current df shape avg current df reset index groupby by month dow mean pjme mw current df pct shape avg current df pjme mw current df shape avg current df current df reset index groupby by month dow woy mean current df head 21 current df count \n",
      " create predict future dataframe future df pd dataframe future df datetime pd date range start 2017 08 03 end 2018 08 02 freq d future df future df set index datetime future df engineer date attributes future df use dow month woy to forward project electricity data into forecast horizon applying the shape of the current year baseline v2 prediction future df merge current df shape avg pct shape avg how left left on month dow woy right on month dow woy baseline v2 prediction pjme mw baseline v2 prediction pct shape avg baseline v2 prediction shape avg baseline v2 prediction baseline v2 prediction set index datetime drop columns shape avg pct shape avg baseline v2 prediction head 14\n",
      " evaluate it s performance using mean absolute error mae from statsmodels tools eval measures import meanabs print mae baseline 20f format meanabs test pjme mw baseline v2 prediction pjme mw \n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot baseline prediction label baseline color orange plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title baseline model daily results for clarify let s limit to only 2017 onwards plt xlim datetime 2017 1 1 datetime 2018 9 1 plt show\n",
      " first construct the residuals basically the errors naive errors test copy naive errors pjme mw prediction baseline prediction pjme mw naive errors error naive errors pjme mw prediction naive errors pjme mw let s visually see the errors via scatterplot plt scatter naive errors index naive errors error label residual errors plot labels legends etc plt title naive one year persistence residual errors distribution plt xlabel timestamp plt ylabel mwh plt legend loc best plt show\n",
      " plot histogram with kernel density estimation kde sns distplot naive errors error kde true plot labels legends etc plt title naive one year persistence residual errors distribution \n",
      "from statsmodels graphics tsaplots import plot acf plot the acf function plot acf naive errors error lags 300 alpha 1 suppresses ci plt title naive one year persistence residual errors autocorrelation plt show\n",
      "from statsmodels tsa holtwinters import exponentialsmoothing first we split it up between train and test htrain train pjme mw hwes takes series not df htest test pjme mw hwes takes series not df model exponentialsmoothing htrain trend add seasonal add freq d seasonal periods 90 default is auto estimated 4 is quarterly and 7 is weekly fit optimized true default is true auto estimates the other parameters using grid search use basinhopping true uses basin hopping algorithm for optimising parameters use boxcox log boxcox transformation via log smoothing level alpha smoothing slope beta smoothing seasonal gamma hwes prediction model predict start htest index 0 end htest index 1 hwes prediction hwes prediction to frame rename columns 0 pjme mw print finished training and predicting let s see what the model did model summary\n",
      " evaluate it s performance using mean absolute error mae from statsmodels tools eval measures import meanabs print mae hwes add 20f format meanabs htest hwes prediction pjme mw \n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot hwes prediction pjme mw label hwes color orange plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title holtwinters model daily results for clarify let s limit to only 2017 onwards plt xlim datetime 2017 1 1 datetime 2018 9 1 plt show\n",
      " feature engineering first def preprocess xgb data df lag start 1 lag end 365 takes data and preprocesses for xgboost param lag start default 1 int lag window start 1 indicates one day behind param lag end default 365 int lag window start 365 indicates one year behind returns tuple data target default is add in lag of 365 days of data ie make the model consider 365 days of prior data for in range lag start lag end df f pjme mw i df shift periods freq d pjme mw df reset index inplace true split out attributes of timestamp hopefully this lets the algorithm consider seasonality df date epoch pd to numeric df datetime easier for algorithm to consider consecutive integers rather than timestamps df dayofweek df datetime dt dayofweek df dayofmonth df datetime dt day df dayofyear df datetime dt dayofyear df weekofyear df datetime dt weekofyear df quarter df datetime dt quarter df month df datetime dt month df year df datetime dt year df drop columns datetime pjme mw don t need timestamp and target df pjme mw target prediction is the load return \n",
      "example data train copy otherwise it becomes pointer example example preprocess xgb data example data example head 10\n",
      "xtrain train copy otherwise it becomes pointer xtest test copy otherwise it becomes pointer train feature train label preprocess xgb data xtrain test feature test label preprocess xgb data xtest\n",
      " train and predict using xgboost from xgboost import xgbregressor from sklearn model selection import kfold train test split we will try with 1000 trees and maximum depth of each tree to be 5 early stop if the model hasn t improved in 100 rounds model xgbregressor max depth 6 default 6 estimators 1000 booster gbtree colsample bytree 1 subsample ratio of columns when constructing each tree default 1 eta 0 3 learning rate default 0 3 importance type weight default is gain model fit train feature train label eval set train feature train label eval metric mae verbose true early stopping rounds 100 stop after 100 rounds if it doesn t after 100 times xtest pjme mw prediction model predict test feature xgb prediction xtest datetime pjme mw prediction set index datetime \n",
      "from sklearn metrics import mean absolute error print mae xgb 20f format mean absolute error test label xgb prediction pjme mw prediction \n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot xgb prediction label xgb one step ahead color orange plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title xgboost model one step ahead daily results for clarify let s limit to only 2015 onwards plt xlim datetime 2015 1 1 datetime 2018 10 1 plt show\n",
      "import xgboost as xgb xgb plot importance model max num features 10 importance type weight weight is the number of times feature appears in tree plt show\n",
      " so because we need the lag data we need to preprocess then do the split all data daily data copy feature label preprocess xgb data all data lag start 365 lag end 720 we will aim for 12 month forecast horizon ie predict the last 365 days in the dataset train feature feature 365 train label label 365 test feature feature 365 test label label 365\n",
      " train and predict using xgboost from xgboost import xgbregressor from sklearn model selection import kfold train test split we will try with 1000 trees and maximum depth of each tree to be 5 early stop if the model hasn t improved in 100 rounds model xgbregressor max depth 6 default 6 estimators 1000 booster gbtree colsample bytree 1 subsample ratio of columns when constructing each tree default 1 eta 0 3 learning rate default 0 3 importance type gain default is gain model fit train feature train label eval set train feature train label eval metric mae verbose true early stopping rounds 100 stop after 100 rounds if it doesn t after 100 times xtest pjme mw prediction model predict test feature xgb prediction no lag xtest datetime pjme mw prediction set index datetime xgb prediction no lag xgb prediction no lag rename columns pjme mw prediction pjme mw \n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot xgb prediction no lag label xgb no lag color orange plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title xgboost model daily results for clarify let s limit to only 2015 onwards plt xlim datetime 2015 1 1 datetime 2018 10 1 plt show\n",
      "import xgboost as xgb xgb plot importance model max num features 10 importance type gain gain is how much each feature contributed to improvement of tree plt show\n",
      " housekeeping for shap conda install conda forge shap 0 39 import shap print shap version \n",
      "explainer shap explainer model algorithm tree we know it s tree based model so use treeexplainer shap values explainer shap values train feature let s see the waterfall visualisation due to bug with pandas compatibility with 0 39 workaround link shap plots waterfall waterfall legacy explainer expected value shap values 0 feature names train feature columns tolist max display 20 max no of features to plot\n",
      "print explaining prediction format pd to datetime train feature date epoch values 1 shap force plot base value explainer expected value shap values shap values 1 feature names train feature columns tolist matplotlib true kaggle doesn t like js plot cmap rdbu figsize 30 5 show true plt tight layout\n",
      "print explaining prediction format pd to datetime train feature date epoch values 870 shap force plot base value explainer expected value shap values shap values 870 feature names train feature columns tolist matplotlib true kaggle doesn t like js plot cmap rdbu figsize 30 5 show true plt tight layout\n",
      " first construct the residuals basically the errors xgboost errors xgb prediction no lag copy xgboost errors pjme mw actual test copy xgboost errors error xgboost errors pjme mw xgboost errors pjme mw actual \n",
      " let s visually see the errors via scatterplot plt scatter xgboost errors index xgboost errors error label residual errors plot labels legends etc plt title xgboost residual errors distribution plt xlabel timestamp plt ylabel mwh plt legend loc best plt show\n",
      " plot histogram with kernel density estimation kde sns distplot xgboost errors error kde true plot labels legends etc plt title xgboost residual errors distribution \n",
      "from statsmodels graphics tsaplots import plot acf plot the acf function plot acf xgboost errors error lags 300 alpha 1 suppresses ci plt title xgboost residual errors autocorrelation plt show\n",
      " so because we need the lag data we need to preprocess then do the split all data daily data copy create train test dataset using xgboost preprocessing 365 days top 720 days lag feature label preprocess xgb data all data lag start 365 lag end 720 we will aim for 12 month forecast horizon ie predict the last 365 days in the dataset train feature feature 365 train label label 365 test feature feature 365 test label label 365 train feature train feature fillna 0 test feature test feature fillna 0 train feature drop columns date epoch don t need timestamp test feature drop columns date epoch don t need timestamp scale dataset from sklearn preprocessing import standardscaler scaler standardscaler train feature scaled scaler fit transform train feature test feature scaled scaler transform test feature\n",
      " create time series fold cross validation from sklearn model selection import timeseriessplit tscv timeseriessplit splits 5 in this case 5 fold train and predict using lasso from sklearn linear model import lassocv model lassocv alphas 0 0001 0 0003 0 0006 0 001 0 003 0 006 0 01 0 03 0 06 0 1 0 3 0 6 1 max iter 1000 1000 iterations random state 42 cv tscv verbose true model fit train feature scaled train label lasso prediction xtest copy lasso prediction pjme mw prediction model predict test feature scaled lasso prediction lasso prediction datetime pjme mw prediction set index datetime lasso prediction lasso prediction rename columns pjme mw prediction pjme mw lasso prediction\n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot lasso prediction label lasso color orange plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title lasso model daily results plt tight layout plt grid true for clarify let s limit to only 2015 onwards plt xlim datetime 2015 1 1 datetime 2018 10 1 plt show\n",
      " plot feature importance by way of coefficients create dataframe coefs pd dataframe model coef train feature columns coefs columns coef only grab the top 10 coefficients coefs abs coefs coef apply np abs coefs coefs sort values by abs ascending false head 10 coefs coefs drop abs axis 1 plot coefs coef plot kind bar plot title and axis line plt title coefficients feature importance plt hlines 0 xmin 0 xmax len coefs linestyles dashed \n",
      " first construct the residuals basically the errors lasso errors lasso prediction copy lasso errors pjme mw actual test copy lasso errors error lasso errors pjme mw lasso errors pjme mw actual \n",
      " plot histogram with kernel density estimation kde sns distplot lasso errors error kde true plot labels legends etc plt title lasso residual errors distribution plt show\n",
      " plot the acf function plot acf lasso errors error lags 300 alpha 1 suppresses ci plt title lasso residual errors autocorrelation plt show\n",
      "from statsmodels tsa statespace sarimax import sarimax from statsmodels tools eval measures import meanabs equivalent to r s auto arima to get the optimal parameters import pmdarima as pm model pm auto arima htrain seasonal true stationary true stepwise true trace true suppress warnings true first we split it up between train and test htrain train pjme mw sarimax takes series not df htest test pjme mw sarimax takes series not df next define hyperparameters default is ar model 1 0 0 0 0 0 0 1 ar order 0 degree 1 ma window 0 ar seasonal order 1 seasonal order 2 ma seasonal order 6 seasonality period length model sarimax htrain order d seasonal order d m enforce stationarity false enforce invertibility false fit maxiter 50 default is 50 results model get prediction start htest index 0 end htest index 1 dynamic false sarima prediction ci results conf int alpha 1 0 8 80 ci sarima prediction results predicted mean sarima prediction sarima prediction to frame rename columns 0 pjme mw evaluate it s performance using mean absolute error mae print finished training and predicting mae sarima 20f aic parameters p d q p d q m format meanabs htest sarima prediction pjme mw model aic d p q \n",
      " let s see what the model did model plot diagnostics figsize 15 12 plt show\n",
      " evaluate it s performance using mean absolute error mae from statsmodels tools eval measures import meanabs print mae sarima 20f format meanabs htest sarima prediction pjme mw \n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot sarima prediction pjme mw label sarima color orange plot confidence interval plt fill between sarima prediction index sarima prediction ci lower pjme mw sarima prediction ci upper pjme mw color skyblue alpha 0 7 70 transparency label 80 ci plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title sarima model daily results for clarify let s limit to only 2017 onwards plt xlim datetime 2017 1 1 datetime 2018 9 1 plt show\n",
      "from fbprophet import prophet ftrain train reset index rename columns datetime ds pjme mw y prophet takes ds and as column names only model prophet changepoints 25 default is 25 changepoint prior scale 0 05 default is 0 05 seasonality mode additive interval width 0 8 ci default is 0 8 or 80 model fit ftrain create the future dataframe with date range that will be used to test accuracy future df test reset index datetime to frame rename columns datetime ds predict the future forecast model predict future df prophet prediction forecast set index ds yhat yhat lower yhat upper cutoff prophet prediction prophet prediction rename columns yhat pjme mw print finished training and predicting \n",
      "model plot forecast plt show\n",
      " let s visually see the results plt scatter train index train pjme mw label training data color black plt scatter test index test pjme mw label test actuals color red plt plot prophet prediction pjme mw label prophet color orange plot confidence interval plt fill between prophet prediction index prophet prediction yhat lower prophet prediction yhat upper color skyblue alpha 0 7 70 transparency label 80 ci plot labels legends etc plt xlabel timestamp plt ylabel mwh plt legend loc best plt title prophet model daily results for clarify let s limit to only 2017 onwards plt xlim datetime 2017 1 1 datetime 2018 9 1 plt show\n",
      "model plot components forecast plt show\n",
      " first construct the residuals basically the errors prophet errors prophet prediction copy prophet errors pjme mw actual test pjme mw prophet errors error prophet errors pjme mw prophet errors pjme mw actual \n",
      " let s visually see the errors via scatterplot plt scatter prophet errors index prophet errors error label residual errors plot labels legends etc plt title prophet residual errors distribution plt xlabel timestamp plt ylabel mwh plt legend loc best plt show\n",
      " plot histogram with kernel density estimation kde sns distplot prophet errors error kde true plot labels legends etc plt title prophet residual errors distribution \n",
      " let s get the top 10 over forecasts i e where the error is the highest negative number top 10 errors prophet errors sort values error ascending true pjme mw actual pjme mw error head 10 plt scatter top 10 errors index top 10 errors pjme mw label predicted mw plt scatter top 10 errors index top 10 errors pjme mw actual label actual mw labels titles etc plt title prophet top 10 errors plt xlabel timestamp plt ylabel mwh plt legend loc best \n",
      "top 10 errors head 10\n",
      "print mae baseline 2f format meanabs test pjme mw baseline prediction pjme mw print mae holtwinters 2f format meanabs test pjme mw hwes prediction pjme mw print mae xgboost 2f format mean absolute error test label xgb prediction no lag pjme mw print mae lasso 2f format mean absolute error test label lasso prediction pjme mw print mae sarima 2f format meanabs test pjme mw sarima prediction pjme mw print mae prophet 2f format meanabs test pjme mw prophet prediction pjme mw \n",
      "def mape true pred function to calculate mape true pred np array true np array pred return np mean np abs true pred true 100\n",
      "print mae baseline 2f format mape test pjme mw baseline prediction pjme mw print mae holtwinters 2f format mape test pjme mw hwes prediction pjme mw print mae xgboost 2f format mape test pjme mw xgb prediction no lag pjme mw print mae lasso 2f format mape test pjme mw lasso prediction pjme mw print mae sarima 2f format mape test pjme mw sarima prediction pjme mw print mae prophet 2f format mape test pjme mw prophet prediction pjme mw \n",
      "def plot model result ax prediction model name color plot model results prediction dataframe model name str return ax training and test actuals ax scatter train index train pjme mw label training data color black ax scatter test index test pjme mw label test actuals color red model results ax plot prediction pjme mw label model name color color alpha 0 7 for clarify let s limit to only august 2016 onwards ax set xlim datetime 2016 8 1 datetime 2018 10 1 set axis ax set ylim 500000 1100000 set axis labels ax set ylabel mwh ax legend loc best ax set title mape 2f mae 2f format model name mape test pjme mw prediction pjme mw mean absolute error test pjme mw prediction pjme mw fontsize 40 return ax\n",
      "fig ax plt subplots 6 1 figsize 30 40 plot labels legends etc plt xlabel timestamp ax 0 plot model result ax 0 baseline prediction model name baseline naive color midnightblue ax 1 plot model result ax 1 xgb prediction no lag model name xgboost color deepskyblue ax 2 plot model result ax 2 prophet prediction model name prophet color steelblue ax 3 plot model result ax 3 lasso prediction model name lasso color royalblue ax 4 plot model result ax 4 hwes prediction model name holtwinters color lightsteelblue ax 5 plot model result ax 5 sarima prediction model name sarima color dodgerblue plt tight layout pad 3 0 plt show\n",
      "now for feature importance let s also have look at it using shap shap is interpretability model designed to explain how the features contribute to the overall predictions of the models shap values are expressed based on particular baseline expected value values can be negative i e reduce the output value below the baseline value or positive i e increase the output value above the baseline the baseline value is generally the average of all predictions shap is both global and local interpretability model that is it can explain how the model generally deals with predictions similiar how to feature importance works in general it can explain how features contributed to specific prediction\n",
      "next we ll train the model using sklearn s time series split cross validation method in this case we ll create 5 fold split \n",
      "now there s tail end where it s not full day so it s dropping off for our purposes we will just delete that part day \n",
      "seems like yesterday s value is the biggest factor in determining today s value this again makes sense given how the autocorrelation function showed yesterday s value had the biggest correlation with today s value \n",
      "now let s do the same thing but over the hours of day to sort of see peak operating hours \n",
      "interestingly the correlation between the pjm east region electricity load and washington d c average temperature is only 13 03 what this means is extreme peaks in weather causes spikes but weather in general isn t really useful for predicting electricity usage \n",
      "now we need to split the time series between training and test like what we did before cross validation is harder in this case as the datasets need to be sequential we will also need to specify features and labels ie the target we want to predict \n",
      "now let s also look at mean absolute percentage error mape unlike mae mape has issues in its calculations namely when the actual value is 0 can t divide by zero and negative values can t go beyond 100 regardless mape is good sense check to see which model is proportionally better \n",
      "let s visualise the data again\n",
      " training prophet works quite well out of the box so just stuck with the default hyperparameters the results can be seen below with the black dots representing historical points and the blue line representing the prediction \n",
      "next we ll process it so the data is at the hourly level \n",
      "now let s see both side by side i ve added in red lines to emphasis particular peaks cold and hot \n",
      "let s zoom into 2017 onwards and visualise it with the consistent graph format as above \n",
      "so let s run the test for stationarity and see the results \n",
      "like holtwinters the training and testing data was split between 2002 to 2017 and 2017 to 2018 the results were as follows \n",
      "so most of the points are within the shaded blue ie confidence interval indicating there s no statistically significant autocorrelation going on this is good as if there was autocorrelation with our errors it means there s some autocorrelation our model is failing to capture \n",
      "another way to visualise seasonality is to use heatmap we can base it on week to see which days have higher electricity usage note we will drop off 2018 because it s not full year and will skewer the heatmap first let s construct the dataframe table \n",
      "pjm is one of the world s largest wholesale electricity markets encompassing over 1 000 companies 65 million customers and delivered 807 terawatt hours of electricity in 2018 see their annual report link we ll use the pjm east region dataset which covers the eastern states of the pjm area there isn t readily available data that identifies which regions are covered so i m going to assume for this analysis the following states the coastal ones delaware maryland new jersey north carolina pennsylvania virginia district of columbia\n",
      "let s get to know the data bit more few high level eda stuff \n",
      "one thing that jumps out right now is very difficult to see what s going on as the graph is very packed together more traditional econometric statistical models such as holtwinters and sarima require 3 characteristics for them to work properly namely seasonality the dataset is cyclical in nature stationarity the properties of the dataset doesn t change over time autocorrelation there is similiar between current and past ie lagged data points how about we aggregate up to weekly level to reduce the noise\n",
      "so using this you can see as expected the weekend has lower electricity use many businesses are closed during weekends and therefore this makes sense \n",
      "we need to clean and aggregate the weather stations into one time series including removing weather stations with missing data so let s do that \n",
      " exploratory data analysis we need to first check few things 1 duplicate and missing data as well as spread 2 how autocorrelated is this data and its seasonal decomposition\n",
      " baseline model naive forecasting before we go knee deep into machine learning it is good to use naive forecasting techniques to determine baseline that is if the ml models cannot beat these baseline forecasts then we would be better off just using naive forecast instead they are naive in the sense they are simple to apply but in reality they are pretty powerful and effective forecasts not too many ml models actually can consistently beat naive forecast common naive forecast for predicting multi step forecast i e for us it would be the next 365 days is to use one year ago persistent forecast this basically means the value for say 31 august 2019 is predicted using the value for 31 august 2018 \n",
      " bringing it all together the final results finally let s bring the models together and see which one did the best \n",
      "the most incorrect days were july and january the peak of summer and winter respectively makes sense you get the biggest peaks in these periods due to weather \n",
      "immediately you can see that it is pretty half decent forecast but of course we need to see the errors residuals to figure out whether 1 it is normally distributed i e it has no bias to under or over forecasting 2 whether the errors are autocorrelated that is whether the model failed to pick up on any autocorrelation patterns \n",
      " seasonal decomposition at high level time series data can be thought of as components put together that is data level trend seasonality noise level the average value in the series trend the increasing or decreasing value in the series seasonality the repeating short term cycle in the series noise residual the random variation in the series using the python statsmodel library the above components can be decomposed ie seasonal decomposition \n",
      "so you can see that despite some tuning the results are not particularly good the confidence interval is very big indicating the model has is not confident in the prediction either \n",
      "now let s see what the algorithm considered most important we ll grab the top 10 features by weight the weight is the percentage representing the relative number of times particular feature occurs in the trees of the model it s rough way of saying the more times you reference particular feature the more likely it is important \n",
      "you can see start to see pattern electricity usage peak and troughs seem to be very seasonal and repetitive this makes sense considering office hours weather patterns shopping holidays etc furthermore you can see the trend of the data seems to be trailing downwards in the last few years \n",
      "we can also do the same with season plot that is compare each year over 12 months \n",
      "next let s see feature importance by way of coefficients we ll only get top 10 remember lasso will zero out irrelevant features so in this case these are the top 10 features that lasso considers are most important \n",
      "now let s evaluate the residuals and see whether the model is biased in any way first we ll look at the distribution of the errors \n",
      "good so there s no missing or duplicate data and you can see the data is from 1 jan 2002 to 2 august 2018 \n",
      "interestingly it means 11am to 9pm is the busiest peak time of the grid \n",
      "the results of the model are fairly accurate however caveat is that because the model knows about yesterday s value therefore the forecast horizon ie the maximum length of time it can predict into the future is only 1 day this is also known as one step ahead forecast if you only have yesterday s value you can only predict today s value if you only have today s value you can only predict tomorrow s value \n",
      "not bad the naive forecast actually is quite normally distributed in terms of its errors \n",
      "you can see the 80 confidence interval in light blue indicating the model is confident that 80 of the actual data will land in that predicted range \n",
      "like holtwinters let s see the components of the model \n",
      "what the above graph shows is how correlated prior point is to the current point the further the number is away from 0 the more correlation there is generally we would only consider any points above for positive numbers and below for negative numbers the blue shaded area the confidence interval as statistically significant and worth noting this shows that yesterday s value has very high correlation with today s value and there is seasonality every 6 months it seems to repeat itself as eluded earlier this makes sense if you factor in weather patterns winter and summer have higher electricity usage due to more heat cooling needed my personal guess is because of weather winter and summer have higher electricity usage \n",
      "the lower the error the better more accurate the model is so therefore in this case the winner is lasso however prophet and xgboost still comes pretty close second you can see that there was lot of noise in this dataset and while there s high seasonality more conventional statistical approaches such as holtwinters and sarima weren t able to get through all that noise interestingly all the forecasting models beat the baseline except holtwinters and sarima this does at least demonstrate that we are still better off using these models than just doing naive forecasting hopefully that gives you bit of flavour to time series forecasting and some of the unique aspects of it \n",
      "let s evaluate our model using mean absolute error mae and visualise the results mean absolute error mae is an evaluation metric that measures the average magnitude of the errors in set of predictions in other words how wrong the model is unlike other metrics such as root mean squared error it does not have any particular weighting training data is in blue while test evaluation data is in red \n",
      "you can see that at the individual each prediction has different main factor compared to the more general model overall quite good results now let s have look at the residuals errors first let s look at the distribution of the errors remember the ideal state is the errors are centred around zero meaning the model does n t particularly over or under forecast in biased way \n",
      " statistical test smoke alarms statistical tests are good way to test whether the data is conductive to conventional statistical methods there are certain good statistical tests you can apply to dataset as smoke alarm test they are good indication whether the data is conducive to accurate forecasting both the below statistical tests use hypothesis testing and values which require cutoff to be picked in advance the general rule of thumb is 5 which means there s only 5 chance of the statistical tests to be incorrect image png attachment image png stationary means the properties of the dataset don t change over time non stationary means the trends seasonality changes over time and the data is affected by factors other than the passsage of time non stationary is sometimes known as random walk which are notoriously difficult to forecast because the underlying properties keep changing e g like trying to hit moving target note that random walk is different to set of random numbers it s random because the next point is based on random modification on the first point e g add minus multiply whereas in random numbers set ther would be little relationship between each data point \n",
      "then we visualise it \n",
      "let s see what this looks like \n",
      "now let s plot the weather data and have look\n",
      "we are using box cox as heteroskedatic test before showed data requires dampening to reduce extremes \n",
      "again let s see feature importance this time by gain not weight as model trained on gain \n",
      "using holtwinters data from 2002 to 2017 was used to train the model while the remaining 2017 to 2018 data was used to test evaluate the model s accuracy training data is in blue while test evaluation data is in red let s evaluate our model using mean absolute error mae and visualise the results\n",
      " xgboost ensemble learning xgboost has gained in popularity recently by being quite good at predicting many different types of problems normally with decision tree models you would get the data and create one tree for it this of course means it is very prone to overfitting and being confused by the unique tendencies of the past data to overcome this you do gradient boosting at very high level it is analogous to the algorithm creating decision tree to try to predict the result figure out how wrong it was and then create another tree that learns from the first one s mistakes this process is then repeated few hundreds or even few thousand times with each tree being boosted by the prior one s mistakes the algorithm keeps going until it stops improving itself the technical aspects of the mathematics are much more complex and bit beyond my knowledge to bef honest if you want more details the documentation is here \n",
      "after preprocessing and cleaning up the data as well as making sure the frequency is at the hourly level let s plot the data \n",
      "you can easily see seasonality and trends there s clear downward trend and seasonality every year more electricity is used in winter \n",
      "first let s have look at the dataset first few lines \n",
      " prophet lastly we will use facebook prophet an open source library that is also generalised additive model ie final result is made up of multiple components added together it s all about probabilities unlike regular generalised linear models facebook prophet s uses bayesian curve fitting approach the concept of bayesian theorem is at high level trying to determine the probability of related events given knowledge assumptions you already know ie priors this is basically fancy talk for saying it focuses on finding bunch of possible parameters and the probability of each one rather than finding fixed optimal values for the model how certain or uncertain the model is about each possible parameter is known as the uncertainty interval the less data the model sees the bigger the interval is the sources of uncertainty that prophet s bayesian approach aims to address are uncertainty of the predicted value uncertainty of the trend and trend changes uncertainty of additional noise trends changepoints and seasonality prophet is different to sarima and holtwinters as it essentially decomposes time series differently by data trend x seasonality x holidays x noise in prophet trend represents non periodic changes while seasonality represents periodic changes where it differs from other statistic models like sarima and holtwinters is prophet factors in the uncertainty of trends changing interestingly prophet fits curve to each component independently ie fits regression for each component with time as independent variable that is trend fits piece wise linear log curve seasonality uses fourier series holidays uses constant fixed values prophet reacts to changes in trends by changepoints that is sudden and abrupt changes in the trend an example is the release of new electric car that will impact sale of petrol cars how reactive flexible prophet is to changepoints will impact how much fluctation the model will do for example when the changepoint scale is to very high it becomes very sensitive and any small changes to the trend will be picked up consequently the sensitive model may detect spikes that won t amount to anything while an insensitive model may miss spikes altogether the uncertainty of the predictions is determined by the number of changepoints and the flexibility of the changepoint allowed in other words if there were many changes in the past there s likely going to be many changes in the future prophet gets the uncertainty by randomly sampling changepoints and seeing what happened before and after furthermore prophet factors in seasonality by considering both its length ie seasonal period and its frequency ie its fourier order for example if something keeps happening every week on monday the period is 7 days and its frequency is 52 times year there s also function for prophet to consider additional factors e g holidays but for the purposes of this forecast we won t use it \n",
      " 1 holtwinters triple exponential smoothing next we will use time series forecasting model that takes advantage of the above identified components this is known as generative additive model as the final forecast value is adding together multiple components the triple refers to the three components 1 level 2 trend 3 seasonality holtwinters works really well when the data is seasonal and has trends smoothing basically means more weight is put on more recent data compared to the past note the main hyperparameters for the model are additive vs multiplicative ie add or mul box cox to use box cox log transformation to reduce the noise of the data alpha smoothing factor between 0 and 1 1 means will always take yesterday s value naive forecasting 0 means take simple average of past additive means the formula looks more like this data level trend seasonality multiplicative means the formula looks more like this data level trend seasonality\n",
      "finally we train the model and see how well it performed using mae and visualise the results to train the model again split the data into two parts training from 2002 to 2017 and evaluation testing from 2017 to 2018 in this case used 1000 runs and maximum depth of each tree to be 6 that is it does 1000 runs or less if it stops improving with each tree having max of 6 levels as tree based algorithm generally xgboost doesn t handle trends in data well compared to linear models however given as shown above in the adf test the data is stationary trend is not really an issue and we can proceed otherwise we would need to de trend the data first as part of preprocessing \n",
      "again you can see that the errors seem to be distributed around zero and there s no statistically significant autocorrelation going on this indicates that the model isn t biased and not leaning torwards under or over forecasting however you can see that the model does seem to miss under forecast bit more particularly for spikes let s have closer look at the biggest under forecasts \n",
      " using machine learning to forecast electricity usage in the future the past dictates the future or does it forecasting time series data is different to other forms of machine learning problems due one main reason time series data often is correlated with the past that is today s value is influenced by for example yesterday s value last week s value etc this is known as autocorrelation ie correlating with self because time series has autocorrelation many traditional machine learning algorithm that consider every data point independent don t work well that is every prediction is solely based on particular day s data and not what happened yesterday day before etc to put it bluntly traditional machine learning algorithms have no concept of the passage of time every single data point is just row in the table it doesn t factor into trends which as humans we would intuitively see e g sales went up by 20 this week generally to make machine learning model factor data from particular point s past you use shifting feature engineering techniques that is for example expressly adding in yesterday s value show as feature column for the model to consider this illustration will attempt to forecast energy consumption to give bit of flavour of how time series forecasting works comparing 1 holtwinters triple exponential smoothing 2 seasonal autoregression integrated moving average sarima 3 xgboost regressor 4 lasso regularised regression 5 facebook s prophet showcase of time series forecasting electricity usage in the electricity industry being able to forecast the electricity usage in the future is essential and core part of any electricity retailer s business the electricity usage of an electricity retailer s customers is generally known as the retailer s load and the usage curve is known as the load curve being able to accurately forecast load is important for number of reasons predict the future base load electricity retailers need to be able to estimate how much electricity they need to buy off the grid in advance smooth out pricing if the load is known is advance electricity retailers can hedge against the price to ensure they aren t caught out when the price skyrockets 3 months into the future in nutshell the business problem is given the historical power consumption what is the expected power consumption for the next 365 days because we want to predict the daily load for the next 365 days there are multiple time steps and this form of forecasting is known as multi step forecasting in contrast if you only wanted to predict the load on the 365th day or the load for tomorrow only this would be known as one step forecast this dataset is from link pjm interconnection llc pjm is regional transmission organization rto in the united states it is part of the eastern interconnection grid operating an electric transmission system serving all or parts of delaware illinois indiana kentucky maryland michigan new jersey north carolina ohio pennsylvania tennessee virginia west virginia and the district of columbia the hourly power consumption data comes from pjm s website and are in megawatts mw the regions have changed over the years so data may only appear for certain dates per region \n",
      " sarima the conventional arima model assumes that the historical data are useful to predict the value at the next time step in this case this is somewhat true as the acf plot before showed past value is somewhat correlated with today s value arima basically integrates two naive forecasting techniques together 1 autoregression uses one or more past values to forecast the future the number of values used is known as the order e g order 2 means yesterday and day before s value is used 2 integrating the part that reduces seasonality how many degrees of differencing is done to reduce seasonality is the order 3 moving average uses the moving average of the historical data to adjust the forecasted values this has smoothing effect on the past data as it uses the moving average rather than the actual values of the past the number of days in the moving average window is the order sarima then adds seasonality flavour to the arima model it factors in trends and seasonality as explained above the main hyperparameters are sarimax p d q p d q m 1 autoregression order p 2 integrating order d 3 moving average window q 4 seasonal autoregressive order p 5 seasonal difference order d 6 seasonal moving average order q 7 number of time steps for single seasonal period m the is the exogenous external variables to the model they are optional and for this model we won t use them \n",
      "so looking quite good you can see some forecasts were pretty off particularly for spikes but overall it seems the model treats overs and under equal \n",
      "let s see the correlation between the two \n",
      "so lasso is producing half decent results now let s have look at the residuals errors first let s look at the distribution of the errors remember the ideal state is the errors are centred around zero meaning the model doesn t particularly over or under forecast in biased way \n",
      " housekeeping and preprocessing\n",
      " an alternative feature engineering approach let s include lag without data leakage let s try to make the xgboost model more than one step ahead we ll only feature engineer day of the week day of the month day of the year week of the year month year hopefully this is enough for the model to artificially pick up seasonality factors e g if same day of week might be correlated however as forecast horizon is 365 days we can still include lag 365 to lag 720 days data i e year before last year s data \n",
      "so you can see the largest contributing factor is the actual date of prediction day of week also weighs heavily while the actual day of the year has negative contributing factor like correlation the further away the value is from 0 means the more it influences the prediction now let s have look at specific prediction e g the 2nd one and 871st one \n",
      "you can see because the frequency is at the hourly level this is will make forecasting difficult and its also difficult to visualise too so what we will do is resample it and aggregate it to the daily level\n",
      "in our case we got the above results which means the data is stationary as the value was below 0 05 the data is heteroskedactic as the value was not below 0 05 \n",
      "let s evaluate the results and visualise it \n",
      "so it seems exponential smoothing is no go let s see how the others fare \n",
      "so you can see that july has the heaviest load on the grid which makes sense as it is the height of summer in the region air conditioners are expensive after all to further do some analysis let s add in weather data as we are focusing on the pjm east region for simplicity i ll use washington d c s weather data as reference for the entire region weather data was obtained from the us noaa s national centers for environmental information ncei link data is in celsius degrees is only from 2016 to 2018 there s 166 weather stations in the dataset that cover the washington d c region \n",
      "next let s have look at some of the summary eda data missing values and duplicates \n",
      "so let s run the test for heteroskedacity and see the results \n",
      "this graph shows that the last 90 days have stronger correlation but the effect becomes much less obvious the further back you go \n",
      " lasso l1 regression lasso regression incorporate regularisation and feature selection into its algorithm regularisation is technique used in regression algorithms to avoid overfitting for lasso this means it will penalise the irrelevant features by effectively by zeroing out those features by multiplying it with 0 coefficient the main hyperparameter to tune is the penalty factor i e lambda or alpha factor of 0 means no penalisation occurs and it effectively just does an ordinary least squares ols regression since we ve already set up all the train test split as well as feature engineering in the prior xgboost model we can just re use it however like the decision tree based xgboost linear regression is sensitive to scale therefore we also need to scale the data \n",
      "this explains why the seasonal decomposition and adf show this data has lot of noise and crazy swings the consequences of these statistical tests means that the traditional assumptions of linear regression have been violated that is more conventional methods of linear regression statistical tests and tests become ineffective therefore this forecasting model won t factor in these tests \n",
      " feature engineering as eluded earlier most machine learning models don t look back to prior values essentially if you have table each row is an independent data point and the ml model doesn t consider the prior row s data this is problematic for time series data as shown above autocorrelation happens to address this issue we use feature engineering to create additional features in this case created 365 extra columns each prior day today minus 1 day today minus 2 days until today minus 365 days \n",
      " train test split before we continue let us split the data up between train and split with specified cutoff date let s pick 3 august 2017 12 months prior \n",
      " heteroskedacity refers to instances where the data is evenly distributed along regression line basically it means the data is more closely grouped together and therefore is less spiky ie has more peaks troughs heteroskedactic data means the peaks and troughs ie outliers are being observed way more often than normally distributed dataset this means that model will have hard time predicting these spikes to alleviate this heteroskedactic data generally needs to be box cox log transformed to dampen the extreme peaks troughs that is bringing the data closer together so model can better fit the whole data and hit the peaks troughs \n",
      "importantly the model should have the residuals uncorrelated and normally distributed ie the mean should be zero that is the the centre point of the residuals should be zero and the distribution plot kde should also be centred on 0 \n",
      "next let s look at the autocorrelation of the errors\n",
      "now let s plot the results\n",
      " autocorrelation autocorrelation is basically how much correlation there is between particular time point and prior one e g today s value is highly correleated with last week s value again python statsmodel has great autocorrelation function acf that easily produces this \n",
      "another modification of this autocorrelation analysis is the partial autocorrelation function pacf this function is variant of acf as it finds correlation of the residuals after removing the effects which are already explained in earlier lags that way you don t get compounding correlation effect \n",
      "import numpy as np import pandas as pd import os for dirname filenames in os walk kaggle input for filename in filenames print os path join dirname filename import seaborn as sns from datetime import datetime import plotly express as px import matplotlib pyplot as plt matplotlib inline import seaborn as sns sns set style white config inlinebackend figure format retina from mpl toolkits mplot3d import axes3d from scipy cluster import hierarchy from scipy spatial distance import pdist from sklearn manifold import tsne from sklearn cluster import kmeans from sklearn import ensemble decomposition from sklearn decomposition import pca from sklearn preprocessing import labelencoder standardscaler from sklearn ensemble import extratreesregressor from sklearn model selection import train test split from sklearn pipeline import pipeline\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test for Text Cleaning \"\"\"\n",
    "test_source = full_df.source.to_numpy()\n",
    "\n",
    "for i in range(len(test_source[0])):\n",
    "    print(cleaning_words(test_source[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db77484-362e-4686-9bf6-137a247e1bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
